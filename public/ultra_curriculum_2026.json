{
  "metadata": {
    "version": "2026.1.0",
    "title": "TutoDeCode - Base de Données de Savoir Absolu",
    "description": "Curriculum ultra-avancé pour développeurs d'élite - 50 cours couvrant Rust Système, Tauri v3, Frontend Elite, IA Agentic et Cyber Ops",
    "tracks": [
      "SYSTEM",
      "TAURI_V3",
      "FRONTEND_ELITE",
      "IA_AGENTIC",
      "CYBER_OPS"
    ],
    "total_courses": 50,
    "estimated_duration": "480 heures",
    "last_updated": "2026-01-15",
    "author": "TutoDeCode Architect Team",
    "difficulty": "Expert/Master"
  },
  "courses": [
    {
      "id": "sys-001-unsafe-mastery",
      "track": "SYSTEM",
      "level": "Expert",
      "title": "Unsafe Rust: Maîtrise Absolue de la Mémoire",
      "duration": "8h30",
      "description": "Plongée dans les profondeurs d'unsafe Rust, gestion manuelle des pointeurs raw, transmutes, et FFI avancé.",
      "markdown_content": "# Unsafe Rust: Maîtrise Absolue de la Mémoire\n\n## Architecture de la Mémoire Rust\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    STACK (LIFO)                             │\n│  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────┐  │\n│  │  Variables  │  │   Structs   │  │   References        │  │\n│  │  locales    │  │   inline    │  │   (&T, &mut T)      │  │\n│  └─────────────┘  └─────────────┘  └─────────────────────┘  │\n│                    Taille connue à la compilation           │\n└─────────────────────────────────────────────────────────────┘\n                              │\n                              ▼\n┌─────────────────────────────────────────────────────────────┐\n│                    HEAP (Allocation dynamique)              │\n│  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────┐  │\n│  │    Box<T>   │  │    Vec<T>   │  │   String            │  │\n│  │  (owned)    │  │  (contigu)  │  │   (UTF-8)           │  │\n│  └─────────────┘  └─────────────┘  └─────────────────────┘  │\n│                    Géré par l'allocateur                    │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Raw Pointers: *const T vs *mut T\n\n| Aspect | *const T | *mut T |\n|--------|----------|--------|\n| Déréférencement | Lecture seule | Lecture/Écriture |\n| Création | `&value as *const _` | `&mut value as *mut _` |\n| Aliasing | Multiple autorisé | Unique requis |\n| Null | `std::ptr::null()` | `std::ptr::null_mut()` |\n\n## Code: Implémentation d'un Arena Allocator\n\n```rust\nuse std::alloc::{alloc, dealloc, Layout};\nuse std::ptr::NonNull;\nuse std::sync::atomic::{AtomicUsize, Ordering};\n\npub struct Arena {\n    memory: NonNull<u8>,\n    layout: Layout,\n    offset: AtomicUsize,\n    capacity: usize,\n}\n\nimpl Arena {\n    pub fn new(capacity: usize) -> Self {\n        let layout = Layout::array::<u8>(capacity).unwrap();\n        let ptr = unsafe { alloc(layout) };\n        let memory = NonNull::new(ptr).expect(\"Allocation failed\");\n\n        Self {\n            memory,\n            layout,\n            offset: AtomicUsize::new(0),\n            capacity,\n        }\n    }\n\n    pub fn alloc<T>(&self, value: T) -> *mut T {\n        let size = std::mem::size_of::<T>();\n        let align = std::mem::align_of::<T>();\n\n        // Alignement\n        let current = self.offset.load(Ordering::Acquire);\n        let aligned = (current + align - 1) & !(align - 1);\n\n        if aligned + size > self.capacity {\n            panic!(\"Arena out of memory\");\n        }\n\n        match self.offset.compare_exchange(\n            current,\n            aligned + size,\n            Ordering::Release,\n            Ordering::Acquire\n        ) {\n            Ok(_) => unsafe {\n                let ptr = self.memory.as_ptr().add(aligned) as *mut T;\n                ptr.write(value);\n                ptr\n            },\n            Err(_) => self.alloc(value), // Retry\n        }\n    }\n}\n```\n\n## FFI Avancé avec C\n\n```rust\nuse std::ffi::{CStr, CString};\nuse std::os::raw::{c_char, c_int};\n\n#[repr(C)]\npub struct NativeStruct {\n    pub id: c_int,\n    pub name: *const c_char,\n}\n\nextern \"C\" {\n    fn native_process(data: *mut NativeStruct) -> c_int;\n}\n\npub fn safe_wrapper(name: &str) -> Result<i32, String> {\n    let c_name = CString::new(name).map_err(|e| e.to_string())?;\n    let mut data = NativeStruct {\n        id: 42,\n        name: c_name.as_ptr(),\n    };\n\n    let result = unsafe { native_process(&mut data) };\n\n    if result < 0 {\n        Err(format!(\"Native error: {}\", result))\n    } else {\n        Ok(result)\n    }\n}\n```",
      "terminal_task": {
        "command": "cargo test arena_allocator -- --nocapture",
        "expected_output": "test arena_allocator::tests::test_concurrent_alloc ... ok",
        "hint": "Implémentez l'Arena avec AtomicUsize pour le offset, utilisez alloc/dealloc de std::alloc, et assurez-vous de l'alignement correct avec (current + align - 1) & !(align - 1)"
      }
    },
    {
      "id": "sys-002-zero-copy",
      "track": "SYSTEM",
      "level": "Expert",
      "title": "Zero-Copy Architecture: I/O Sans Allocation",
      "duration": "6h45",
      "description": "Techniques avancées de zero-copy, vecteurs d'I/O, scatter-gather, et traitement de données en streaming.",
      "markdown_content": "# Zero-Copy Architecture: I/O Sans Allocation\n\n## Principe Fondamental\n\n```\nTRADITIONNEL (3 copies):          ZERO-COPY (0 copie):\n┌─────────┐    ┌─────────┐        ┌─────────┐    ┌─────────┐\n│  Kernel │───▶│  User   │        │  Kernel │───▶│  User   │\n│  Buffer │    │  Buffer │        │  Buffer │    │ (view)  │\n└─────────┘    └─────────┘        └─────────┘    └─────────┘\n      │              │                  │              │\n      ▼              ▼                  ▼              ▼\n┌─────────┐    ┌─────────┐        ┌─────────┐    ┌─────────┐\n│   NIC   │    │  App    │        │   NIC   │    │  App    │\n└─────────┘    └─────────┘        └─────────┘    └─────────┘\n```\n\n## Ring Buffer Lock-Free MPSC\n\n```rust\nuse std::sync::atomic::{AtomicU64, Ordering};\nuse std::mem::MaybeUninit;\nuse std::cell::UnsafeCell;\n\nconst BUFFER_SIZE: usize = 1024;\n\npub struct RingBuffer<T> {\n    buffer: [UnsafeCell<MaybeUninit<T>>; BUFFER_SIZE],\n    head: AtomicU64, // Écriture\n    tail: AtomicU64, // Lecture\n}\n\nunsafe impl<T: Send> Send for RingBuffer<T> {}\nunsafe impl<T: Send> Sync for RingBuffer<T> {}\n\nimpl<T> RingBuffer<T> {\n    pub fn new() -> Self {\n        let buffer = unsafe {\n            let mut b: [UnsafeCell<MaybeUninit<T>>; BUFFER_SIZE] = \n                std::mem::zeroed();\n            b\n        };\n\n        Self {\n            buffer,\n            head: AtomicU64::new(0),\n            tail: AtomicU64::new(0),\n        }\n    }\n\n    pub fn push(&self, value: T) -> Result<(), T> {\n        let head = self.head.load(Ordering::Acquire);\n        let tail = self.tail.load(Ordering::Acquire);\n\n        // Vérifier espace disponible\n        if head.wrapping_sub(tail) >= BUFFER_SIZE as u64 {\n            return Err(value);\n        }\n\n        let idx = (head % BUFFER_SIZE as u64) as usize;\n\n        unsafe {\n            (*self.buffer[idx].get()).write(value);\n        }\n\n        self.head.store(head.wrapping_add(1), Ordering::Release);\n        Ok(())\n    }\n\n    pub fn pop(&self) -> Option<T> {\n        let tail = self.tail.load(Ordering::Acquire);\n        let head = self.head.load(Ordering::Acquire);\n\n        if tail == head {\n            return None;\n        }\n\n        let idx = (tail % BUFFER_SIZE as u64) as usize;\n\n        let value = unsafe {\n            (*self.buffer[idx].get()).assume_init_read()\n        };\n\n        self.tail.store(tail.wrapping_add(1), Ordering::Release);\n        Some(value)\n    }\n}\n```\n\n## io_uring Zero-Copy (Linux)\n\n```rust\nuse io_uring::{IoUring, opcode, types};\nuse std::os::unix::io::RawFd;\n\npub struct IoUringZeroCopy {\n    ring: IoUring,\n    registered_buffers: Vec<Vec<u8>>,\n}\n\nimpl IoUringZeroCopy {\n    pub fn new(entries: u32, buffer_size: usize, buffer_count: usize) -> Self {\n        let ring = IoUring::new(entries).unwrap();\n        let registered_buffers: Vec<_> = (0..buffer_count)\n            .map(|_| vec![0u8; buffer_size])\n            .collect();\n\n        // Enregistrer les buffers pour zero-copy\n        let slice_refs: Vec<_> = registered_buffers\n            .iter()\n            .map(|v| v.as_slice())\n            .collect();\n\n        Self { ring, registered_buffers }\n    }\n\n    pub fn read_zero_copy(&mut self, fd: RawFd, buf_idx: usize) {\n        let buf = &self.registered_buffers[buf_idx];\n\n        let read_op = opcode::Read::new(\n            types::Fd(fd),\n            buf.as_ptr() as *mut u8,\n            buf.len() as u32\n        )\n        .build();\n\n        unsafe {\n            self.ring.submission().push(&read_op).unwrap();\n        }\n    }\n}\n```",
      "terminal_task": {
        "command": "cargo test ring_buffer -- --nocapture",
        "expected_output": "test ring_buffer::tests::test_mpsc ... ok",
        "hint": "Utilisez AtomicU64 pour head/tail avec Ordering::Acquire/Release, MaybeUninit pour les slots non initialisés, et wrapping_add/sub pour éviter l'overflow"
      }
    },
    {
      "id": "sys-003-lock-free",
      "track": "SYSTEM",
      "level": "Expert",
      "title": "Structures de Données Lock-Free",
      "duration": "10h00",
      "description": "Queues MPSC/MPMC lock-free, hazard pointers, RCU, et algorithmes de consensus distribué.",
      "markdown_content": "# Structures de Données Lock-Free\n\n## Memory Ordering Deep Dive\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│              MEMORY ORDERING HIERARCHY                      │\n├─────────────────────────────────────────────────────────────┤\n│  Relaxed  ◄──►  Acquire/Release  ◄──►  SeqCst              │\n│     │                │                    │                 │\n│  Aucune         Synchronise          Ordre total            │\n│  garantie       lecture/écriture     global                 │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Treiber Stack (Lock-Free)\n\n```rust\nuse std::sync::atomic::{AtomicPtr, Ordering};\nuse std::ptr;\n\npub struct TreiberStack<T> {\n    head: AtomicPtr<Node<T>>,\n}\n\nstruct Node<T> {\n    data: T,\n    next: *mut Node<T>,\n}\n\nimpl<T> TreiberStack<T> {\n    pub fn new() -> Self {\n        Self {\n            head: AtomicPtr::new(ptr::null_mut()),\n        }\n    }\n\n    pub fn push(&self, data: T) {\n        let new_node = Box::into_raw(Box::new(Node {\n            data,\n            next: ptr::null_mut(),\n        }));\n\n        loop {\n            let head = self.head.load(Ordering::Acquire);\n            unsafe { (*new_node).next = head; }\n\n            match self.head.compare_exchange(\n                head,\n                new_node,\n                Ordering::Release,\n                Ordering::Acquire\n            ) {\n                Ok(_) => break,\n                Err(_) => continue,\n            }\n        }\n    }\n\n    pub fn pop(&self) -> Option<T> {\n        loop {\n            let head = self.head.load(Ordering::Acquire);\n            if head.is_null() {\n                return None;\n            }\n\n            let next = unsafe { (*head).next };\n\n            match self.head.compare_exchange(\n                head,\n                next,\n                Ordering::Release,\n                Ordering::Acquire\n            ) {\n                Ok(_) => unsafe {\n                    let node = Box::from_raw(head);\n                    return Some(node.data);\n                },\n                Err(_) => continue,\n            }\n        }\n    }\n}\n```\n\n## Michael-Scott Queue (MPMC)\n\n```rust\nuse std::sync::atomic::{AtomicPtr, Ordering};\nuse std::ptr;\n\npub struct MSQueue<T> {\n    head: AtomicPtr<Node<T>>,\n    tail: AtomicPtr<Node<T>>,\n}\n\nstruct Node<T> {\n    data: T,\n    next: AtomicPtr<Node<T>>,\n}\n\nimpl<T> MSQueue<T> {\n    pub fn new() -> Self {\n        let dummy = Box::into_raw(Box::new(Node {\n            data: unsafe { std::mem::zeroed() },\n            next: AtomicPtr::new(ptr::null_mut()),\n        }));\n\n        Self {\n            head: AtomicPtr::new(dummy),\n            tail: AtomicPtr::new(dummy),\n        }\n    }\n\n    pub fn enqueue(&self, data: T) {\n        let new_node = Box::into_raw(Box::new(Node {\n            data,\n            next: AtomicPtr::new(ptr::null_mut()),\n        }));\n\n        loop {\n            let tail = self.tail.load(Ordering::Acquire);\n            let next = unsafe { (*tail).next.load(Ordering::Acquire) };\n\n            // Vérifier si tail est encore valide\n            if tail != self.tail.load(Ordering::Acquire) {\n                continue;\n            }\n\n            if next.is_null() {\n                // Essayer de lier le nouveau noeud\n                match unsafe { (*tail).next.compare_exchange(\n                    ptr::null_mut(),\n                    new_node,\n                    Ordering::Release,\n                    Ordering::Acquire\n                )} {\n                    Ok(_) => {\n                        // Avancer tail\n                        let _ = self.tail.compare_exchange(\n                            tail,\n                            new_node,\n                            Ordering::Release,\n                            Ordering::Acquire\n                        );\n                        return;\n                    }\n                    Err(_) => continue,\n                }\n            } else {\n                // Tail est en retard, l'avancer\n                let _ = self.tail.compare_exchange(\n                    tail,\n                    next,\n                    Ordering::Release,\n                    Ordering::Acquire\n                );\n            }\n        }\n    }\n}\n```",
      "terminal_task": {
        "command": "cargo test michael_scott_queue --release -- --nocapture",
        "expected_output": "test michael_scott_queue::tests::test_mpmc ... ok",
        "hint": "Utilisez un noeud dummy, compare_exchange sur tail.next puis sur tail, et gérez le cas où tail est en retard avec un retry"
      }
    },
    {
      "id": "sys-004-ffi-bindgen",
      "track": "SYSTEM",
      "level": "Expert",
      "title": "FFI Avancé et Bindgen Automatisé",
      "duration": "7h15",
      "description": "Création de bindings FFI sécurisés, utilisation de bindgen, et patterns de sécurité mémoire.",
      "markdown_content": "# FFI Avancé et Bindgen\n\n## Architecture FFI Sécurisé\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    FFI BOUNDARY LAYER                       │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  Rust Safe API          FFI Boundary          C Library     │\n│  ┌─────────────┐       ┌─────────────┐       ┌─────────┐   │\n│  │  Structs    │◄─────▶│   -sys      │◄─────▶│  C API  │   │\n│  │  Methods    │       │  crate      │       │         │   │\n│  │  Iterators  │       │             │       │         │   │\n│  └─────────────┘       └─────────────┘       └─────────┘   │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Bindgen Configuration\n\n```toml\n# build.rs\nuse std::env;\nuse std::path::PathBuf;\n\nfn main() {\n    println!(\"cargo:rustc-link-lib=mylib\");\n\n    let bindings = bindgen::Builder::default()\n        .header(\"wrapper.h\")\n        .parse_callbacks(Box::new(bindgen::CargoCallbacks))\n        .whitelist_function(\"mylib_.*\")\n        .whitelist_type(\"mylib_.*\")\n        .whitelist_var(\"MYLIB_.*\")\n        .derive_default(true)\n        .derive_debug(true)\n        .impl_debug(true)\n        .generate()\n        .expect(\"Unable to generate bindings\");\n\n    let out_path = PathBuf::from(env::var(\"OUT_DIR\").unwrap());\n    bindings\n        .write_to_file(out_path.join(\"bindings.rs\"))\n        .expect(\"Couldn't write bindings\");\n}\n```\n\n## Safe Wrapper Pattern\n\n```rust\n// sys crate (unsafe)\npub mod ffi {\n    #![allow(non_upper_case_globals)]\n    #![allow(non_camel_case_types)]\n    #![allow(non_snake_case)]\n    include!(concat!(env!(\"OUT_DIR\"), \"/bindings.rs\"));\n}\n\n// Safe wrapper crate\npub struct Database {\n    ptr: *mut ffi::mylib_db_t,\n}\n\nunsafe impl Send for Database {}\nunsafe impl Sync for Database {}\n\nimpl Database {\n    pub fn new(path: &str) -> Result<Self, Error> {\n        let c_path = CString::new(path)?;\n        let mut ptr = ptr::null_mut();\n\n        let rc = unsafe {\n            ffi::mylib_db_open(c_path.as_ptr(), &mut ptr)\n        };\n\n        if rc != 0 {\n            return Err(Error::OpenFailed(rc));\n        }\n\n        Ok(Self { ptr })\n    }\n\n    pub fn query(&self, sql: &str) -> Result<Rows, Error> {\n        let c_sql = CString::new(sql)?;\n        let mut stmt = ptr::null_mut();\n\n        let rc = unsafe {\n            ffi::mylib_prepare(self.ptr, c_sql.as_ptr(), &mut stmt)\n        };\n\n        if rc != 0 {\n            return Err(Error::QueryFailed(rc));\n        }\n\n        Ok(Rows { stmt })\n    }\n}\n\nimpl Drop for Database {\n    fn drop(&mut self) {\n        unsafe {\n            ffi::mylib_db_close(self.ptr);\n        }\n    }\n}\n```",
      "terminal_task": {
        "command": "cargo test ffi_wrapper -- --nocapture",
        "expected_output": "test ffi_wrapper::tests::test_safe_api ... ok",
        "hint": "Utilisez bindgen pour générer les bindings, créez un crate -sys séparé, et implémentez Drop pour la libération automatique"
      }
    },
    {
      "id": "sys-005-async-runtime",
      "track": "SYSTEM",
      "level": "Expert",
      "title": "Runtime Asynchrone: Construire Tokio from Scratch",
      "duration": "12h00",
      "description": "Implémentation d'un runtime async from scratch, executors, wakers, et polling.",
      "markdown_content": "# Runtime Asynchrone From Scratch\n\n## Architecture d'un Runtime Async\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    ASYNC RUNTIME                            │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  ┌─────────────┐    ┌─────────────┐    ┌─────────────────┐ │\n│  │   Executor  │    │   Reactor   │    │    Timer Heap   │ │\n│  │  (task Q)   │◄──►│  (I/O epoll)│◄──►│  (sleep/delay)  │ │\n│  └──────┬──────┘    └─────────────┘    └─────────────────┘ │\n│         │                                                   │\n│         ▼                                                   │\n│  ┌─────────────────────────────────────────────────────┐   │\n│  │              Worker Threads Pool                    │   │\n│  │  ┌─────┐  ┌─────┐  ┌─────┐  ┌─────┐  ┌─────┐      │   │\n│  │  │ W1  │  │ W2  │  │ W3  │  │ W4  │  │ Wn  │      │   │\n│  │  └─────┘  └─────┘  └─────┘  └─────┘  └─────┘      │   │\n│  └─────────────────────────────────────────────────────┘   │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Task et Waker\n\n```rust\nuse std::future::Future;\nuse std::pin::Pin;\nuse std::sync::{Arc, Mutex};\nuse std::task::{Context, Poll, RawWaker, RawWakerVTable, Waker};\n\npub struct Task {\n    future: Mutex<Pin<Box<dyn Future<Output = ()> + Send>>>,\n    state: AtomicU8,\n}\n\nimpl Task {\n    pub fn new<F>(future: F) -> Arc<Self>\n    where\n        F: Future<Output = ()> + Send + 'static,\n    {\n        Arc::new(Self {\n            future: Mutex::new(Box::pin(future)),\n            state: AtomicU8::new(STATE_IDLE),\n        })\n    }\n\n    pub fn poll(self: Arc<Self>) -> Poll<()> {\n        let waker = create_waker(self.clone());\n        let mut context = Context::from_waker(&waker);\n\n        let mut future = self.future.lock().unwrap();\n        future.as_mut().poll(&mut context)\n    }\n}\n\nfn create_waker(task: Arc<Task>) -> Waker {\n    let raw = Arc::into_raw(task) as *const ();\n\n    unsafe fn clone_waker(data: *const ()) -> RawWaker {\n        let arc = Arc::from_raw(data as *const Task);\n        let _ = Arc::clone(&arc);\n        RawWaker::new(data, &VTABLE)\n    }\n\n    unsafe fn wake(data: *const ()) {\n        let arc = Arc::from_raw(data as *const Task);\n        // Ajouter à la queue d'exécution\n        EXECUTOR.spawn_task(arc);\n    }\n\n    static VTABLE: RawWakerVTable = RawWakerVTable::new(\n        clone_waker,\n        wake,\n        wake_by_ref,\n        drop_waker,\n    );\n\n    unsafe { Waker::from_raw(RawWaker::new(raw, &VTABLE)) }\n}\n```\n\n## Executor Simple\n\n```rust\nuse crossbeam::channel::{unbounded, Sender, Receiver};\n\npub struct Executor {\n    task_queue: Sender<Arc<Task>>,\n    task_receiver: Receiver<Arc<Task>>,\n}\n\nimpl Executor {\n    pub fn new() -> Self {\n        let (tx, rx) = unbounded();\n        Self {\n            task_queue: tx,\n            task_receiver: rx,\n        }\n    }\n\n    pub fn spawn<F>(&self, future: F)\n    where\n        F: Future<Output = ()> + Send + 'static,\n    {\n        let task = Task::new(future);\n        self.task_queue.send(task).unwrap();\n    }\n\n    pub fn run(&self) {\n        while let Ok(task) = self.task_receiver.recv() {\n            match task.poll() {\n                Poll::Ready(()) => {}\n                Poll::Pending => {\n                    // La tâche s'est réenregistrée via le waker\n                }\n            }\n        }\n    }\n}\n```",
      "terminal_task": {
        "command": "cargo test mini_tokio -- --nocapture",
        "expected_output": "test mini_tokio::tests::test_executor ... ok",
        "hint": "Implémentez RawWakerVTable avec clone/wake/wake_by_ref/drop, utilisez Arc pour le partage, et un channel MPSC pour la task queue"
      }
    },
    {
      "id": "sys-006-proc-macros",
      "track": "SYSTEM",
      "level": "Expert",
      "title": "Procedural Macros: DSL et Code Generation",
      "duration": "8h00",
      "description": "Création de macros procédurales dérivées, attributs, et fonction pour DSL.",
      "markdown_content": "# Procedural Macros\n\n## Types de Proc Macros\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                 PROCEDURAL MACROS                           │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  #[derive(MyTrait)]        #[my_attr]        my_macro!{}   │\n│       │                         │                 │        │\n│       ▼                         ▼                 ▼        │\n│  ┌─────────┐              ┌─────────┐       ┌─────────┐    │\n│  │ Derive  │              │ Attrib  │       │  Func   │    │\n│  │ Macro   │              │ Macro   │       │  Macro  │    │\n│  └─────────┘              └─────────┘       └─────────┘    │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Derive Macro\n\n```rust\n// my-derive/src/lib.rs\nuse proc_macro::TokenStream;\nuse quote::quote;\nuse syn::{parse_macro_input, DeriveInput};\n\n#[proc_macro_derive(Builder, attributes(builder))]\npub fn derive_builder(input: TokenStream) -> TokenStream {\n    let input = parse_macro_input!(input as DeriveInput);\n\n    let name = input.ident;\n    let builder_name = quote::format_ident!(\"{}Builder\", name);\n\n    let fields = match input.data {\n        syn::Data::Struct(s) => s.fields,\n        _ => panic!(\"Builder only works on structs\"),\n    };\n\n    let builder_fields = fields.iter().map(|f| {\n        let name = &f.ident;\n        let ty = &f.ty;\n        quote! { #name: Option<#ty> }\n    });\n\n    let builder_methods = fields.iter().map(|f| {\n        let name = &f.ident;\n        let ty = &f.ty;\n        quote! {\n            pub fn #name(mut self, value: #ty) -> Self {\n                self.#name = Some(value);\n                self\n            }\n        }\n    });\n\n    let build_fields = fields.iter().map(|f| {\n        let name = &f.ident;\n        quote! {\n            #name: self.#name.ok_or(concat!(stringify!(#name), \" is required\"))?\n        }\n    });\n\n    let expanded = quote! {\n        pub struct #builder_name {\n            #(#builder_fields,)*\n        }\n\n        impl #builder_name {\n            pub fn new() -> Self {\n                Self {\n                    #(#name: None,)*\n                }\n            }\n\n            #(#builder_methods)*\n\n            pub fn build(self) -> Result<#name, &'static str> {\n                Ok(#name {\n                    #(#build_fields,)*\n                })\n            }\n        }\n\n        impl #name {\n            pub fn builder() -> #builder_name {\n                #builder_name::new()\n            }\n        }\n    };\n\n    TokenStream::from(expanded)\n}\n```\n\n## Attribut Macro\n\n```rust\n#[proc_macro_attribute]\npub fn trace(attr: TokenStream, item: TokenStream) -> TokenStream {\n    let input = parse_macro_input!(item as syn::ItemFn);\n\n    let vis = &input.vis;\n    let sig = &input.sig;\n    let block = &input.block;\n    let fn_name = &sig.ident;\n\n    let expanded = quote! {\n        #vis #sig {\n            println!(\"[TRACE] Entering: {}\", stringify!(#fn_name));\n            let start = std::time::Instant::now();\n\n            let result = #block;\n\n            println!(\"[TRACE] Exiting: {} (took {:?})\", \n                stringify!(#fn_name), \n                start.elapsed()\n            );\n            result\n        }\n    };\n\n    TokenStream::from(expanded)\n}\n```",
      "terminal_task": {
        "command": "cargo test proc_macro -- --nocapture",
        "expected_output": "test proc_macro::tests::test_derive_builder ... ok",
        "hint": "Utilisez syn pour parser le TokenStream, quote! pour générer du code, et proc_macro2 pour les tests"
      }
    },
    {
      "id": "sys-007-mmap-shared",
      "track": "SYSTEM",
      "level": "Expert",
      "title": "Mémoire Partagée et mmap",
      "duration": "6h30",
      "description": "Memory-mapped files, shared memory IPC, et synchronisation entre processus.",
      "markdown_content": "# Mémoire Partagée et mmap\n\n## Architecture Shared Memory\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    SHARED MEMORY                            │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  Process A                    Process B                     │\n│  ┌─────────────┐              ┌─────────────┐              │\n│  │  Virtual    │              │  Virtual    │              │\n│  │  Address    │              │  Address    │              │\n│  │  Space      │              │  Space      │              │\n│  └──────┬──────┘              └──────┬──────┘              │\n│         │                            │                      │\n│         └────────────┬───────────────┘                      │\n│                      │                                      │\n│                      ▼                                      │\n│         ┌─────────────────────┐                            │\n│         │   Physical Memory   │                            │\n│         │   (Shared Page)     │                            │\n│         └─────────────────────┘                            │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Mmap en Rust\n\n```rust\nuse memmap2::{MmapMut, MmapOptions};\nuse std::fs::OpenOptions;\nuse std::sync::atomic::{AtomicU64, Ordering};\n\npub struct SharedRingBuffer {\n    mmap: MmapMut,\n    capacity: usize,\n}\n\n#[repr(C)]\nstruct Header {\n    write_pos: AtomicU64,\n    read_pos: AtomicU64,\n}\n\nimpl SharedRingBuffer {\n    pub fn create(path: &str, capacity: usize) -> Result<Self, std::io::Error> {\n        let file = OpenOptions::new()\n            .read(true)\n            .write(true)\n            .create(true)\n            .open(path)?;\n\n        let header_size = std::mem::size_of::<Header>();\n        let total_size = header_size + capacity;\n\n        file.set_len(total_size as u64)?;\n\n        let mut mmap = unsafe { MmapMut::map_mut(&file)? };\n\n        // Initialiser le header\n        let header = unsafe { \n            &mut *(mmap.as_mut_ptr() as *mut Header) \n        };\n        header.write_pos.store(0, Ordering::Relaxed);\n        header.read_pos.store(0, Ordering::Relaxed);\n\n        Ok(Self { mmap, capacity })\n    }\n\n    pub fn write(&mut self, data: &[u8]) -> Result<usize, &'static str> {\n        let header = unsafe { \n            &*(self.mmap.as_ptr() as *const Header) \n        };\n\n        let write_pos = header.write_pos.load(Ordering::Acquire);\n        let read_pos = header.read_pos.load(Ordering::Acquire);\n\n        let available = self.capacity - (write_pos as usize - read_pos as usize);\n\n        if data.len() > available {\n            return Err(\"Buffer full\");\n        }\n\n        let header_size = std::mem::size_of::<Header>();\n        let offset = header_size + (write_pos as usize % self.capacity);\n\n        unsafe {\n            let ptr = self.mmap.as_mut_ptr().add(offset);\n            std::ptr::copy_nonoverlapping(\n                data.as_ptr(),\n                ptr,\n                data.len()\n            );\n        }\n\n        header.write_pos.store(\n            write_pos + data.len() as u64, \n            Ordering::Release\n        );\n\n        Ok(data.len())\n    }\n\n    pub fn read(&self, buf: &mut [u8]) -> Result<usize, &'static str> {\n        let header = unsafe { \n            &*(self.mmap.as_ptr() as *const Header) \n        };\n\n        let write_pos = header.write_pos.load(Ordering::Acquire);\n        let read_pos = header.read_pos.load(Ordering::Acquire);\n\n        let available = (write_pos - read_pos) as usize;\n        let to_read = buf.len().min(available);\n\n        if to_read == 0 {\n            return Ok(0);\n        }\n\n        let header_size = std::mem::size_of::<Header>();\n        let offset = header_size + (read_pos as usize % self.capacity);\n\n        unsafe {\n            let ptr = self.mmap.as_ptr().add(offset);\n            std::ptr::copy_nonoverlapping(\n                ptr,\n                buf.as_mut_ptr(),\n                to_read\n            );\n        }\n\n        header.read_pos.store(\n            read_pos + to_read as u64,\n            Ordering::Release\n        );\n\n        Ok(to_read)\n    }\n}\n```",
      "terminal_task": {
        "command": "cargo test shared_memory -- --nocapture",
        "expected_output": "test shared_memory::tests::test_mmap_ring_buffer ... ok",
        "hint": "Utilisez memmap2 pour le mapping, AtomicU64 pour la synchronisation cross-process, et copy_nonoverlapping pour les transferts"
      }
    },
    {
      "id": "sys-008-simd-optimization",
      "track": "SYSTEM",
      "level": "Expert",
      "title": "SIMD et Optimisations CPU",
      "duration": "7h00",
      "description": "Vectorisation SIMD avec std::simd, autovectorisation, et profilage.",
      "markdown_content": "# SIMD et Optimisations CPU\n\n## SIMD Architecture\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    SIMD REGISTERS                           │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  SCALAR (1 op):          SIMD (16 ops):                    │\n│  ┌─────┐                 ┌─────────────────────────────┐   │\n│  │ f32 │                 │ f32 │ f32 │ ... │ f32 │ f32 │   │\n│  └─────┘                 └─────────────────────────────┘   │\n│    32 bits                      512 bits (AVX-512)         │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## std::simd\n\n```rust\n#![feature(portable_simd)]\nuse std::simd::{Simd, SimdFloat};\n\npub fn simd_dot_product(a: &[f32], b: &[f32]) -> f32 {\n    const LANES: usize = 16; // AVX-512\n\n    let mut sum = Simd::<f32, LANES>::splat(0.0);\n\n    // Chunk principal\n    let chunks = a.len() / LANES;\n    for i in 0..chunks {\n        let va = Simd::<f32, LANES>::from_slice(&a[i * LANES..]);\n        let vb = Simd::<f32, LANES>::from_slice(&b[i * LANES..]);\n        sum += va * vb;\n    }\n\n    // Réduction\n    let mut total = sum.reduce_sum();\n\n    // Reste scalaire\n    for i in (chunks * LANES)..a.len() {\n        total += a[i] * b[i];\n    }\n\n    total\n}\n\npub fn simd_matrix_multiply(\n    a: &[f32], \n    b: &[f32], \n    c: &mut [f32],\n    n: usize\n) {\n    const LANES: usize = 8; // AVX2\n\n    for i in 0..n {\n        for j in (0..n).step_by(LANES) {\n            let mut sum = Simd::<f32, LANES>::splat(0.0);\n\n            for k in 0..n {\n                let a_val = Simd::<f32, LANES>::splat(a[i * n + k]);\n                let b_vec = Simd::<f32, LANES>::from_slice(\n                    &b[k * n + j..]\n                );\n                sum += a_val * b_vec;\n            }\n\n            sum.copy_to_slice(&mut c[i * n + j..]);\n        }\n    }\n}\n```\n\n## Intrinseques AVX-512\n\n```rust\n#[cfg(target_arch = \"x86_64\")]\nuse std::arch::x86_64::*;\n\n#[target_feature(enable = \"avx512f\")]\nunsafe fn avx512_sum(arr: &[f32]) -> f32 {\n    let mut sum = _mm512_set1_ps(0.0);\n\n    let chunks = arr.len() / 16;\n    for i in 0..chunks {\n        let vec = _mm512_loadu_ps(arr.as_ptr().add(i * 16));\n        sum = _mm512_add_ps(sum, vec);\n    }\n\n    _mm512_reduce_add_ps(sum)\n}\n\npub fn safe_avx512_sum(arr: &[f32]) -> f32 {\n    if is_x86_feature_detected!(\"avx512f\") {\n        unsafe { avx512_sum(arr) }\n    } else {\n        arr.iter().sum()\n    }\n}\n```",
      "terminal_task": {
        "command": "cargo test simd --release -- --nocapture",
        "expected_output": "test simd::tests::test_dot_product ... ok",
        "hint": "Utilisez std::simd pour la portabilité, vérifiez les features CPU avec is_x86_feature_detected, et traitez le reste scalaire"
      }
    },
    {
      "id": "sys-009-ebpf-kernel",
      "track": "SYSTEM",
      "level": "Expert",
      "title": "eBPF: Programmation Kernel-Space",
      "duration": "9h30",
      "description": "Écriture de programmes eBPF, instrumentation du kernel, et sécurité.",
      "markdown_content": "# eBPF: Programmation Kernel-Space\n\n## Architecture eBPF\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    eBPF ARCHITECTURE                        │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  User Space                    Kernel Space                 │\n│  ┌─────────────┐              ┌─────────────────────┐      │\n│  │  Compiler   │─────────────▶│  Verifier (safety)  │      │\n│  │  (LLVM)     │              └──────────┬──────────┘      │\n│  └─────────────┘                         │                   │\n│                                          ▼                   │\n│                              ┌─────────────────────┐        │\n│                              │  JIT Compiler       │        │\n│                              │  (x86/ARM64)        │        │\n│                              └──────────┬──────────┘        │\n│                                         │                    │\n│                              ┌──────────▼──────────┐        │\n│                              │  BPF VM / Maps      │        │\n│                              └─────────────────────┘        │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Programme eBPF avec Aya\n\n```rust\n// ebpf-program/src/main.rs\n#![no_std]\n#![no_main]\n\nuse aya_ebpf::{macros::tracepoint, programs::TracePointContext};\nuse aya_ebpf::maps::HashMap;\nuse aya_ebpf::helpers::gen::bpf_probe_read_user;\n\n#[map(name = \"SYSCALL_COUNTS\")]\nstatic SYSCALL_COUNTS: HashMap<u64, u64> = HashMap::with_max_entries(1024, 0);\n\n#[tracepoint]\npub fn trace_sys_enter(ctx: TracePointContext) -> u32 {\n    match try_trace_sys_enter(ctx) {\n        Ok(ret) => ret,\n        Err(ret) => ret,\n    }\n}\n\nfn try_trace_sys_enter(ctx: TracePointContext) -> Result<u32, u32> {\n    // ID du syscall\n    let syscall_id: u64 = unsafe {\n        ctx.read_at::<u64>(16).map_err(|e| e as u32)?\n    };\n\n    // Incrémenter le compteur\n    let count = unsafe {\n        SYSCALL_COUNTS.get(&syscall_id).unwrap_or(&0) + 1\n    };\n\n    unsafe {\n        SYSCALL_COUNTS.insert(&syscall_id, &count, 0)\n            .map_err(|e| e as u32)?;\n    }\n\n    Ok(0)\n}\n\n#[panic_handler]\nfn panic(_info: &core::panic::PanicInfo) -> ! {\n    unsafe { core::hint::unreachable_unchecked() }\n}\n```\n\n## User-Space Loader\n\n```rust\n// loader/src/main.rs\nuse aya::{include_bytes_aligned, maps::HashMap, Ebpf};\nuse aya::programs::TracePoint;\nuse std::time::Duration;\nuse tokio::time::sleep;\n\n#[tokio::main]\nasync fn main() -> Result<(), Box<dyn std::error::Error>> {\n    #[cfg(debug_assertions)]\n    let mut bpf = Ebpf::load(include_bytes_aligned!(\n        \"../../target/bpfel-unknown-none/debug/ebpf-program\"\n    ))?;\n\n    let program: &mut TracePoint = bpf\n        .program_mut(\"trace_sys_enter\")\n        .unwrap()\n        .try_into()?;\n\n    program.load()?;\n    program.attach(\"syscalls\", \"sys_enter\")?;\n\n    // Lire les statistiques\n    let syscall_counts: HashMap<_, u64, u64> = \n        HashMap::try_from(bpf.map_mut(\"SYSCALL_COUNTS\").unwrap())?;\n\n    loop {\n        sleep(Duration::from_secs(1)).await;\n\n        for item in syscall_counts.iter() {\n            let (syscall_id, count) = item?;\n            println!(\"Syscall {}: {} calls\", syscall_id, count);\n        }\n    }\n}\n```",
      "terminal_task": {
        "command": "cargo xtask run-ebpf -- --nocapture",
        "expected_output": "Syscall 0: 1234 calls",
        "hint": "Utilisez aya pour le loader, aya-ebpf pour le programme kernel, et vérifiez les capacités CAP_BPF"
      }
    },
    {
      "id": "sys-010-wasm-runtime",
      "track": "SYSTEM",
      "level": "Expert",
      "title": "Runtime WebAssembly: Créer un Mini-Wasmtime",
      "duration": "10h00",
      "description": "Implémentation d'un runtime WASM, parsing des modules, et exécution.",
      "markdown_content": "# Runtime WebAssembly From Scratch\n\n## Architecture WASM Runtime\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    WASM RUNTIME                             │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  ┌─────────────┐    ┌─────────────┐    ┌─────────────────┐ │\n│  │   Parser    │───▶│  Validator  │───▶│   Compiler      │ │\n│  │  (LEB128)   │    │  (typeck)   │    │  (to native)    │ │\n│  └─────────────┘    └─────────────┘    └────────┬────────┘ │\n│                                                  │          │\n│  ┌─────────────┐    ┌─────────────┐    ┌────────▼────────┐ │\n│  │   Store     │◄───│   Stack     │◄───│    VM Loop      │ │\n│  │ (instances) │    │ (values)    │    │  (dispatch)     │ │\n│  └─────────────┘    └─────────────┘    └─────────────────┘ │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Parser WASM\n\n```rust\nuse nom::{\n    bytes::complete::{tag, take},\n    combinator::{map, opt},\n    multi::{many0, many_m_n},\n    number::complete::{le_u32, le_u64, le_f32, le_f64},\n    IResult,\n};\n\n#[derive(Debug, Clone)]\npub struct WasmModule {\n    pub version: u32,\n    pub types: Vec<FuncType>,\n    pub funcs: Vec<Func>,\n    pub exports: Vec<Export>,\n    pub code: Vec<Code>,\n}\n\n#[derive(Debug, Clone)]\npub struct FuncType {\n    pub params: Vec<ValType>,\n    pub results: Vec<ValType>,\n}\n\n#[derive(Debug, Clone, Copy)]\npub enum ValType {\n    I32,\n    I64,\n    F32,\n    F64,\n}\n\npub fn parse_module(input: &[u8]) -> IResult<&[u8], WasmModule> {\n    let (input, _) = tag(b\"\\x00asm\")(input)?;\n    let (input, version) = le_u32(input)?;\n\n    let (input, sections) = many0(parse_section)(input)?;\n\n    // Construire le module à partir des sections\n    let mut module = WasmModule {\n        version,\n        types: vec![],\n        funcs: vec![],\n        exports: vec![],\n        code: vec![],\n    };\n\n    for section in sections {\n        match section {\n            Section::Type(types) => module.types = types,\n            Section::Function(funcs) => module.funcs = funcs,\n            Section::Export(exports) => module.exports = exports,\n            Section::Code(code) => module.code = code,\n            _ => {}\n        }\n    }\n\n    Ok((input, module))\n}\n\nfn parse_section(input: &[u8]) -> IResult<&[u8], Section> {\n    let (input, id) = le_u32(input)?;\n    let (input, size) = le_u32(input)?;\n    let (input, content) = take(size)(input)?;\n\n    let (_, section) = match id {\n        1 => map(parse_type_section, Section::Type)(content)?,\n        3 => map(parse_func_section, Section::Function)(content)?,\n        7 => map(parse_export_section, Section::Export)(content)?,\n        10 => map(parse_code_section, Section::Code)(content)?,\n        _ => (content, Section::Custom),\n    };\n\n    Ok((input, section))\n}\n\nfn parse_type_section(input: &[u8]) -> IResult<&[u8], Vec<FuncType>> {\n    let (input, count) = le_u32(input)?;\n    many_m_n(count as usize, count as usize, parse_functype)(input)\n}\n\nfn parse_functype(input: &[u8]) -> IResult<&[u8], FuncType> {\n    let (input, _) = tag(&[0x60])(input)?;\n    let (input, params) = parse_vec(parse_valtype)(input)?;\n    let (input, results) = parse_vec(parse_valtype)(input)?;\n\n    Ok((input, FuncType { params, results }))\n}\n\nfn parse_valtype(input: &[u8]) -> IResult<&[u8], ValType> {\n    use nom::branch::alt;\n    alt((\n        map(tag(&[0x7F]), |_| ValType::I32),\n        map(tag(&[0x7E]), |_| ValType::I64),\n        map(tag(&[0x7D]), |_| ValType::F32),\n        map(tag(&[0x7C]), |_| ValType::F64),\n    ))(input)\n}\n```\n\n## VM d'Exécution\n\n```rust\npub struct VM {\n    stack: Vec<Value>,\n    locals: Vec<Value>,\n    memory: Vec<u8>,\n}\n\n#[derive(Debug, Clone, Copy)]\npub enum Value {\n    I32(i32),\n    I64(i64),\n    F32(f32),\n    F64(f64),\n}\n\nimpl VM {\n    pub fn new(memory_size: usize) -> Self {\n        Self {\n            stack: vec![],\n            locals: vec![],\n            memory: vec![0; memory_size],\n        }\n    }\n\n    pub fn execute(&mut self, instructions: &[Instruction]) -> Result<(), Trap> {\n        use Instruction::*;\n\n        for instr in instructions {\n            match instr {\n                I32Const(val) => self.push(Value::I32(*val)),\n                I32Add => {\n                    let b = self.pop_i32()?;\n                    let a = self.pop_i32()?;\n                    self.push(Value::I32(a.wrapping_add(b)));\n                }\n                I32Sub => {\n                    let b = self.pop_i32()?;\n                    let a = self.pop_i32()?;\n                    self.push(Value::I32(a.wrapping_sub(b)));\n                }\n                LocalGet(idx) => {\n                    let val = self.locals.get(*idx as usize)\n                        .ok_or(Trap::LocalOutOfBounds)?;\n                    self.push(*val);\n                }\n                LocalSet(idx) => {\n                    let val = self.pop()?;\n                    if *idx as usize >= self.locals.len() {\n                        self.locals.resize((*idx + 1) as usize, Value::I32(0));\n                    }\n                    self.locals[*idx as usize] = val;\n                }\n                I32Load(align, offset) => {\n                    let addr = self.pop_i32()? as usize + *offset as usize;\n                    let val = self.load_i32(addr, *align)?;\n                    self.push(Value::I32(val));\n                }\n                I32Store(align, offset) => {\n                    let val = self.pop_i32()?;\n                    let addr = self.pop_i32()? as usize + *offset as usize;\n                    self.store_i32(addr, val, *align)?;\n                }\n                _ => return Err(Trap::Unimplemented),\n            }\n        }\n\n        Ok(())\n    }\n\n    fn push(&mut self, val: Value) {\n        self.stack.push(val);\n    }\n\n    fn pop(&mut self) -> Result<Value, Trap> {\n        self.stack.pop().ok_or(Trap::StackUnderflow)\n    }\n\n    fn pop_i32(&mut self) -> Result<i32, Trap> {\n        match self.pop()? {\n            Value::I32(v) => Ok(v),\n            _ => Err(Trap::TypeMismatch),\n        }\n    }\n\n    fn load_i32(&self, addr: usize, _align: u32) -> Result<i32, Trap> {\n        if addr + 4 > self.memory.len() {\n            return Err(Trap::MemoryOutOfBounds);\n        }\n        let bytes = &self.memory[addr..addr+4];\n        Ok(i32::from_le_bytes([bytes[0], bytes[1], bytes[2], bytes[3]]))\n    }\n\n    fn store_i32(&mut self, addr: usize, val: i32, _align: u32) -> Result<(), Trap> {\n        if addr + 4 > self.memory.len() {\n            return Err(Trap::MemoryOutOfBounds);\n        }\n        let bytes = val.to_le_bytes();\n        self.memory[addr..addr+4].copy_from_slice(&bytes);\n        Ok(())\n    }\n}\n\n#[derive(Debug)]\npub enum Trap {\n    StackUnderflow,\n    TypeMismatch,\n    MemoryOutOfBounds,\n    LocalOutOfBounds,\n    Unimplemented,\n}\n```",
      "terminal_task": {
        "command": "cargo test wasm_runtime -- --nocapture",
        "expected_output": "test wasm_runtime::tests::test_add_i32 ... ok",
        "hint": "Implémentez LEB128 pour les entiers, un dispatch table pour les opcodes, et vérifiez les limites mémoire à chaque accès"
      }
    },
    {
      "id": "tauri-001-ipc-advanced",
      "track": "TAURI_V3",
      "level": "Expert",
      "title": "IPC Asynchrone: Channels et Streaming",
      "duration": "7h30",
      "description": "Communication bidirectionnelle avancée, streaming de données, channels multiplexés et gestion des backpressure.",
      "markdown_content": "# IPC Asynchrone Avancé\n\n## Architecture IPC Tauri v3\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                      FRONTEND (WebView)                     │\n│  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────┐  │\n│  │   invoke()  │  │  listen()   │  │   Event Emitter     │  │\n│  └──────┬──────┘  └──────┬──────┘  └──────────┬──────────┘  │\n└─────────┼────────────────┼────────────────────┼─────────────┘\n          │                │                    │\n          └────────────────┴────────────────────┘\n                              │\n                    ┌─────────┴─────────┐\n                    │   IPC BRIDGE      │\n                    │  (WebView2/WK)    │\n                    └─────────┬─────────┘\n                              │\n┌─────────────────────────────┼───────────────────────────────┐\n│                      BACKEND (Rust)                         │\n│  ┌─────────────┐  ┌─────────┴─────────┐  ┌────────────────┐ │\n│  │  Commands   │  │  Event Channel    │  │  State Manager │ │\n│  │  #[command] │  │  (broadcast)      │  │  (Managed)     │ │\n│  └─────────────┘  └───────────────────┘  └────────────────┘ │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Channel Bidirectionnel avec Backpressure\n\n```rust\nuse tauri::{AppHandle, Emitter, Listener};\nuse tokio::sync::{mpsc, broadcast};\nuse serde::{Serialize, Deserialize};\nuse std::sync::Arc;\nuse std::time::Duration;\n\n#[derive(Clone, Serialize, Deserialize, Debug)]\npub struct IPCMessage {\n    pub id: String,\n    pub payload: serde_json::Value,\n    pub timestamp: u64,\n}\n\npub struct IPCChannel {\n    to_frontend: broadcast::Sender<IPCMessage>,\n    from_frontend: mpsc::Receiver<IPCMessage>,\n    app_handle: AppHandle,\n}\n\nimpl IPCChannel {\n    pub fn new(app_handle: AppHandle, buffer_size: usize) -> Self {\n        let (to_frontend, _) = broadcast::channel(buffer_size);\n        let (tx, from_frontend) = mpsc::channel(buffer_size);\n\n        // Écouter les messages du frontend\n        app_handle.listen(\"ipc:message\", move |event| {\n            if let Ok(msg) = serde_json::from_str::<IPCMessage>(&event.payload()) {\n                let _ = tx.try_send(msg);\n            }\n        });\n\n        Self {\n            to_frontend,\n            from_frontend,\n            app_handle,\n        }\n    }\n\n    pub async fn send_to_frontend(&self, msg: IPCMessage) -> Result<(), String> {\n        match self.to_frontend.send(msg) {\n            Ok(_) => Ok(()),\n            Err(_) => Err(\"Channel closed\".to_string()),\n        }\n    }\n\n    pub async fn recv_from_frontend(&mut self) -> Option<IPCMessage> {\n        self.from_frontend.recv().await\n    }\n}\n\n// Command Tauri avec streaming\n#[tauri::command]\npub async fn stream_data(\n    window: tauri::Window,\n    request: StreamRequest,\n) -> Result<(), String> {\n    let (tx, mut rx) = mpsc::channel::<DataChunk>(100);\n\n    // Spawn producer\n    tokio::spawn(async move {\n        for i in 0..request.total_chunks {\n            let chunk = DataChunk {\n                index: i,\n                data: vec![0u8; request.chunk_size],\n            };\n            if tx.send(chunk).await.is_err() {\n                break;\n            }\n            tokio::time::sleep(Duration::from_millis(10)).await;\n        }\n    });\n\n    // Stream to frontend\n    while let Some(chunk) = rx.recv().await {\n        window.emit(\"stream:chunk\", &chunk)\n            .map_err(|e| e.to_string())?;\n    }\n\n    window.emit(\"stream:complete\", ())\n        .map_err(|e| e.to_string())?;\n\n    Ok(())\n}\n```\n\n## Gestion d'État Partagée\n\n```rust\nuse tauri::State;\nuse std::sync::{Arc, RwLock};\nuse dashmap::DashMap;\n\npub struct AppState {\n    pub sessions: DashMap<String, Session>,\n    pub config: RwLock<AppConfig>,\n    pub metrics: Arc<RwLock<Metrics>>,\n}\n\n#[derive(Default)]\npub struct Session {\n    pub user_id: String,\n    pub connected_at: u64,\n    pub last_activity: u64,\n}\n\n#[derive(Default)]\npub struct AppConfig {\n    pub max_connections: usize,\n    pub timeout_seconds: u64,\n}\n\n#[derive(Default)]\npub struct Metrics {\n    pub total_requests: u64,\n    pub active_connections: usize,\n}\n\nimpl AppState {\n    pub fn new() -> Self {\n        Self {\n            sessions: DashMap::new(),\n            config: RwLock::new(AppConfig {\n                max_connections: 100,\n                timeout_seconds: 300,\n            }),\n            metrics: Arc::new(RwLock::new(Metrics::default())),\n        }\n    }\n}\n\n#[tauri::command]\npub fn create_session(\n    state: State<AppState>,\n    user_id: String,\n) -> Result<String, String> {\n    let session_id = uuid::Uuid::new_v4().to_string();\n\n    let config = state.config.read().unwrap();\n    if state.sessions.len() >= config.max_connections {\n        return Err(\"Max connections reached\".to_string());\n    }\n\n    let now = std::time::SystemTime::now()\n        .duration_since(std::time::UNIX_EPOCH)\n        .unwrap()\n        .as_secs();\n\n    state.sessions.insert(session_id.clone(), Session {\n        user_id,\n        connected_at: now,\n        last_activity: now,\n    });\n\n    state.metrics.write().unwrap().active_connections = state.sessions.len();\n\n    Ok(session_id)\n}\n```",
      "terminal_task": {
        "command": "cargo test ipc_channel -- --nocapture",
        "expected_output": "test ipc_channel::tests::test_backpressure ... ok",
        "hint": "Utilisez broadcast::channel pour le fan-out, mpsc::channel pour le fan-in, et implémentez un mécanisme de backpressure avec try_send et timeout"
      }
    },
    {
      "id": "tauri-002-sidecars",
      "track": "TAURI_V3",
      "level": "Expert",
      "title": "Sidecars et Custom Protocol Handlers",
      "duration": "6h00",
      "description": "Intégration de binaires externes, gestion de processus sidecar, et création de protocols URI personnalisés.",
      "markdown_content": "# Sidecars et Protocols\n\n## Architecture Sidecar\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    APPLICATION TAURI                        │\n│  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────┐  │\n│  │   Frontend  │  │   Core      │  │   Sidecar Manager   │  │\n│  │   (React)   │  │   (Rust)    │  │                     │  │\n│  └──────┬──────┘  └──────┬──────┘  └──────────┬──────────┘  │\n│         │                │                    │             │\n│         └────────────────┴────────────────────┘             │\n│                          │                                  │\n└──────────────────────────┼──────────────────────────────────┘\n                           │\n           ┌───────────────┼───────────────┐\n           ▼               ▼               ▼\n    ┌────────────┐  ┌────────────┐  ┌────────────┐\n    │  Python    │  │   Go       │  │   Node.js  │\n    │  ML Engine │  │  Service   │  │  Compiler  │\n    └────────────┘  └────────────┘  └────────────┘\n```\n\n## Sidecar Manager\n\n```rust\nuse tauri::api::process::{Command, CommandEvent};\nuse std::collections::HashMap;\nuse std::sync::{Arc, Mutex};\nuse tokio::sync::mpsc;\nuse uuid::Uuid;\n\npub struct SidecarManager {\n    processes: Arc<Mutex<HashMap<String, SidecarProcess>>>,\n}\n\npub struct SidecarProcess {\n    pub id: String,\n    pub command: Command,\n    pub stdout_rx: mpsc::Receiver<String>,\n    pub stderr_rx: mpsc::Receiver<String>,\n}\n\nimpl SidecarManager {\n    pub fn new() -> Self {\n        Self {\n            processes: Arc::new(Mutex::new(HashMap::new())),\n        }\n    }\n\n    pub async fn spawn(\n        &self,\n        name: &str,\n        args: &[&str],\n    ) -> Result<String, String> {\n        let (mut rx, child) = Command::new_sidecar(name)\n            .map_err(|e| e.to_string())?\n            .args(args)\n            .spawn()\n            .map_err(|e| e.to_string())?;\n\n        let id = format!(\"{}_{}\", name, Uuid::new_v4());\n        let (stdout_tx, stdout_rx) = mpsc::channel(100);\n        let (stderr_tx, stderr_rx) = mpsc::channel(100);\n\n        let process_id = id.clone();\n        let processes = self.processes.clone();\n\n        // Spawn listener\n        tokio::spawn(async move {\n            while let Some(event) = rx.recv().await {\n                match event {\n                    CommandEvent::Stdout(line) => {\n                        let _ = stdout_tx.send(line).await;\n                    }\n                    CommandEvent::Stderr(line) => {\n                        let _ = stderr_tx.send(line).await;\n                    }\n                    CommandEvent::Error(e) => {\n                        eprintln!(\"Sidecar error: {}\", e);\n                        break;\n                    }\n                    CommandEvent::Terminated(_) => {\n                        break;\n                    }\n                    _ => {}\n                }\n            }\n\n            // Cleanup\n            processes.lock().unwrap().remove(&process_id);\n        });\n\n        let process = SidecarProcess {\n            id: id.clone(),\n            command: child,\n            stdout_rx,\n            stderr_rx,\n        };\n\n        self.processes.lock().unwrap().insert(id.clone(), process);\n\n        Ok(id)\n    }\n\n    pub fn kill(&self, id: &str) -> Result<(), String> {\n        let mut processes = self.processes.lock().unwrap();\n        if let Some(process) = processes.get(id) {\n            process.command.kill()\n                .map_err(|e| e.to_string())?;\n            processes.remove(id);\n            Ok(())\n        } else {\n            Err(\"Process not found\".to_string())\n        }\n    }\n}\n```\n\n## Custom Protocol Handler\n\n```rust\nuse tauri::Manager;\nuse url::Url;\n\npub fn register_protocol<R: tauri::Runtime>(\n    app: &mut tauri::App<R>,\n) -> Result<(), Box<dyn std::error::Error>> {\n    app.deep_link().on_open_url(|event| {\n        for url in event.urls() {\n            handle_deep_link(url);\n        }\n    });\n\n    Ok(())\n}\n\nfn handle_deep_link(url: &Url) {\n    match url.host_str() {\n        Some(\"open\") => {\n            let path = url.path();\n            // Ouvrir le fichier\n        }\n        Some(\"auth\") => {\n            let code = url.query_pairs()\n                .find(|(k, _)| k == \"code\")\n                .map(|(_, v)| v.to_string());\n            // Gérer l'authentification\n        }\n        _ => {}\n    }\n}\n\n// Configuration tauri.conf.json\n// \"deep-link\": {\n//   \"desktop\": {\n//     \"schemes\": [\"tutodecode\"]\n//   }\n// }\n```",
      "terminal_task": {
        "command": "cargo test sidecar_manager -- --nocapture",
        "expected_output": "test sidecar_manager::tests::test_spawn_kill ... ok",
        "hint": "Utilisez Command::new_sidecar pour les binaires embarqués, tokio::spawn pour le listener d'events, et mpsc::channel pour la communication"
      }
    },
    {
      "id": "tauri-003-custom-invoke",
      "track": "TAURI_V3",
      "level": "Expert",
      "title": "Système d'Invocation Personnalisé",
      "duration": "5h45",
      "description": "Création d'un système de commandes avec middleware, validation, et gestion d'erreurs avancée.",
      "markdown_content": "# Système d'Invocation Personnalisé\n\n## Architecture Middleware\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    INVOKE PIPELINE                          │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  Frontend                    Backend                        │\n│     │                           │                           │\n│     ▼                           ▼                           │\n│  ┌─────────┐    ┌─────────────────────────────┐            │\n│  │ invoke  │───▶│  Router                     │            │\n│  └─────────┘    │  ┌───────────────────────┐  │            │\n│                 │  │ Middleware Chain      │  │            │\n│                 │  │ ┌─────┐┌─────┐┌─────┐ │  │            │\n│                 │  │ │Auth ││Rate ││Log  │ │  │            │\n│                 │  │ │     ││Limit││     │ │  │            │\n│                 │  │ └──┬──┘└──┬──┘└──┬──┘ │  │            │\n│                 │  └────┼──────┼──────┼────┘  │            │\n│                 │       └──────┴──────┘       │            │\n│                 │              │              │            │\n│                 │         ┌────┴────┐         │            │\n│                 │         │ Handler │         │            │\n│                 │         └────┬────┘         │            │\n│                 └──────────────┼──────────────┘            │\n│                                ▼                            │\n│                         ┌─────────────┐                     │\n│                         │  Response   │                     │\n│                         └─────────────┘                     │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Router avec Middleware\n\n```rust\nuse serde::{Serialize, Deserialize};\nuse std::future::Future;\nuse std::pin::Pin;\n\npub type BoxFuture<T> = Pin<Box<dyn Future<Output = T> + Send>>;\n\npub trait Middleware: Send + Sync {\n    fn handle(\n        &self,\n        ctx: &mut Context,\n        next: Next,\n    ) -> BoxFuture<Result<(), InvokeError>>;\n}\n\npub type Next = Box<dyn FnOnce(&mut Context) -> BoxFuture<Result<(), InvokeError>> + Send>;\n\npub struct Context {\n    pub request: InvokeRequest,\n    pub metadata: std::collections::HashMap<String, String>,\n}\n\n#[derive(Debug)]\npub struct InvokeError {\n    pub code: String,\n    pub message: String,\n    pub details: Option<serde_json::Value>,\n}\n\npub struct Router {\n    handlers: std::collections::HashMap<String, Box<dyn Handler>>,\n    middlewares: Vec<Box<dyn Middleware>>,\n}\n\ntrait Handler: Send + Sync {\n    fn handle(&self, ctx: &Context) -> BoxFuture<Result<serde_json::Value, InvokeError>>;\n}\n\nimpl Router {\n    pub fn new() -> Self {\n        Self {\n            handlers: std::collections::HashMap::new(),\n            middlewares: vec![],\n        }\n    }\n\n    pub fn register<H, F>(&mut self, name: &str, handler: H)\n    where\n        H: Fn(Context) -> F + Send + Sync + 'static,\n        F: Future<Output = Result<serde_json::Value, InvokeError>> + Send + 'static,\n    {\n        self.handlers.insert(\n            name.to_string(),\n            Box::new(move |ctx: &Context| Box::pin(handler(ctx.clone()))),\n        );\n    }\n\n    pub fn add_middleware<M: Middleware + 'static>(&mut self, middleware: M) {\n        self.middlewares.push(Box::new(middleware));\n    }\n\n    pub async fn invoke(\n        &self,\n        name: &str,\n        request: InvokeRequest,\n    ) -> Result<serde_json::Value, InvokeError> {\n        let mut ctx = Context {\n            request,\n            metadata: std::collections::HashMap::new(),\n        };\n\n        // Exécuter middlewares\n        let handler = self.handlers.get(name)\n            .ok_or_else(|| InvokeError {\n                code: \"NOT_FOUND\".to_string(),\n                message: format!(\"Handler '{}' not found\", name),\n                details: None,\n            })?;\n\n        // Build chain\n        let final_handler = move |ctx: &mut Context| {\n            let handler = handler;\n            Box::pin(async move { handler.handle(ctx).await }) as BoxFuture<_>\n        };\n\n        // Exécuter\n        final_handler(&mut ctx).await\n    }\n}\n\n// Middleware d'authentification\npub struct AuthMiddleware;\n\nimpl Middleware for AuthMiddleware {\n    fn handle(\n        &self,\n        ctx: &mut Context,\n        next: Next,\n    ) -> BoxFuture<Result<(), InvokeError>> {\n        Box::pin(async move {\n            let token = ctx.request.headers.get(\"Authorization\")\n                .ok_or_else(|| InvokeError {\n                    code: \"UNAUTHORIZED\".to_string(),\n                    message: \"Missing authorization header\".to_string(),\n                    details: None,\n                })?;\n\n            // Valider token\n            if !validate_token(token) {\n                return Err(InvokeError {\n                    code: \"UNAUTHORIZED\".to_string(),\n                    message: \"Invalid token\".to_string(),\n                    details: None,\n                });\n            }\n\n            next(ctx).await\n        })\n    }\n}\n\nfn validate_token(token: &str) -> bool {\n    token.starts_with(\"Bearer \")\n}\n```",
      "terminal_task": {
        "command": "cargo test custom_router -- --nocapture",
        "expected_output": "test custom_router::tests::test_middleware_chain ... ok",
        "hint": "Utilisez BoxFuture pour le type erasure, une linked list pour la chaîne de middlewares, et Context pour partager les données"
      }
    },
    {
      "id": "tauri-004-fs-watcher",
      "track": "TAURI_V3",
      "level": "Expert",
      "title": "File System Watcher et Synchronisation",
      "duration": "5h30",
      "description": "Surveillance de fichiers en temps réel, synchronisation bidirectionnelle, et gestion des conflits.",
      "markdown_content": "# File System Watcher et Synchronisation\n\n## Architecture FS Watcher\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    FS WATCHER SYSTEM                        │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  ┌─────────────┐    ┌─────────────┐    ┌─────────────────┐ │\n│  │   notify    │───▶│  Debouncer  │───▶│   Event Bus     │ │\n│  │  (native)   │    │  (300ms)    │    │  (broadcast)    │ │\n│  └─────────────┘    └─────────────┘    └────────┬────────┘ │\n│                                                  │          │\n│                              ┌───────────────────┼───────┐  │\n│                              ▼                   ▼       │  │\n│                       ┌────────────┐      ┌────────────┐  │  │\n│                       │ Sync Engine│      │ UI Update  │  │  │\n│                       └────────────┘      └────────────┘  │  │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Watcher avec Debounce\n\n```rust\nuse notify::{Config, Event, RecommendedWatcher, RecursiveMode, Watcher};\nuse std::collections::HashMap;\nuse std::path::Path;\nuse std::sync::Arc;\nuse std::time::{Duration, Instant};\nuse tokio::sync::{mpsc, RwLock};\nuse tokio::time::sleep;\n\npub struct FsWatcher {\n    watcher: RecommendedWatcher,\n    debounce_map: Arc<RwLock<HashMap<std::path::PathBuf, Instant>>>,\n    event_tx: mpsc::Sender<DebouncedEvent>,\n}\n\n#[derive(Debug, Clone)]\npub struct DebouncedEvent {\n    pub path: std::path::PathBuf,\n    pub kind: EventKind,\n    pub timestamp: Instant,\n}\n\n#[derive(Debug, Clone)]\npub enum EventKind {\n    Create,\n    Modify,\n    Delete,\n    Rename,\n}\n\nimpl FsWatcher {\n    pub fn new(event_tx: mpsc::Sender<DebouncedEvent>) -> Result<Self, notify::Error> {\n        let debounce_map = Arc::new(RwLock::new(HashMap::new()));\n        let debounce_map_clone = debounce_map.clone();\n\n        let tx = event_tx.clone();\n\n        let watcher = RecommendedWatcher::new(\n            move |res: Result<Event, notify::Error>| {\n                if let Ok(event) = res {\n                    for path in event.paths {\n                        let kind = match event.kind {\n                            notify::EventKind::Create(_) => EventKind::Create,\n                            notify::EventKind::Modify(_) => EventKind::Modify,\n                            notify::EventKind::Remove(_) => EventKind::Delete,\n                            _ => continue,\n                        };\n\n                        let _ = tx.try_send(DebouncedEvent {\n                            path,\n                            kind,\n                            timestamp: Instant::now(),\n                        });\n                    }\n                }\n            },\n            Config::default(),\n        )?;\n\n        // Spawn debouncer\n        let tx = event_tx.clone();\n        tokio::spawn(async move {\n            loop {\n                sleep(Duration::from_millis(100)).await;\n\n                let now = Instant::now();\n                let debounce_duration = Duration::from_millis(300);\n\n                let ready: Vec<_> = {\n                    let map = debounce_map_clone.read().await;\n                    map.iter()\n                        .filter(|(_, t)| now.duration_since(**t) >= debounce_duration)\n                        .map(|(p, _)| p.clone())\n                        .collect()\n                };\n\n                for path in ready {\n                    let mut map = debounce_map_clone.write().await;\n                    map.remove(&path);\n                }\n            }\n        });\n\n        Ok(Self {\n            watcher,\n            debounce_map,\n            event_tx,\n        })\n    }\n\n    pub fn watch(&mut self, path: &Path) -> Result<(), notify::Error> {\n        self.watcher.watch(path, RecursiveMode::Recursive)\n    }\n\n    pub fn unwatch(&mut self, path: &Path) -> Result<(), notify::Error> {\n        self.watcher.unwatch(path)\n    }\n}\n```\n\n## Synchronisation Bidirectionnelle\n\n```rust\nuse std::collections::HashSet;\nuse std::path::{Path, PathBuf};\nuse walkdir::WalkDir;\nuse blake3::Hasher;\n\npub struct SyncEngine {\n    local_root: PathBuf,\n    remote_root: PathBuf,\n}\n\n#[derive(Debug)]\npub struct FileEntry {\n    pub path: PathBuf,\n    pub hash: String,\n    pub modified: u64,\n    pub size: u64,\n}\n\nimpl SyncEngine {\n    pub fn new(local: &Path, remote: &Path) -> Self {\n        Self {\n            local_root: local.to_path_buf(),\n            remote_root: remote.to_path_buf(),\n        }\n    }\n\n    pub fn scan_directory(&self, root: &Path) -> Vec<FileEntry> {\n        let mut entries = vec![];\n\n        for entry in WalkDir::new(root).follow_links(true) {\n            if let Ok(entry) = entry {\n                if entry.file_type().is_file() {\n                    if let Some(file_entry) = self.hash_file(&entry) {\n                        entries.push(file_entry);\n                    }\n                }\n            }\n        }\n\n        entries\n    }\n\n    fn hash_file(&self, entry: &walkdir::DirEntry) -> Option<FileEntry> {\n        let path = entry.path();\n        let metadata = entry.metadata().ok()?;\n\n        let mut hasher = Hasher::new();\n        if let Ok(content) = std::fs::read(path) {\n            hasher.update(&content);\n        }\n\n        Some(FileEntry {\n            path: path.strip_prefix(&self.local_root).ok()?.to_path_buf(),\n            hash: hasher.finalize().to_hex().to_string(),\n            modified: metadata.modified().ok()?\n                .duration_since(std::time::UNIX_EPOCH).ok()?.as_secs(),\n            size: metadata.len(),\n        })\n    }\n\n    pub fn compute_sync_plan(\n        &self,\n        local: &[FileEntry],\n        remote: &[FileEntry],\n    ) -> SyncPlan {\n        let local_map: std::collections::HashMap<_, _> = local\n            .iter()\n            .map(|e| (&e.path, e))\n            .collect();\n\n        let remote_map: std::collections::HashMap<_, _> = remote\n            .iter()\n            .map(|e| (&e.path, e))\n            .collect();\n\n        let mut plan = SyncPlan::default();\n\n        // Fichiers à uploader (existants localement, absents ou différents à distance)\n        for local_entry in local {\n            match remote_map.get(&local_entry.path) {\n                None => plan.upload.push(local_entry.path.clone()),\n                Some(remote_entry) => {\n                    if local_entry.hash != remote_entry.hash {\n                        if local_entry.modified > remote_entry.modified {\n                            plan.upload.push(local_entry.path.clone());\n                        } else {\n                            plan.conflicts.push(local_entry.path.clone());\n                        }\n                    }\n                }\n            }\n        }\n\n        // Fichiers à télécharger\n        for remote_entry in remote {\n            if !local_map.contains_key(&remote_entry.path) {\n                plan.download.push(remote_entry.path.clone());\n            }\n        }\n\n        plan\n    }\n}\n\n#[derive(Default)]\npub struct SyncPlan {\n    pub upload: Vec<PathBuf>,\n    pub download: Vec<PathBuf>,\n    pub conflicts: Vec<PathBuf>,\n}\n```",
      "terminal_task": {
        "command": "cargo test fs_watcher -- --nocapture",
        "expected_output": "test fs_watcher::tests::test_sync_plan ... ok",
        "hint": "Utilisez notify pour le watching, tokio::time::sleep pour le debounce, et blake3 pour le hashing rapide"
      }
    },
    {
      "id": "tauri-005-notification-system",
      "track": "TAURI_V3",
      "level": "Expert",
      "title": "Système de Notifications Cross-Platform",
      "duration": "4h30",
      "description": "Notifications natives, badges, tray icons, et actions interactives.",
      "markdown_content": "# Système de Notifications Cross-Platform\n\n## Architecture Notification\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    NOTIFICATION SYSTEM                      │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  ┌─────────────┐    ┌─────────────┐    ┌─────────────────┐ │\n│  │   Queue     │───▶│  Priority   │───▶│  Native Adapter │ │\n│  │  (FIFO)     │    │  Filter     │    │  (OS-specific)  │ │\n│  └─────────────┘    └─────────────┘    └────────┬────────┘ │\n│                                                  │          │\n│                              ┌───────────────────┼───────┐  │\n│                              ▼                   ▼       │  │\n│                       ┌────────────┐      ┌────────────┐  │  │\n│                       │ Windows    │      │ macOS      │  │  │\n│                       │ (WinRT)    │      │ (UN)       │  │  │\n│                       └────────────┘      └────────────┘  │  │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Notification Manager\n\n```rust\nuse tauri::Manager;\nuse serde::{Serialize, Deserialize};\nuse std::collections::VecDeque;\nuse std::sync::{Arc, Mutex};\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Notification {\n    pub id: String,\n    pub title: String,\n    pub body: String,\n    pub icon: Option<String>,\n    pub actions: Vec<NotificationAction>,\n    pub urgency: Urgency,\n    pub timestamp: u64,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct NotificationAction {\n    pub id: String,\n    pub label: String,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum Urgency {\n    Low,\n    Normal,\n    High,\n    Critical,\n}\n\npub struct NotificationManager {\n    queue: Arc<Mutex<VecDeque<Notification>>>,\n    history: Arc<Mutex<Vec<Notification>>>,\n    max_history: usize,\n}\n\nimpl NotificationManager {\n    pub fn new(max_history: usize) -> Self {\n        Self {\n            queue: Arc::new(Mutex::new(VecDeque::new())),\n            history: Arc::new(Mutex::new(vec![])),\n            max_history,\n        }\n    }\n\n    pub fn schedule(&self, notification: Notification) {\n        let mut queue = self.queue.lock().unwrap();\n\n        // Insérer selon la priorité\n        let insert_pos = queue.iter()\n            .position(|n| urgency_value(&n.urgency) < urgency_value(&notification.urgency))\n            .unwrap_or(queue.len());\n\n        queue.insert(insert_pos, notification);\n    }\n\n    pub fn show_next<R: tauri::Runtime>(\n        &self,\n        app: &tauri::AppHandle<R>,\n    ) -> Result<(), String> {\n        let notification = {\n            let mut queue = self.queue.lock().unwrap();\n            queue.pop_front()\n        };\n\n        if let Some(notification) = notification {\n            // Envoyer au frontend\n            app.emit(\"notification:show\", &notification)\n                .map_err(|e| e.to_string())?;\n\n            // Notification native\n            #[cfg(target_os = \"macos\")]\n            self.show_native_macos(&notification)?;\n\n            #[cfg(target_os = \"windows\")]\n            self.show_native_windows(&notification)?;\n\n            #[cfg(target_os = \"linux\")]\n            self.show_native_linux(&notification)?;\n\n            // Ajouter à l'historique\n            let mut history = self.history.lock().unwrap();\n            history.push(notification);\n\n            if history.len() > self.max_history {\n                history.remove(0);\n            }\n        }\n\n        Ok(())\n    }\n\n    #[cfg(target_os = \"macos\")]\n    fn show_native_macos(&self, notification: &Notification) -> Result<(), String> {\n        use tauri::notification::NotificationBuilder;\n\n        let mut builder = NotificationBuilder::new(notification.id.clone())\n            .title(&notification.title)\n            .body(&notification.body);\n\n        if let Some(icon) = &notification.icon {\n            builder = builder.icon(icon);\n        }\n\n        for action in &notification.actions {\n            builder = builder.action(&action.id, &action.label);\n        }\n\n        builder.show()\n            .map_err(|e| e.to_string())?;\n\n        Ok(())\n    }\n\n    #[cfg(target_os = \"windows\")]\n    fn show_native_windows(&self, notification: &Notification) -> Result<(), String> {\n        // Utiliser winrt-notification ou tauri built-in\n        Ok(())\n    }\n\n    pub fn get_history(&self) -> Vec<Notification> {\n        self.history.lock().unwrap().clone()\n    }\n\n    pub fn clear_history(&self) {\n        self.history.lock().unwrap().clear();\n    }\n}\n\nfn urgency_value(urgency: &Urgency) -> u8 {\n    match urgency {\n        Urgency::Low => 0,\n        Urgency::Normal => 1,\n        Urgency::High => 2,\n        Urgency::Critical => 3,\n    }\n}\n```\n\n## Tray Icon avec Menu\n\n```rust\nuse tauri::tray::{TrayIconBuilder, TrayIconEvent};\nuse tauri::menu::{Menu, MenuItem};\n\npub fn setup_tray<R: tauri::Runtime>(\n    app: &tauri::AppHandle<R>,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let menu = Menu::with_items(app, &[\n        &MenuItem::with_id(app, \"show\", \"Show\", true, None::<&str>)?,\n        &MenuItem::with_id(app, \"hide\", \"Hide\", true, None::<&str>)?,\n        &MenuItem::with_id(app, \"settings\", \"Settings...\", true, None::<&str>)?,\n        &MenuItem::with_id(app, \"separator\", \"---\", false, None::<&str>)?,\n        &MenuItem::with_id(app, \"quit\", \"Quit\", true, None::<&str>)?,\n    ])?;\n\n    TrayIconBuilder::new()\n        .menu(&menu)\n        .on_menu_event(|app, event| {\n            match event.id().as_ref() {\n                \"show\" => {\n                    if let Some(window) = app.get_webview_window(\"main\") {\n                        window.show().unwrap();\n                        window.set_focus().unwrap();\n                    }\n                }\n                \"hide\" => {\n                    if let Some(window) = app.get_webview_window(\"main\") {\n                        window.hide().unwrap();\n                    }\n                }\n                \"settings\" => {\n                    // Ouvrir fenêtre settings\n                }\n                \"quit\" => {\n                    app.exit(0);\n                }\n                _ => {}\n            }\n        })\n        .on_tray_icon_event(|tray, event| {\n            if let TrayIconEvent::Click { .. } = event {\n                let app = tray.app_handle();\n                if let Some(window) = app.get_webview_window(\"main\") {\n                    if window.is_visible().unwrap() {\n                        window.hide().unwrap();\n                    } else {\n                        window.show().unwrap();\n                        window.set_focus().unwrap();\n                    }\n                }\n            }\n        })\n        .build(app)?;\n\n    Ok(())\n}\n```",
      "terminal_task": {
        "command": "cargo test notification -- --nocapture",
        "expected_output": "test notification::tests::test_priority_queue ... ok",
        "hint": "Utilisez VecDeque pour la queue FIFO, une insertion triée pour la priorité, et tauri::notification pour l'OS integration"
      }
    },
    {
      "id": "tauri-006-autoupdate",
      "track": "TAURI_V3",
      "level": "Expert",
      "title": "Système de Mise à Jour Automatique",
      "duration": "6h00",
      "description": "Mise à jour delta, rollback, canary releases, et signature de paquets.",
      "markdown_content": "# Système de Mise à Jour Automatique\n\n## Architecture Auto-Update\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    AUTO-UPDATE SYSTEM                       │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  ┌─────────────┐    ┌─────────────┐    ┌─────────────────┐ │\n│  │   Check     │───▶│  Download   │───▶│   Verify        │ │\n│  │  Version    │    │  (delta)    │    │  (signature)    │ │\n│  └─────────────┘    └─────────────┘    └────────┬────────┘ │\n│                                                  │          │\n│                              ┌───────────────────┼───────┐  │\n│                              ▼                   ▼       │  │\n│                       ┌────────────┐      ┌────────────┐  │  │\n│                       │  Install   │      │  Rollback  │  │  │\n│                       │ (atomic)   │      │  (backup)  │  │  │\n│                       └────────────┘      └────────────┘  │  │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Update Manager\n\n```rust\nuse serde::{Deserialize, Serialize};\nuse std::path::PathBuf;\nuse tokio::fs;\nuse tokio::io::AsyncWriteExt;\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ReleaseInfo {\n    pub version: String,\n    pub changelog: String,\n    pub download_url: String,\n    pub signature: String,\n    pub size: u64,\n    pub is_delta: bool,\n    pub min_version: Option<String>,\n}\n\n#[derive(Debug, Clone)]\npub enum UpdateState {\n    Idle,\n    Checking,\n    Available(ReleaseInfo),\n    Downloading { progress: f64 },\n    Verifying,\n    Installing,\n    Restarting,\n    Error(String),\n}\n\npub struct UpdateManager {\n    current_version: String,\n    update_endpoint: String,\n    state: std::sync::Arc<tokio::sync::RwLock<UpdateState>>,\n    download_dir: PathBuf,\n}\n\nimpl UpdateManager {\n    pub fn new(current_version: &str, update_endpoint: &str) -> Self {\n        let download_dir = dirs::cache_dir()\n            .unwrap_or_else(|| PathBuf::from(\".\"))\n            .join(\"tutodecode\")\n            .join(\"updates\");\n\n        Self {\n            current_version: current_version.to_string(),\n            update_endpoint: update_endpoint.to_string(),\n            state: std::sync::Arc::new(tokio::sync::RwLock::new(UpdateState::Idle)),\n            download_dir,\n        }\n    }\n\n    pub async fn check_for_updates(&self) -> Result<Option<ReleaseInfo>, String> {\n        let mut state = self.state.write().await;\n        *state = UpdateState::Checking;\n\n        let client = reqwest::Client::new();\n        let response = client\n            .get(&format!(\"{}/releases/latest\", self.update_endpoint))\n            .query(&[(\"current_version\", &self.current_version)])\n            .send()\n            .await\n            .map_err(|e| e.to_string())?;\n\n        if response.status().is_success() {\n            let release: ReleaseInfo = response.json().await\n                .map_err(|e| e.to_string())?;\n\n            if self.is_newer(&release.version) {\n                *state = UpdateState::Available(release.clone());\n                Ok(Some(release))\n            } else {\n                *state = UpdateState::Idle;\n                Ok(None)\n            }\n        } else {\n            *state = UpdateState::Error(\"Failed to check updates\".to_string());\n            Ok(None)\n        }\n    }\n\n    pub async fn download_and_install(&self, release: &ReleaseInfo) -> Result<(), String> {\n        // Créer le répertoire de téléchargement\n        fs::create_dir_all(&self.download_dir).await\n            .map_err(|e| e.to_string())?;\n\n        let file_path = self.download_dir.join(format!(\"update-{}\", release.version));\n\n        // Télécharger\n        {\n            let mut state = self.state.write().await;\n            *state = UpdateState::Downloading { progress: 0.0 };\n        }\n\n        let client = reqwest::Client::new();\n        let response = client.get(&release.download_url)\n            .send()\n            .await\n            .map_err(|e| e.to_string())?;\n\n        let total_size = response.content_length().unwrap_or(0);\n        let mut downloaded = 0u64;\n\n        let mut file = fs::File::create(&file_path).await\n            .map_err(|e| e.to_string())?;\n\n        let mut stream = response.bytes_stream();\n\n        while let Some(chunk) = stream.next().await {\n            let chunk = chunk.map_err(|e| e.to_string())?;\n            file.write_all(&chunk).await.map_err(|e| e.to_string())?;\n\n            downloaded += chunk.len() as u64;\n            let progress = if total_size > 0 {\n                (downloaded as f64 / total_size as f64) * 100.0\n            } else {\n                0.0\n            };\n\n            let mut state = self.state.write().await;\n            *state = UpdateState::Downloading { progress };\n        }\n\n        file.flush().await.map_err(|e| e.to_string())?;\n        drop(file);\n\n        // Vérifier la signature\n        {\n            let mut state = self.state.write().await;\n            *state = UpdateState::Verifying;\n        }\n\n        self.verify_signature(&file_path, &release.signature).await?;\n\n        // Installer\n        {\n            let mut state = self.state.write().await;\n            *state = UpdateState::Installing;\n        }\n\n        self.install_update(&file_path).await?;\n\n        // Redémarrer\n        {\n            let mut state = self.state.write().await;\n            *state = UpdateState::Restarting;\n        }\n\n        self.restart_app().await?;\n\n        Ok(())\n    }\n\n    async fn verify_signature(&self, file_path: &PathBuf, expected: &str) -> Result<(), String> {\n        let content = fs::read(file_path).await\n            .map_err(|e| e.to_string())?;\n\n        let hash = blake3::hash(&content);\n        let actual = hash.to_hex().to_string();\n\n        if actual != expected {\n            return Err(\"Signature verification failed\".to_string());\n        }\n\n        Ok(())\n    }\n\n    async fn install_update(&self, file_path: &PathBuf) -> Result<(), String> {\n        // Utiliser tauri-updater ou implémentation custom\n        #[cfg(target_os = \"macos\")]\n        {\n            // Extraire et remplacer l'app bundle\n        }\n\n        #[cfg(target_os = \"windows\")]\n        {\n            // Utiliser installer MSI/NSIS\n        }\n\n        Ok(())\n    }\n\n    async fn restart_app(&self) -> Result<(), String> {\n        // Relancer l'application\n        std::process::Command::new(std::env::current_exe().unwrap())\n            .spawn()\n            .map_err(|e| e.to_string())?;\n\n        std::process::exit(0);\n    }\n\n    fn is_newer(&self, version: &str) -> bool {\n        let current: Vec<u32> = self.current_version\n            .split('.')\n            .filter_map(|s| s.parse().ok())\n            .collect();\n\n        let new: Vec<u32> = version\n            .split('.')\n            .filter_map(|s| s.parse().ok())\n            .collect();\n\n        for (c, n) in current.iter().zip(new.iter()) {\n            if n > c {\n                return true;\n            }\n            if n < c {\n                return false;\n            }\n        }\n\n        new.len() > current.len()\n    }\n}\n```",
      "terminal_task": {
        "command": "cargo test update_manager -- --nocapture",
        "expected_output": "test update_manager::tests::test_version_compare ... ok",
        "hint": "Utilisez reqwest pour le download, blake3 pour la signature, et std::process::Command pour le restart"
      }
    },
    {
      "id": "tauri-007-plugin-dev",
      "track": "TAURI_V3",
      "level": "Expert",
      "title": "Développement de Plugins Tauri",
      "duration": "7h00",
      "description": "Création de plugins avec JS API, state management, et lifecycle hooks.",
      "markdown_content": "# Développement de Plugins Tauri\n\n## Architecture Plugin\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    TAURI PLUGIN                             │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  Frontend (JS/TS)              Backend (Rust)               │\n│  ┌─────────────┐               ┌─────────────────────┐     │\n│  │ JavaScript  │──────────────▶│   Plugin Builder    │     │\n│  │   API       │               │                     │     │\n│  └─────────────┘               │ ┌─────────────────┐ │     │\n│                                │ │ Commands        │ │     │\n│  ┌─────────────┐               │ │ Event Listeners │ │     │\n│  │  Types      │◄──────────────│ │ State Manager   │ │     │\n│  │ Definitions │               │ └─────────────────┘ │     │\n│  └─────────────┘               └─────────────────────┘     │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Structure d'un Plugin\n\n```rust\n// src/lib.rs\nuse tauri::{\n    plugin::{Builder, TauriPlugin},\n    Manager, Runtime,\n};\n\npub struct DatabasePlugin;\n\n#[derive(Default)]\npub struct DatabaseState {\n    connections: std::collections::HashMap<String, Connection>,\n}\n\npub fn init<R: Runtime>() -> TauriPlugin<R> {\n    Builder::new(\"database\")\n        .invoke_handler(tauri::generate_handler![\n            connect,\n            query,\n            execute,\n            disconnect,\n        ])\n        .setup(|app, api| {\n            // Initialisation\n            let state = DatabaseState::default();\n            app.manage(state);\n\n            // Écouter les événements\n            app.listen(\"database:config-changed\", |event| {\n                // Recharger la configuration\n            });\n\n            Ok(())\n        })\n        .on_event(|app, event| {\n            match event {\n                tauri::RunEvent::Exit => {\n                    // Cleanup\n                    let state = app.state::<DatabaseState>();\n                    for (name, conn) in &state.connections {\n                        conn.close();\n                    }\n                }\n                _ => {}\n            }\n        })\n        .build()\n}\n\n#[tauri::command]\nasync fn connect<R: Runtime>(\n    app: tauri::AppHandle<R>,\n    name: String,\n    connection_string: String,\n) -> Result<String, String> {\n    let mut state = app.state::<DatabaseState>();\n\n    let conn = Connection::new(&connection_string)\n        .map_err(|e| e.to_string())?;\n\n    state.connections.insert(name.clone(), conn);\n\n    Ok(format!(\"Connected to {}\", name))\n}\n\n#[tauri::command]\nasync fn query<R: Runtime>(\n    app: tauri::AppHandle<R>,\n    connection: String,\n    sql: String,\n) -> Result<Vec<serde_json::Value>, String> {\n    let state = app.state::<DatabaseState>();\n\n    let conn = state.connections.get(&connection)\n        .ok_or(\"Connection not found\")?;\n\n    conn.query(&sql).map_err(|e| e.to_string())\n}\n\n#[tauri::command]\nasync fn execute<R: Runtime>(\n    app: tauri::AppHandle<R>,\n    connection: String,\n    sql: String,\n) -> Result<u64, String> {\n    let state = app.state::<DatabaseState>();\n\n    let conn = state.connections.get(&connection)\n        .ok_or(\"Connection not found\")?;\n\n    conn.execute(&sql).map_err(|e| e.to_string())\n}\n\n#[tauri::command]\nasync fn disconnect<R: Runtime>(\n    app: tauri::AppHandle<R>,\n    name: String,\n) -> Result<(), String> {\n    let mut state = app.state::<DatabaseState>();\n\n    if let Some(conn) = state.connections.remove(&name) {\n        conn.close();\n    }\n\n    Ok(())\n}\n```\n\n## API JavaScript\n\n```typescript\n// guest-js/index.ts\nimport { invoke } from '@tauri-apps/api/core';\n\nexport interface ConnectionConfig {\n  host: string;\n  port: number;\n  database: string;\n  username: string;\n  password: string;\n}\n\nexport interface QueryResult {\n  rows: Record<string, unknown>[];\n  columns: string[];\n  rowCount: number;\n}\n\nexport class DatabaseClient {\n  private connectionName: string;\n\n  constructor(connectionName: string) {\n    this.connectionName = connectionName;\n  }\n\n  static async connect(\n    name: string,\n    config: ConnectionConfig\n  ): Promise<DatabaseClient> {\n    const connectionString = `postgresql://${config.username}:${config.password}@${config.host}:${config.port}/${config.database}`;\n\n    await invoke('plugin:database|connect', {\n      name,\n      connectionString,\n    });\n\n    return new DatabaseClient(name);\n  }\n\n  async query<T = Record<string, unknown>>(sql: string): Promise<T[]> {\n    return invoke('plugin:database|query', {\n      connection: this.connectionName,\n      sql,\n    });\n  }\n\n  async execute(sql: string): Promise<number> {\n    return invoke('plugin:database|execute', {\n      connection: this.connectionName,\n      sql,\n    });\n  }\n\n  async disconnect(): Promise<void> {\n    await invoke('plugin:database|disconnect', {\n      name: this.connectionName,\n    });\n  }\n}\n\n// Re-export\nexport { DatabaseClient as default };\n```\n\n## Configuration Cargo.toml\n\n```toml\n[package]\nname = \"tauri-plugin-database\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\n[dependencies]\ntauri = { version = \"2.0\", features = [] }\nserde = { version = \"1.0\", features = [\"derive\"] }\ntokio = { version = \"1\", features = [\"full\"] }\n\n[build-dependencies]\ntauri-build = \"2.0\"\n```",
      "terminal_task": {
        "command": "cargo test plugin -- --nocapture",
        "expected_output": "test plugin::tests::test_plugin_init ... ok",
        "hint": "Utilisez Builder::new pour créer le plugin, manage pour le state, et generate_handler! pour les commands"
      }
    },
    {
      "id": "tauri-008-security-hardening",
      "track": "TAURI_V3",
      "level": "Expert",
      "title": "Sécurité: Hardening et CSP",
      "duration": "6h30",
      "description": "Content Security Policy, isolation contextes, et audit de sécurité.",
      "markdown_content": "# Sécurité: Hardening et CSP\n\n## Architecture Sécurisée\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    SECURITY LAYERS                          │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  Layer 1: CSP                    Layer 2: Capabilities      │\n│  ┌─────────────┐                 ┌─────────────────────┐   │\n│  │ default-src │                 │ fs:read-only        │   │\n│  │ 'self'      │                 │ shell:deny          │   │\n│  │ script-src  │                 │ http:api.github.com │   │\n│  │ 'unsafe-    │                 └─────────────────────┘   │\n│  │  inline'    │                                           │\n│  └─────────────┘                 Layer 3: IPC Validation   │\n│                                  ┌─────────────────────┐   │\n│  Layer 4: Process                │ Input Sanitization  │   │\n│  Isolation                       │ Type Checking       │   │\n│  ┌─────────────┐                 │ Rate Limiting       │   │\n│  │ WebView     │                 └─────────────────────┘   │\n│  │ (sandboxed) │                                           │\n│  └─────────────┘                                           │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## CSP Configuration\n\n```json\n// tauri.conf.json\n{\n  \"app\": {\n    \"security\": {\n      \"csp\": {\n        \"default-src\": \"'self'\",\n        \"script-src\": \"'self' 'unsafe-inline'\",\n        \"style-src\": \"'self' 'unsafe-inline'\",\n        \"img-src\": \"'self' data: https:\",\n        \"font-src\": \"'self'\",\n        \"connect-src\": \"'self' https://api.github.com https://api.openai.com\",\n        \"frame-src\": \"'none'\",\n        \"object-src\": \"'none'\",\n        \"base-uri\": \"'self'\",\n        \"form-action\": \"'self'\"\n      },\n      \"dangerousDisableAssetCspModification\": false\n    }\n  }\n}\n```\n\n## Capabilities (Tauri v2+)\n\n```json\n// capabilities/default.json\n{\n  \"identifier\": \"default\",\n  \"description\": \"Default capabilities\",\n  \"local\": true,\n  \"windows\": [\"main\"],\n  \"permissions\": [\n    \"path:default\",\n    \"event:default\",\n    \"window:default\",\n    \"webview:default\",\n    \"app:default\",\n    \"image:default\",\n    \"resources:default\",\n    \"menu:default\",\n    \"tray:default\",\n    {\n      \"identifier\": \"fs:scope\",\n      \"allow\": [\n        { \"path\": \"$APPDATA\" },\n        { \"path\": \"$APPDATA/**\" },\n        { \"path\": \"$DOWNLOAD\" },\n        { \"path\": \"$DOWNLOAD/**\" }\n      ],\n      \"deny\": [\n        { \"path\": \"$APPDATA/secrets.json\" }\n      ]\n    },\n    {\n      \"identifier\": \"shell:allow-execute\",\n      \"allow\": [\n        {\n          \"args\": [\"-la\"],\n          \"cmd\": \"ls\",\n          \"name\": \"list-files\",\n          \"sidecar\": false\n        }\n      ]\n    },\n    {\n      \"identifier\": \"http:default\",\n      \"allow\": [\n        { \"url\": \"https://api.github.com\" },\n        { \"url\": \"https://api.openai.com\" }\n      ]\n    }\n  ]\n}\n```\n\n## Validation d'Input\n\n```rust\nuse validator::{Validate, ValidationError};\nuse serde::{Deserialize, Serialize};\n\n#[derive(Debug, Serialize, Deserialize, Validate)]\npub struct FileOperationRequest {\n    #[validate(length(min = 1, max = 255))]\n    #[validate(regex(path = \"FILE_PATH_REGEX\"))]\n    pub path: String,\n\n    #[validate(range(min = 0, max = 100_000_000))]\n    pub max_size: Option<usize>,\n}\n\nlazy_static::lazy_static! {\n    static ref FILE_PATH_REGEX: regex::Regex = \n        regex::Regex::new(r\"^[a-zA-Z0-9_./-]+$\").unwrap();\n}\n\n#[tauri::command]\npub async fn read_file_secure(\n    request: FileOperationRequest,\n) -> Result<Vec<u8>, String> {\n    // Valider la requête\n    request.validate()\n        .map_err(|e| format!(\"Validation failed: {}\", e))?;\n\n    // Vérifier le chemin\n    let path = std::path::Path::new(&request.path);\n\n    // Normaliser et vérifier\n    let canonical = path.canonicalize()\n        .map_err(|e| format!(\"Invalid path: {}\", e))?;\n\n    // Vérifier que le chemin est dans un répertoire autorisé\n    let allowed_roots = get_allowed_roots();\n    let is_allowed = allowed_roots.iter()\n        .any(|root| canonical.starts_with(root));\n\n    if !is_allowed {\n        return Err(\"Path not in allowed directory\".to_string());\n    }\n\n    // Lire avec limite de taille\n    let content = if let Some(max_size) = request.max_size {\n        read_file_with_limit(&canonical, max_size).await?\n    } else {\n        tokio::fs::read(&canonical).await\n            .map_err(|e| e.to_string())?\n    };\n\n    Ok(content)\n}\n\nasync fn read_file_with_limit(\n    path: &std::path::Path,\n    limit: usize,\n) -> Result<Vec<u8>, String> {\n    use tokio::io::AsyncReadExt;\n\n    let mut file = tokio::fs::File::open(path).await\n        .map_err(|e| e.to_string())?;\n\n    let mut buffer = Vec::with_capacity(limit.min(8192));\n    let n = file.read_to_end(&mut buffer).await\n        .map_err(|e| e.to_string())?;\n\n    if n > limit {\n        return Err(\"File exceeds size limit\".to_string());\n    }\n\n    Ok(buffer)\n}\n\nfn get_allowed_roots() -> Vec<std::path::PathBuf> {\n    vec![\n        dirs::data_dir().unwrap_or_default(),\n        dirs::download_dir().unwrap_or_default(),\n    ]\n}\n```",
      "terminal_task": {
        "command": "cargo test security -- --nocapture",
        "expected_output": "test security::tests::test_path_validation ... ok",
        "hint": "Utilisez validator pour la validation, canonicalize pour la normalisation, et capabilities pour les permissions"
      }
    },
    {
      "id": "tauri-009-mobile-adaptation",
      "track": "TAURI_V3",
      "level": "Expert",
      "title": "Adaptation Mobile: iOS et Android",
      "duration": "8h00",
      "description": "Plugins natifs mobiles, gestion du cycle de vie, et optimisation des performances.",
      "markdown_content": "# Adaptation Mobile: iOS et Android\n\n## Architecture Mobile\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    MOBILE ARCHITECTURE                      │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  ┌─────────────────────────────────────────────────────┐   │\n│  │                 WebView (WK/AWV)                    │   │\n│  └─────────────────────────┬───────────────────────────┘   │\n│                            │                                │\n│  ┌─────────────────────────▼───────────────────────────┐   │\n│  │              Tauri Bridge (Rust)                    │   │\n│  └─────────────────────────┬───────────────────────────┘   │\n│                            │                                │\n│           ┌────────────────┼────────────────┐              │\n│           ▼                ▼                ▼              │\n│    ┌────────────┐   ┌────────────┐   ┌────────────┐       │\n│    │ iOS Plugin │   │ Android    │   │ Universal  │       │\n│    │ (Swift)    │   │ Plugin     │   │ Plugin     │       │\n│    │            │   │ (Kotlin)   │   │ (Rust)     │       │\n│    └────────────┘   └────────────┘   └────────────┘       │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Plugin Mobile (Swift)\n\n```swift\n// ios/Sources/Plugin.swift\nimport UIKit\nimport WebKit\nimport Tauri\n\nclass HapticsPlugin: Plugin {\n    private let feedbackGenerator = UIImpactFeedbackGenerator(style: .medium)\n\n    @objc public func impact(_ invoke: Invoke) {\n        let style = invoke.getString(\"style\") ?? \"medium\"\n\n        let feedbackStyle: UIImpactFeedbackGenerator.FeedbackStyle\n        switch style {\n        case \"light\":\n            feedbackStyle = .light\n        case \"medium\":\n            feedbackStyle = .medium\n        case \"heavy\":\n            feedbackStyle = .heavy\n        default:\n            feedbackStyle = .medium\n        }\n\n        let generator = UIImpactFeedbackGenerator(style: feedbackStyle)\n        generator.prepare()\n        generator.impactOccurred()\n\n        invoke.resolve()\n    }\n\n    @objc public func notification(_ invoke: Invoke) {\n        let type = invoke.getString(\"type\") ?? \"success\"\n\n        let notificationType: UINotificationFeedbackGenerator.FeedbackType\n        switch type {\n        case \"success\":\n            notificationType = .success\n        case \"warning\":\n            notificationType = .warning\n        case \"error\":\n            notificationType = .error\n        default:\n            notificationType = .success\n        }\n\n        let generator = UINotificationFeedbackGenerator()\n        generator.prepare()\n        generator.notificationOccurred(notificationType)\n\n        invoke.resolve()\n    }\n}\n\n@_cdecl(\"init_plugin_haptics\")\nfunc initPlugin() -> Plugin {\n    return HapticsPlugin()\n}\n```\n\n## Plugin Mobile (Kotlin)\n\n```kotlin\n// android/src/main/java/Plugin.kt\npackage com.tutodecode.haptics\n\nimport android.app.Activity\nimport android.os.VibrationEffect\nimport android.os.Vibrator\nimport android.os.VibratorManager\nimport app.tauri.annotation.Command\nimport app.tauri.annotation.InvokeArg\nimport app.tauri.annotation.TauriPlugin\nimport app.tauri.plugin.Invoke\nimport app.tauri.plugin.Plugin\n\n@InvokeArg\nclass ImpactArgs {\n    var style: String? = \"medium\"\n}\n\n@InvokeArg\nclass NotificationArgs {\n    var type: String? = \"success\"\n}\n\n@TauriPlugin\nclass HapticsPlugin(private val activity: Activity) : Plugin(activity) {\n    private val vibrator: Vibrator by lazy {\n        if (android.os.Build.VERSION.SDK_INT >= android.os.Build.VERSION_CODES.S) {\n            val vibratorManager = activity.getSystemService(VibratorManager::class.java)\n            vibratorManager.defaultVibrator\n        } else {\n            @Suppress(\"DEPRECATION\")\n            activity.getSystemService(Vibrator::class.java)\n        }\n    }\n\n    @Command\n    fun impact(invoke: Invoke) {\n        val args = invoke.parseArgs(ImpactArgs::class.java)\n\n        val amplitude = when (args.style) {\n            \"light\" -> 50\n            \"medium\" -> 128\n            \"heavy\" -> 255\n            else -> 128\n        }\n\n        if (android.os.Build.VERSION.SDK_INT >= android.os.Build.VERSION_CODES.O) {\n            val effect = VibrationEffect.createOneShot(50, amplitude)\n            vibrator.vibrate(effect)\n        } else {\n            @Suppress(\"DEPRECATION\")\n            vibrator.vibrate(50)\n        }\n\n        invoke.resolve()\n    }\n\n    @Command\n    fun notification(invoke: Invoke) {\n        val args = invoke.parseArgs(NotificationArgs::class.java)\n\n        val pattern = when (args.type) {\n            \"success\" -> longArrayOf(0, 100, 50, 100)\n            \"warning\" -> longArrayOf(0, 200, 100, 200)\n            \"error\" -> longArrayOf(0, 300, 100, 300, 100, 300)\n            else -> longArrayOf(0, 100, 50, 100)\n        }\n\n        if (android.os.Build.VERSION.SDK_INT >= android.os.Build.VERSION_CODES.O) {\n            val effect = VibrationEffect.createWaveform(pattern, -1)\n            vibrator.vibrate(effect)\n        } else {\n            @Suppress(\"DEPRECATION\")\n            vibrator.vibrate(pattern, -1)\n        }\n\n        invoke.resolve()\n    }\n}\n```\n\n## Bridge Rust\n\n```rust\n// src/lib.rs\nuse tauri::plugin::{Builder, TauriPlugin};\nuse tauri::Runtime;\n\n#[tauri::command]\nasync fn impact(style: String) -> Result<(), String> {\n    #[cfg(target_os = \"ios\")]\n    {\n        // Appeler le plugin Swift\n    }\n\n    #[cfg(target_os = \"android\")]\n    {\n        // Appeler le plugin Kotlin\n    }\n\n    #[cfg(not(any(target_os = \"ios\", target_os = \"android\")))]\n    {\n        // Desktop: no-op\n    }\n\n    Ok(())\n}\n\npub fn init<R: Runtime>() -> TauriPlugin<R> {\n    Builder::new(\"haptics\")\n        .invoke_handler(tauri::generate_handler![impact])\n        .build()\n}\n```",
      "terminal_task": {
        "command": "cargo test mobile -- --nocapture",
        "expected_output": "test mobile::tests::test_plugin_init ... ok",
        "hint": "Utilisez #[cfg(target_os)] pour la compilation conditionnelle, et TauriPlugin pour l'intégration"
      }
    },
    {
      "id": "tauri-010-performance",
      "track": "TAURI_V3",
      "level": "Expert",
      "title": "Optimisation des Performances",
      "duration": "6h30",
      "description": "Profiling, optimisation du démarrage, mémoire, et bundle size.",
      "markdown_content": "# Optimisation des Performances\n\n## Profiling Architecture\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    PERFORMANCE PROFILING                    │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  ┌─────────────┐    ┌─────────────┐    ┌─────────────────┐ │\n│  │  Startup    │    │  Memory     │    │   CPU Usage     │ │\n│  │  Time       │    │  Tracking   │    │   (flamegraph)  │ │\n│  └─────────────┘    └─────────────┘    └─────────────────┘ │\n│                                                             │\n│  ┌─────────────┐    ┌─────────────┐    ┌─────────────────┐ │\n│  │  Bundle     │    │  IPC        │    │   Rendering     │ │\n│  │  Analysis   │    │  Latency    │    │   (FPS)         │ │\n│  └─────────────┘    └─────────────┘    └─────────────────┘ │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Mesure du Temps de Démarrage\n\n```rust\nuse std::time::Instant;\nuse tauri::Manager;\n\npub struct StartupProfiler {\n    start_time: Instant,\n    checkpoints: Vec<(String, u128)>,\n}\n\nimpl StartupProfiler {\n    pub fn new() -> Self {\n        Self {\n            start_time: Instant::now(),\n            checkpoints: vec![],\n        }\n    }\n\n    pub fn checkpoint(&mut self, name: &str) {\n        let elapsed = self.start_time.elapsed().as_millis();\n        self.checkpoints.push((name.to_string(), elapsed));\n    }\n\n    pub fn report(&self) -> serde_json::Value {\n        let mut report = serde_json::Map::new();\n        let mut prev = 0u128;\n\n        for (name, time) in &self.checkpoints {\n            report.insert(\n                name.clone(),\n                serde_json::json!({\n                    \"total_ms\": time,\n                    \"step_ms\": time - prev\n                }),\n            );\n            prev = *time;\n        }\n\n        serde_json::Value::Object(report)\n    }\n}\n\n// Utilisation\npub fn setup_profiler<R: tauri::Runtime>(\n    app: &mut tauri::App<R>,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let mut profiler = StartupProfiler::new();\n\n    profiler.checkpoint(\"app_created\");\n\n    // Setup plugins\n    profiler.checkpoint(\"plugins_loaded\");\n\n    // Setup state\n    profiler.checkpoint(\"state_initialized\");\n\n    // Setup window\n    profiler.checkpoint(\"window_created\");\n\n    // Store profiler\n    app.manage(profiler);\n\n    Ok(())\n}\n\n#[tauri::command]\npub fn get_startup_profile(\n    profiler: tauri::State<StartupProfiler>,\n) -> serde_json::Value {\n    profiler.report()\n}\n```\n\n## Analyse de Bundle\n\n```rust\nuse std::collections::HashMap;\n\npub struct BundleAnalyzer;\n\nimpl BundleAnalyzer {\n    pub fn analyze_bindings(bindings: &[Binding]) -> BundleReport {\n        let mut by_crate: HashMap<String, usize> = HashMap::new();\n        let mut total_size = 0;\n\n        for binding in bindings {\n            let crate_name = binding.path.split(\"::\").next().unwrap_or(\"unknown\");\n            *by_crate.entry(crate_name.to_string()).or_insert(0) += binding.size;\n            total_size += binding.size;\n        }\n\n        let mut crates: Vec<_> = by_crate.into_iter().collect();\n        crates.sort_by(|a, b| b.1.cmp(&a.1));\n\n        BundleReport {\n            total_size,\n            crates,\n            recommendations: Self::generate_recommendations(bindings),\n        }\n    }\n\n    fn generate_recommendations(bindings: &[Binding]) -> Vec<String> {\n        let mut recommendations = vec![];\n\n        // Détecter les bindings lourds\n        let heavy_bindings: Vec<_> = bindings\n            .iter()\n            .filter(|b| b.size > 1000)\n            .collect();\n\n        if !heavy_bindings.is_empty() {\n            recommendations.push(format!(\n                \"Consider lazy loading {} heavy bindings\",\n                heavy_bindings.len()\n            ));\n        }\n\n        recommendations\n    }\n}\n\npub struct BundleReport {\n    pub total_size: usize,\n    pub crates: Vec<(String, usize)>,\n    pub recommendations: Vec<String>,\n}\n```\n\n## Optimisation de la Mémoire\n\n```rust\nuse std::sync::Arc;\nuse lru::LruCache;\n\npub struct MemoryOptimizedCache<T> {\n    cache: LruCache<String, Arc<T>>,\n    max_memory: usize,\n    current_memory: usize,\n}\n\nimpl<T: Sized> MemoryOptimizedCache<T> {\n    pub fn new(capacity: usize, max_memory_mb: usize) -> Self {\n        Self {\n            cache: LruCache::new(capacity),\n            max_memory: max_memory_mb * 1024 * 1024,\n            current_memory: 0,\n        }\n    }\n\n    pub fn get(&mut self, key: &str) -> Option<Arc<T>> {\n        self.cache.get(key).cloned()\n    }\n\n    pub fn put(&mut self, key: String, value: Arc<T>) {\n        let size = std::mem::size_of_val(&*value);\n\n        // Libérer de la mémoire si nécessaire\n        while self.current_memory + size > self.max_memory {\n            if let Some((_, old)) = self.cache.pop_lru() {\n                self.current_memory -= std::mem::size_of_val(&*old);\n            } else {\n                break;\n            }\n        }\n\n        self.cache.put(key, value);\n        self.current_memory += size;\n    }\n\n    pub fn memory_usage(&self) -> (usize, usize) {\n        (self.current_memory, self.max_memory)\n    }\n}\n```",
      "terminal_task": {
        "command": "cargo test performance -- --nocapture",
        "expected_output": "test performance::tests::test_startup_profiler ... ok",
        "hint": "Utilisez Instant pour le timing, LruCache pour la mémoire, et serde_json pour les rapports"
      }
    },
    {
      "id": "frontend-001-react-compiler",
      "track": "FRONTEND_ELITE",
      "level": "Expert",
      "title": "React 19 Compiler: Optimisation Automatique",
      "duration": "5h30",
      "description": "Maîtrise du React Compiler, mémoïsation automatique, et patterns de performance avancés.",
      "markdown_content": "# React 19 Compiler\n\n## Architecture du Compiler\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    REACT COMPILER                           │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  ┌─────────────┐    ┌─────────────┐    ┌─────────────────┐ │\n│  │    JSX      │───▶│    AST      │───▶│  Analyse de      │ │\n│  │   Source    │    │   (Babel)   │    │  dépendances    │ │\n│  └─────────────┘    └─────────────┘    └────────┬────────┘ │\n│                                                  │          │\n│  ┌─────────────┐    ┌─────────────┐    ┌────────▼────────┐ │\n│  │  Output     │◄───│  Code Gen   │◄───│  Insertion de   │ │\n│  │  Optimisé   │    │             │    │  useMemo/cache  │ │\n│  └─────────────┘    └─────────────┘    └─────────────────┘ │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Configuration du Compiler\n\n```typescript\n// babel.config.js\nmodule.exports = {\n  presets: [\n    ['@babel/preset-react', { runtime: 'automatic' }]\n  ],\n  plugins: [\n    ['babel-plugin-react-compiler', {\n      // Optimisation agressive\n      memoizationStrategy: 'aggressive',\n      // Conserver les noms pour le debug\n      preserveMemoNames: process.env.NODE_ENV === 'development',\n      // Rapport de compilation\n      reportCompilation: true,\n    }]\n  ]\n};\n```\n\n## Code: Composant Auto-Mémoïsé\n\n```typescript\n// Avant (manuel)\nimport { memo, useMemo, useCallback } from 'react';\n\nconst ExpensiveList = memo(function ExpensiveList({ items, onSelect }) {\n  const sorted = useMemo(() => \n    [...items].sort((a, b) => b.score - a.score),\n    [items]\n  );\n\n  const handleClick = useCallback((id: string) => {\n    onSelect(id);\n  }, [onSelect]);\n\n  return (\n    <ul>\n      {sorted.map(item => (\n        <li key={item.id} onClick={() => handleClick(item.id)}>\n          {item.name}\n        </li>\n      ))}\n    </ul>\n  );\n});\n\n// Après (React Compiler)\n// 'use memo' directive active la mémoïsation automatique\n'use memo';\n\nfunction ExpensiveList({ items, onSelect }) {\n  // Le compiler injecte automatiquement useMemo/useCallback\n  const sorted = [...items].sort((a, b) => b.score - a.score);\n\n  const handleClick = (id: string) => {\n    onSelect(id);\n  };\n\n  return (\n    <ul>\n      {sorted.map(item => (\n        <li key={item.id} onClick={() => handleClick(item.id)}>\n          {item.name}\n        </li>\n      ))}\n    </ul>\n  );\n}\n```\n\n## 'use cache' Directive\n\n```typescript\n// Server Component avec cache automatique\n'use cache';\n\nimport { unstable_cacheLife as cacheLife } from 'next/cache';\n\n// Cache pendant 1 heure\nexport const revalidate = 3600;\n\nasync function getProductData(id: string) {\n  // Cette fonction est automatiquement mémoïsée\n  const response = await fetch(`https://api.example.com/products/${id}`);\n  return response.json();\n}\n\nexport default async function ProductPage({ params }: { params: { id: string } }) {\n  const product = await getProductData(params.id);\n\n  return (\n    <div>\n      <h1>{product.name}</h1>\n      <p>{product.description}</p>\n    </div>\n  );\n}\n\n// Cache avec tags pour invalidation\nasync function getDashboardData() {\n  'use cache';\n  cacheLife('hours');\n\n  const data = await fetchDashboard();\n  return data;\n}\n\n// Invalidation programmatique\nimport { revalidateTag } from 'next/cache';\n\nexport async function updateDashboard() {\n  // ... update logic\n  revalidateTag('dashboard');\n}\n```\n\n## Analyse des Dépendances\n\n```typescript\n// Le compiler analyse les dépendances automatiquement\nfunction useUserData(userId: string) {\n  // Détecté comme dépendance: userId\n  const user = useSyncExternalStore(\n    subscribe,\n    () => getUser(userId),\n    () => getUser(userId)\n  );\n\n  // Détecté comme dépendance: user.preferences\n  const theme = useMemo(() => {\n    return user.preferences.theme ?? 'light';\n  }, [user.preferences.theme]); // Auto-injecté par le compiler\n\n  return { user, theme };\n}\n\n// Patterns à éviter (empêchent l'optimisation)\nfunction BadPattern() {\n  // ❌ Dépendances dynamiques\n  const deps = [a, b, c];\n  const value = useMemo(() => compute(), deps);\n\n  // ❌ Mutations après création\n  const obj = useMemo(() => ({ a: 1 }), []);\n  obj.a = 2; // Mutation!\n\n  // ✅ Correct\n  const value = useMemo(() => compute(a, b, c), [a, b, c]);\n  const obj = useMemo(() => ({ a: 1 }), []);\n}\n```",
      "terminal_task": {
        "command": "npm run test:compiler",
        "expected_output": "✓ All compiler optimizations passed",
        "hint": "Configurez babel-plugin-react-compiler avec memoizationStrategy: 'aggressive', utilisez 'use memo' et 'use cache' directives, et vérifiez le rapport de compilation"
      }
    },
    {
      "id": "frontend-002-webgpu",
      "track": "FRONTEND_ELITE",
      "level": "Expert",
      "title": "WebGPU: Compute Shaders et Rendu Haute Performance",
      "duration": "12h00",
      "description": "Programmation GPU dans le navigateur, compute shaders, et traitement parallèle de données massives.",
      "markdown_content": "# WebGPU Mastery\n\n## Architecture WebGPU\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    WEBGPU CONTEXT                           │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  ┌─────────────┐    ┌─────────────┐    ┌─────────────────┐ │\n│  │   Adapter   │───▶│   Device    │───▶│  Command Queue  │ │\n│  │  (GPU sel)  │    │  (context)  │    │                 │ │\n│  └─────────────┘    └──────┬──────┘    └─────────────────┘ │\n│                            │                                │\n│           ┌────────────────┼────────────────┐               │\n│           ▼                ▼                ▼               │\n│    ┌──────────┐     ┌──────────┐     ┌──────────┐         │\n│    │ Buffers  │     │ Textures │     │ Samplers │         │\n│    │ (data)   │     │ (images) │     │ (filter) │         │\n│    └────┬─────┘     └────┬─────┘     └──────────┘         │\n│         │                │                                  │\n│         └────────────────┴────────────────┐               │\n│                            ▼                                │\n│                   ┌─────────────────┐                       │\n│                   │  Shader Modules │                       │\n│                   │  (WGSL/SPV)     │                       │\n│                   └─────────────────┘                       │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Initialisation et Contexte\n\n```typescript\ninterface GPUContext {\n  adapter: GPUAdapter;\n  device: GPUDevice;\n  context: GPUCanvasContext;\n  format: GPUTextureFormat;\n}\n\nasync function initWebGPU(canvas: HTMLCanvasElement): Promise<GPUContext> {\n  if (!navigator.gpu) {\n    throw new Error('WebGPU non supporté');\n  }\n\n  const adapter = await navigator.gpu.requestAdapter({\n    powerPreference: 'high-performance',\n  });\n\n  if (!adapter) {\n    throw new Error('Aucun adaptateur GPU trouvé');\n  }\n\n  const device = await adapter.requestDevice({\n    requiredFeatures: ['shader-f16'],\n    requiredLimits: {\n      maxStorageBufferBindingSize: 268435456, // 256MB\n    },\n  });\n\n  const context = canvas.getContext('webgpu')!;\n  const format = navigator.gpu.getPreferredCanvasFormat();\n\n  context.configure({\n    device,\n    format,\n    alphaMode: 'premultiplied',\n  });\n\n  return { adapter, device, context, format };\n}\n```\n\n## Compute Shader: Multiplication Matricielle\n\n```typescript\n// WGSL Shader\nconst matrixMultiplyShader = `\n  @group(0) @binding(0) var<storage, read> matrixA: array<f32>;\n  @group(0) @binding(1) var<storage, read> matrixB: array<f32>;\n  @group(0) @binding(2) var<storage, read_write> result: array<f32>;\n\n  struct Uniforms {\n    N: u32,\n    M: u32,\n    K: u32,\n  }\n  @group(0) @binding(3) var<uniform> uniforms: Uniforms;\n\n  @compute @workgroup_size(16, 16)\n  fn main(@builtin(global_invocation_id) global_id: vec3<u32>) {\n    let row = global_id.y;\n    let col = global_id.x;\n\n    if (row >= uniforms.N || col >= uniforms.M) {\n      return;\n    }\n\n    var sum: f32 = 0.0;\n    for (var k: u32 = 0; k < uniforms.K; k = k + 1) {\n      let a = matrixA[row * uniforms.K + k];\n      let b = matrixB[k * uniforms.M + col];\n      sum = sum + a * b;\n    }\n\n    result[row * uniforms.M + col] = sum;\n  }\n`;\n\n// JavaScript Implementation\nasync function matrixMultiplyGPU(\n  device: GPUDevice,\n  A: Float32Array,\n  B: Float32Array,\n  N: number,\n  M: number,\n  K: number\n): Promise<Float32Array> {\n  // Créer les buffers\n  const bufferA = device.createBuffer({\n    size: A.byteLength,\n    usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,\n  });\n\n  const bufferB = device.createBuffer({\n    size: B.byteLength,\n    usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,\n  });\n\n  const resultSize = N * M * 4;\n  const bufferResult = device.createBuffer({\n    size: resultSize,\n    usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC,\n  });\n\n  const uniformBuffer = device.createBuffer({\n    size: 12, // 3 x u32\n    usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST,\n  });\n\n  // Écrire les données\n  device.queue.writeBuffer(bufferA, 0, A);\n  device.queue.writeBuffer(bufferB, 0, B);\n  device.queue.writeBuffer(\n    uniformBuffer,\n    0,\n    new Uint32Array([N, M, K])\n  );\n\n  // Créer le shader module\n  const shaderModule = device.createShaderModule({\n    code: matrixMultiplyShader,\n  });\n\n  // Créer le pipeline\n  const pipeline = device.createComputePipeline({\n    layout: 'auto',\n    compute: {\n      module: shaderModule,\n      entryPoint: 'main',\n    },\n  });\n\n  // Créer le bind group\n  const bindGroup = device.createBindGroup({\n    layout: pipeline.getBindGroupLayout(0),\n    entries: [\n      { binding: 0, resource: { buffer: bufferA } },\n      { binding: 1, resource: { buffer: bufferB } },\n      { binding: 2, resource: { buffer: bufferResult } },\n      { binding: 3, resource: { buffer: uniformBuffer } },\n    ],\n  });\n\n  // Encoder et soumettre\n  const commandEncoder = device.createCommandEncoder();\n  const computePass = commandEncoder.beginComputePass();\n\n  computePass.setPipeline(pipeline);\n  computePass.setBindGroup(0, bindGroup);\n  computePass.dispatchWorkgroups(\n    Math.ceil(M / 16),\n    Math.ceil(N / 16)\n  );\n\n  computePass.end();\n\n  // Copier le résultat\n  const readBuffer = device.createBuffer({\n    size: resultSize,\n    usage: GPUBufferUsage.COPY_DST | GPUBufferUsage.MAP_READ,\n  });\n\n  commandEncoder.copyBufferToBuffer(\n    bufferResult,\n    0,\n    readBuffer,\n    0,\n    resultSize\n  );\n\n  device.queue.submit([commandEncoder.finish()]);\n\n  // Lire le résultat\n  await readBuffer.mapAsync(GPUMapMode.READ);\n  const result = new Float32Array(readBuffer.getMappedRange().slice(0));\n  readBuffer.unmap();\n\n  return result;\n}\n```",
      "terminal_task": {
        "command": "npm run test:webgpu",
        "expected_output": "✓ Matrix multiplication: 4096x4096 in 45ms",
        "hint": "Utilisez GPUShaderStage.COMPUTE pour les compute shaders, dispatchWorkgroups avec la taille appropriée, et mapAsync pour récupérer les résultats"
      }
    },
    {
      "id": "frontend-003-signals",
      "track": "FRONTEND_ELITE",
      "level": "Expert",
      "title": "Signals: Reactivité Fine-Grained",
      "duration": "6h00",
      "description": "Architecture des signals, effets dérivés, et intégration avec React.",
      "markdown_content": "# Signals: Reactivité Fine-Grained\n\n## Architecture des Signals\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    SIGNALS SYSTEM                           │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  ┌─────────────┐         ┌─────────────┐                   │\n│  │   Signal    │◄───────▶│  Effect     │                   │\n│  │  (state)    │         │  (reaction) │                   │\n│  └──────┬──────┘         └─────────────┘                   │\n│         │                                                   │\n│         ▼                                                   │\n│  ┌─────────────┐         ┌─────────────┐                   │\n│  │  Computed   │◄───────▶│  Memo       │                   │\n│  │  (derived)  │         │  (cached)   │                   │\n│  └─────────────┘         └─────────────┘                   │\n│                                                             │\n│  Dependency Graph: A ──► B ──► C                           │\n│                    └──► D ──► E                             │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Implementation from Scratch\n\n```typescript\n// Types\ninterface Subscriber {\n  execute(): void;\n  dependencies: Set<Signal<any>>;\n}\n\n// Context global\nlet currentEffect: Subscriber | null = null;\nconst effectQueue: Set<Subscriber> = new Set();\nlet isFlushing = false;\n\n// Signal class\nclass Signal<T> {\n  private _value: T;\n  private _subscribers: Set<Subscriber> = new Set();\n\n  constructor(initialValue: T) {\n    this._value = initialValue;\n  }\n\n  get value(): T {\n    // Track dependency\n    if (currentEffect) {\n      this._subscribers.add(currentEffect);\n      currentEffect.dependencies.add(this);\n    }\n    return this._value;\n  }\n\n  set value(newValue: T) {\n    if (this._value !== newValue) {\n      this._value = newValue;\n      this._notify();\n    }\n  }\n\n  private _notify(): void {\n    // Queue all subscribers\n    for (const subscriber of this._subscribers) {\n      effectQueue.add(subscriber);\n    }\n\n    // Flush in next microtask\n    if (!isFlushing) {\n      isFlushing = true;\n      queueMicrotask(() => this._flush());\n    }\n  }\n\n  private _flush(): void {\n    for (const subscriber of effectQueue) {\n      subscriber.execute();\n    }\n    effectQueue.clear();\n    isFlushing = false;\n  }\n\n  // Unsubscribe a subscriber\n  _unsubscribe(subscriber: Subscriber): void {\n    this._subscribers.delete(subscriber);\n  }\n}\n\n// Signal factory\nexport function signal<T>(initialValue: T): Signal<T> {\n  return new Signal(initialValue);\n}\n\n// Effect\nexport function effect(fn: () => void | (() => void)): () => void {\n  let cleanup: (() => void) | void;\n\n  const subscriber: Subscriber = {\n    execute: () => {\n      // Cleanup previous\n      if (cleanup) cleanup();\n\n      // Clear old dependencies\n      for (const dep of subscriber.dependencies) {\n        dep._unsubscribe(subscriber);\n      }\n      subscriber.dependencies.clear();\n\n      // Run with new context\n      const prevEffect = currentEffect;\n      currentEffect = subscriber;\n      try {\n        cleanup = fn();\n      } finally {\n        currentEffect = prevEffect;\n      }\n    },\n    dependencies: new Set(),\n  };\n\n  // Initial run\n  subscriber.execute();\n\n  // Return cleanup\n  return () => {\n    if (cleanup) cleanup();\n    for (const dep of subscriber.dependencies) {\n      dep._unsubscribe(subscriber);\n    }\n  };\n}\n\n// Computed\nexport function computed<T>(fn: () => T): Signal<T> {\n  const computedSignal = signal<T>(undefined as T);\n  let isFirstRun = true;\n\n  effect(() => {\n    const value = fn();\n    if (isFirstRun || computedSignal.value !== value) {\n      computedSignal.value = value;\n      isFirstRun = false;\n    }\n  });\n\n  return computedSignal;\n}\n\n// Usage\nconst count = signal(0);\nconst doubled = computed(() => count.value * 2);\nconst isEven = computed(() => count.value % 2 === 0);\n\neffect(() => {\n  console.log('Count:', count.value);\n  console.log('Doubled:', doubled.value);\n});\n\ncount.value = 5; // Triggers effect\n```\n\n## Integration React\n\n```typescript\nimport { useSyncExternalStore, useCallback } from 'react';\n\n// Hook pour les signals\nexport function useSignal<T>(signal: Signal<T>): T {\n  return useSyncExternalStore(\n    (callback) => {\n      const cleanup = effect(() => {\n        // Access to subscribe\n        signal.value;\n        callback();\n      });\n      return cleanup;\n    },\n    () => signal.value,\n    () => signal.value\n  );\n}\n\n// Hook pour créer un signal local\nexport function useLocalSignal<T>(initialValue: T): [Signal<T>, (v: T) => void] {\n  const [signal] = useState(() => signal(initialValue));\n\n  const setValue = useCallback((value: T) => {\n    signal.value = value;\n  }, [signal]);\n\n  return [signal, setValue];\n}\n\n// Composant avec signals\nfunction Counter() {\n  const count = useSignal(counterSignal);\n  const doubled = useSignal(doubledSignal);\n\n  return (\n    <div>\n      <p>Count: {count}</p>\n      <p>Doubled: {doubled}</p>\n      <button onClick={() => counterSignal.value++}>\n        Increment\n      </button>\n    </div>\n  );\n}\n```",
      "terminal_task": {
        "command": "npm run test:signals",
        "expected_output": "✓ Signal reactivity tests passed",
        "hint": "Implémentez un context global pour currentEffect, utilisez Set pour les subscribers, et queueMicrotask pour le batching"
      }
    },
    {
      "id": "frontend-004-virtual-list",
      "track": "FRONTEND_ELITE",
      "level": "Expert",
      "title": "Virtualisation de Listes Massives",
      "duration": "5h30",
      "description": "Virtual scrolling, windowing, et optimisation du rendu à grande échelle.",
      "markdown_content": "# Virtualisation de Listes\n\n## Architecture Virtual List\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    VIRTUAL LIST                             │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  ┌─────────────────────────────────────────────────────┐   │\n│  │              VISIBLE VIEWPORT                       │   │\n│  │  ┌─────┐  ┌─────┐  ┌─────┐  ┌─────┐  ┌─────┐      │   │\n│  │  │Item │  │Item │  │Item │  │Item │  │Item │      │   │\n│  │  │ 10  │  │ 11  │  │ 12  │  │ 13  │  │ 14  │      │   │\n│  │  └─────┘  └─────┘  └─────┘  └─────┘  └─────┘      │   │\n│  └─────────────────────────────────────────────────────┘   │\n│                                                             │\n│  Buffer (invisible)        Buffer (invisible)              │\n│  ┌─────┐ ┌─────┐           ┌─────┐ ┌─────┐                │\n│  │ 8   │ │ 9   │           │ 15  │ │ 16  │                │\n│  └─────┘ └─────┘           └─────┘ └─────┘                │\n│                                                             │\n│  Total items: 100,000                                       │\n│  Rendered items: 9 (viewport + 2 buffers)                   │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Implementation\n\n```typescript\nimport { useRef, useState, useCallback, useMemo, useEffect } from 'react';\n\ninterface VirtualListProps<T> {\n  items: T[];\n  itemHeight: number;\n  overscan?: number;\n  renderItem: (item: T, index: number) => React.ReactNode;\n  containerHeight: number;\n}\n\ninterface VirtualItem<T> {\n  item: T;\n  index: number;\n  style: React.CSSProperties;\n}\n\nexport function VirtualList<T>({\n  items,\n  itemHeight,\n  overscan = 3,\n  renderItem,\n  containerHeight,\n}: VirtualListProps<T>) {\n  const containerRef = useRef<HTMLDivElement>(null);\n  const [scrollTop, setScrollTop] = useState(0);\n\n  // Calculer les items visibles\n  const virtualItems = useMemo(() => {\n    const totalHeight = items.length * itemHeight;\n    const startIndex = Math.max(0, Math.floor(scrollTop / itemHeight) - overscan);\n    const visibleCount = Math.ceil(containerHeight / itemHeight) + overscan * 2;\n    const endIndex = Math.min(items.length, startIndex + visibleCount);\n\n    const virtualItems: VirtualItem<T>[] = [];\n\n    for (let i = startIndex; i < endIndex; i++) {\n      virtualItems.push({\n        item: items[i],\n        index: i,\n        style: {\n          position: 'absolute',\n          top: i * itemHeight,\n          height: itemHeight,\n          left: 0,\n          right: 0,\n        },\n      });\n    }\n\n    return {\n      items: virtualItems,\n      totalHeight,\n      startIndex,\n      endIndex,\n    };\n  }, [items, itemHeight, scrollTop, containerHeight, overscan]);\n\n  // Handler de scroll\n  const handleScroll = useCallback((e: React.UIEvent<HTMLDivElement>) => {\n    setScrollTop(e.currentTarget.scrollTop);\n  }, []);\n\n  return (\n    <div\n      ref={containerRef}\n      onScroll={handleScroll}\n      style={{\n        height: containerHeight,\n        overflow: 'auto',\n        position: 'relative',\n      }}\n    >\n      <div style={{ height: virtualItems.totalHeight, position: 'relative' }}>\n        {virtualItems.items.map(({ item, index, style }) => (\n          <div key={index} style={style}>\n            {renderItem(item, index)}\n          </div>\n        ))}\n      </div>\n    </div>\n  );\n}\n\n// Version avec hauteurs variables\ninterface VariableHeightItem {\n  height: number;\n  [key: string]: any;\n}\n\nexport function VariableHeightVirtualList<T extends VariableHeightItem>({\n  items,\n  overscan = 3,\n  renderItem,\n  containerHeight,\n  estimateHeight,\n}: {\n  items: T[];\n  overscan?: number;\n  renderItem: (item: T, index: number) => React.ReactNode;\n  containerHeight: number;\n  estimateHeight: (item: T) => number;\n}) {\n  const containerRef = useRef<HTMLDivElement>(null);\n  const [scrollTop, setScrollTop] = useState(0);\n  const measuredHeights = useRef<Map<number, number>>(new Map());\n\n  // Calculer les positions cumulées\n  const positions = useMemo(() => {\n    const pos: number[] = [0];\n    for (let i = 0; i < items.length; i++) {\n      const height = measuredHeights.current.get(i) ?? estimateHeight(items[i]);\n      pos.push(pos[i] + height);\n    }\n    return pos;\n  }, [items, estimateHeight]);\n\n  // Mesurer les éléments réels\n  const measureElement = useCallback((index: number, element: HTMLElement | null) => {\n    if (element) {\n      const height = element.getBoundingClientRect().height;\n      measuredHeights.current.set(index, height);\n    }\n  }, []);\n\n  // Trouver l'index de départ\n  const startIndex = useMemo(() => {\n    let low = 0;\n    let high = positions.length - 1;\n\n    while (low < high) {\n      const mid = Math.floor((low + high) / 2);\n      if (positions[mid] < scrollTop) {\n        low = mid + 1;\n      } else {\n        high = mid;\n      }\n    }\n\n    return Math.max(0, low - overscan);\n  }, [scrollTop, positions, overscan]);\n\n  const endIndex = useMemo(() => {\n    const viewportBottom = scrollTop + containerHeight;\n    let index = startIndex;\n    while (index < items.length && positions[index] < viewportBottom) {\n      index++;\n    }\n    return Math.min(items.length, index + overscan);\n  }, [scrollTop, containerHeight, positions, startIndex, items.length, overscan]);\n\n  const virtualItems = useMemo(() => {\n    return items.slice(startIndex, endIndex).map((item, i) => {\n      const index = startIndex + i;\n      return {\n        item,\n        index,\n        style: {\n          position: 'absolute',\n          top: positions[index],\n          left: 0,\n          right: 0,\n        } as React.CSSProperties,\n      };\n    });\n  }, [items, startIndex, endIndex, positions]);\n\n  const totalHeight = positions[positions.length - 1];\n\n  return (\n    <div\n      ref={containerRef}\n      onScroll={(e) => setScrollTop(e.currentTarget.scrollTop)}\n      style={{\n        height: containerHeight,\n        overflow: 'auto',\n        position: 'relative',\n      }}\n    >\n      <div style={{ height: totalHeight, position: 'relative' }}>\n        {virtualItems.map(({ item, index, style }) => (\n          <div\n            key={index}\n            ref={(el) => measureElement(index, el)}\n            style={style}\n          >\n            {renderItem(item, index)}\n          </div>\n        ))}\n      </div>\n    </div>\n  );\n}\n```",
      "terminal_task": {
        "command": "npm run test:virtual-list",
        "expected_output": "✓ Virtual list renders 10 items for 100000 total",
        "hint": "Utilisez useMemo pour les calculs de positions, binary search pour l'index de départ, et measureElement pour les hauteurs variables"
      }
    },
    {
      "id": "frontend-005-state-machines",
      "track": "FRONTEND_ELITE",
      "level": "Expert",
      "title": "Machines à États: XState et Patterns",
      "duration": "6h00",
      "description": "Finite State Machines, statecharts, et gestion de logique complexe.",
      "markdown_content": "# Machines à États\n\n## Architecture State Machine\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    STATE MACHINE                            │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  ┌─────────┐    event:FETCH    ┌─────────┐                 │\n│  │  idle   │ ────────────────▶ │loading  │                 │\n│  └─────────┘                   └────┬────┘                 │\n│       ▲                             │                       │\n│       │                             │ event:RESOLVE         │\n│       │ event:RETRY                 ▼                       │\n│       │                        ┌─────────┐                 │\n│       └─────────────────────── │ success │                 │\n│                                └─────────┘                 │\n│                                                             │\n│  ┌─────────┐                   ┌─────────┐                 │\n│  │  idle   │ ──event:FETCH──▶ │loading  │                 │\n│  └─────────┘                   └────┬────┘                 │\n│       ▲                             │                       │\n│       │                             │ event:REJECT          │\n│       │                             ▼                       │\n│       └────────────────────────┌─────────┐                 │\n│                                │  error  │                 │\n│                                └─────────┘                 │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Implementation from Scratch\n\n```typescript\n// Types\ntype StateValue = string;\ntype EventType = string;\n\ninterface Event {\n  type: EventType;\n  [key: string]: any;\n}\n\ninterface State {\n  value: StateValue;\n  context: any;\n}\n\ninterface Transition {\n  target: StateValue;\n  actions?: Action[];\n  cond?: (context: any, event: Event) => boolean;\n}\n\ninterface StateConfig {\n  on?: Record<EventType, Transition | Transition[]>;\n  entry?: Action[];\n  exit?: Action[];\n}\n\ninterface MachineConfig {\n  id: string;\n  initial: StateValue;\n  context: any;\n  states: Record<StateValue, StateConfig>;\n}\n\ntype Action = (context: any, event: Event) => any;\n\n// State Machine class\nclass StateMachine {\n  private config: MachineConfig;\n  private currentState: State;\n  private listeners: Set<(state: State) => void> = new Set();\n\n  constructor(config: MachineConfig) {\n    this.config = config;\n    this.currentState = {\n      value: config.initial,\n      context: config.context,\n    };\n  }\n\n  get state(): State {\n    return this.currentState;\n  }\n\n  send(event: Event): void {\n    const stateConfig = this.config.states[this.currentState.value];\n\n    if (!stateConfig.on || !stateConfig.on[event.type]) {\n      return; // No transition\n    }\n\n    const transitions = Array.isArray(stateConfig.on[event.type])\n      ? stateConfig.on[event.type] as Transition[]\n      : [stateConfig.on[event.type] as Transition];\n\n    // Find valid transition\n    const transition = transitions.find(t => \n      !t.cond || t.cond(this.currentState.context, event)\n    );\n\n    if (!transition) return;\n\n    // Execute exit actions\n    if (stateConfig.exit) {\n      for (const action of stateConfig.exit) {\n        this.currentState.context = action(this.currentState.context, event);\n      }\n    }\n\n    // Execute transition actions\n    if (transition.actions) {\n      for (const action of transition.actions) {\n        this.currentState.context = action(this.currentState.context, event);\n      }\n    }\n\n    // Change state\n    this.currentState = {\n      value: transition.target,\n      context: this.currentState.context,\n    };\n\n    // Execute entry actions\n    const newStateConfig = this.config.states[transition.target];\n    if (newStateConfig.entry) {\n      for (const action of newStateConfig.entry) {\n        this.currentState.context = action(this.currentState.context, event);\n      }\n    }\n\n    // Notify listeners\n    for (const listener of this.listeners) {\n      listener(this.currentState);\n    }\n  }\n\n  subscribe(listener: (state: State) => void): () => void {\n    this.listeners.add(listener);\n    return () => this.listeners.delete(listener);\n  }\n\n  can(eventType: EventType): boolean {\n    const stateConfig = this.config.states[this.currentState.value];\n    return !!stateConfig.on?.[eventType];\n  }\n}\n\n// Factory function\nexport function createMachine(config: MachineConfig): StateMachine {\n  return new StateMachine(config);\n}\n\n// Usage: Fetch Machine\nconst fetchMachine = createMachine({\n  id: 'fetch',\n  initial: 'idle',\n  context: {\n    data: null,\n    error: null,\n  },\n  states: {\n    idle: {\n      on: {\n        FETCH: {\n          target: 'loading',\n          actions: [\n            (ctx) => ({ ...ctx, error: null }),\n          ],\n        },\n      },\n    },\n    loading: {\n      entry: [\n        (ctx, event) => {\n          // Start fetch\n          fetchData(event.url).then(\n            data => fetchMachine.send({ type: 'RESOLVE', data }),\n            error => fetchMachine.send({ type: 'REJECT', error }),\n          );\n          return ctx;\n        },\n      ],\n      on: {\n        RESOLVE: {\n          target: 'success',\n          actions: [\n            (ctx, event) => ({ ...ctx, data: event.data }),\n          ],\n        },\n        REJECT: {\n          target: 'error',\n          actions: [\n            (ctx, event) => ({ ...ctx, error: event.error }),\n          ],\n        },\n      },\n    },\n    success: {\n      on: {\n        FETCH: 'loading',\n      },\n    },\n    error: {\n      on: {\n        RETRY: 'loading',\n      },\n    },\n  },\n});\n```",
      "terminal_task": {
        "command": "npm run test:state-machine",
        "expected_output": "✓ State machine transitions correctly",
        "hint": "Implémentez un système de transitions avec conditions, actions entry/exit/transition, et un système de subscription"
      }
    },
    {
      "id": "frontend-006-micro-frontends",
      "track": "FRONTEND_ELITE",
      "level": "Expert",
      "title": "Micro-Frontends: Architecture à Grande Échelle",
      "duration": "8h00",
      "description": "Module Federation, import maps, et composition d'applications.",
      "markdown_content": "# Micro-Frontends\n\n## Architecture Micro-Frontend\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    SHELL APPLICATION                        │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────┐ │\n│  │   Header    │  │   Router    │  │   Event Bus         │ │\n│  │  (shared)   │  │  (shell)    │  │  (communication)    │ │\n│  └─────────────┘  └──────┬──────┘  └─────────────────────┘ │\n│                          │                                  │\n│           ┌──────────────┼──────────────┐                  │\n│           ▼              ▼              ▼                  │\n│    ┌────────────┐  ┌────────────┐  ┌────────────┐         │\n│    │  Catalog   │  │   Cart     │  │  Checkout  │         │\n│    │  (team A)  │  │  (team B)  │  │  (team C)  │         │\n│    │  React     │  │  Vue       │  │  Svelte    │         │\n│    └────────────┘  └────────────┘  └────────────┘         │\n│                                                             │\n│  Independent deployment, shared dependencies                │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Module Federation (Vite)\n\n```typescript\n// vite.config.ts - Shell\nimport { defineConfig } from 'vite';\nimport federation from '@originjs/vite-plugin-federation';\n\nexport default defineConfig({\n  plugins: [\n    federation({\n      name: 'shell',\n      remotes: {\n        catalog: 'http://localhost:3001/assets/remoteEntry.js',\n        cart: 'http://localhost:3002/assets/remoteEntry.js',\n        checkout: 'http://localhost:3003/assets/remoteEntry.js',\n      },\n      shared: ['react', 'react-dom', 'react-router-dom'],\n    }),\n  ],\n  build: {\n    target: 'esnext',\n  },\n});\n\n// vite.config.ts - Remote (Catalog)\nimport { defineConfig } from 'vite';\nimport federation from '@originjs/vite-plugin-federation';\n\nexport default defineConfig({\n  plugins: [\n    federation({\n      name: 'catalog',\n      filename: 'remoteEntry.js',\n      exposes: {\n        './ProductList': './src/components/ProductList.tsx',\n        './ProductDetail': './src/components/ProductDetail.tsx',\n        './productService': './src/services/productService.ts',\n      },\n      shared: ['react', 'react-dom'],\n    }),\n  ],\n  build: {\n    target: 'esnext',\n    minify: false,\n    cssCodeSplit: false,\n  },\n});\n```\n\n## Dynamic Import avec Import Maps\n\n```html\n<!-- index.html -->\n<!DOCTYPE html>\n<html>\n<head>\n  <script type=\"importmap\">\n  {\n    \"imports\": {\n      \"react\": \"https://esm.sh/react@18\",\n      \"react-dom\": \"https://esm.sh/react-dom@18\",\n      \"shell/\": \"./src/\"\n    }\n  }\n  </script>\n</head>\n<body>\n  <div id=\"root\"></div>\n  <script type=\"module\" src=\"./src/main.tsx\"></script>\n</body>\n</html>\n```\n\n```typescript\n// Shell: Dynamic Component Loader\nimport { lazy, Suspense, useEffect, useState } from 'react';\n\ninterface RemoteComponentProps {\n  remoteUrl: string;\n  scope: string;\n  module: string;\n  fallback?: React.ReactNode;\n}\n\n// Load remote module\nasync function loadRemoteModule(remoteUrl: string, scope: string, module: string) {\n  // Inject script\n  await new Promise<void>((resolve, reject) => {\n    const script = document.createElement('script');\n    script.src = remoteUrl;\n    script.type = 'text/javascript';\n    script.async = true;\n    script.onload = () => resolve();\n    script.onerror = reject;\n    document.head.appendChild(script);\n  });\n\n  // Get module from global scope\n  const container = (window as any)[scope];\n  await container.init(__webpack_share_scopes__.default);\n  const factory = await container.get(module);\n  return factory();\n}\n\n// Hook for remote component\nexport function useRemoteComponent(remoteUrl: string, scope: string, module: string) {\n  const [Component, setComponent] = useState<React.ComponentType<any> | null>(null);\n  const [error, setError] = useState<Error | null>(null);\n  const [loading, setLoading] = useState(true);\n\n  useEffect(() => {\n    loadRemoteModule(remoteUrl, scope, module)\n      .then((module) => {\n        setComponent(() => module.default);\n        setLoading(false);\n      })\n      .catch((err) => {\n        setError(err);\n        setLoading(false);\n      });\n  }, [remoteUrl, scope, module]);\n\n  return { Component, error, loading };\n}\n\n// Remote Component wrapper\nexport function RemoteComponent({\n  remoteUrl,\n  scope,\n  module,\n  fallback = <div>Loading...</div>,\n  ...props\n}: RemoteComponentProps & Record<string, any>) {\n  const { Component, error, loading } = useRemoteComponent(remoteUrl, scope, module);\n\n  if (loading) return <>{fallback}</>;\n  if (error) return <div>Error loading component: {error.message}</div>;\n  if (!Component) return null;\n\n  return (\n    <Suspense fallback={fallback}>\n      <Component {...props} />\n    </Suspense>\n  );\n}\n\n// Usage\nfunction App() {\n  return (\n    <div>\n      <h1>Shell Application</h1>\n\n      <RemoteComponent\n        remoteUrl=\"http://localhost:3001/assets/remoteEntry.js\"\n        scope=\"catalog\"\n        module=\"./ProductList\"\n        category=\"electronics\"\n      />\n    </div>\n  );\n}\n```",
      "terminal_task": {
        "command": "npm run test:micro-frontends",
        "expected_output": "✓ Remote component loaded successfully",
        "hint": "Utilisez @originjs/vite-plugin-federation, loadRemoteModule pour le chargement dynamique, et Suspense pour le fallback"
      }
    },
    {
      "id": "frontend-007-graphql-advanced",
      "track": "FRONTEND_ELITE",
      "level": "Expert",
      "title": "GraphQL Avancé: Cache et Optimisations",
      "duration": "7h00",
      "description": "Apollo Client avancé, normalisation de cache, et patterns de données.",
      "markdown_content": "# GraphQL Avancé\n\n## Architecture Cache Apollo\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    APOLLO CACHE                             │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  ROOT_QUERY                                                 │\n│  ├── user({\"id\":\"1\"}) ──▶ User:1                           │\n│  ├── users ──▶ [User:1, User:2, User:3]                    │\n│  └── posts ──▶ [Post:1, Post:2]                            │\n│                                                             │\n│  User:1                                                     │\n│  ├── id: \"1\"                                                │\n│  ├── name: \"Alice\"                                          │\n│  ├── email: \"alice@example.com\"                             │\n│  └── posts ──▶ [Post:1, Post:3]                            │\n│                                                             │\n│  Post:1                                                     │\n│  ├── id: \"1\"                                                │\n│  ├── title: \"Hello\"                                         │\n│  └── author ──▶ User:1 (circular ref)                      │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Cache Configuration Avancée\n\n```typescript\nimport { ApolloClient, InMemoryCache, gql } from '@apollo/client';\n\nconst cache = new InMemoryCache({\n  typePolicies: {\n    Query: {\n      fields: {\n        // Pagination avec merge\n        users: {\n          keyArgs: ['filter'],\n          merge(existing = [], incoming, { args }) {\n            if (args?.offset === 0) {\n              return incoming;\n            }\n            return [...existing, ...incoming];\n          },\n          read(existing, { args }) {\n            const { offset = 0, limit = existing?.length } = args || {};\n            return existing?.slice(offset, offset + limit);\n          },\n        },\n\n        // Cache séparé par ID\n        user: {\n          read(_, { args, toReference }) {\n            return toReference({\n              __typename: 'User',\n              id: args?.id,\n            });\n          },\n        },\n      },\n    },\n\n    User: {\n      fields: {\n        // Champ calculé\n        fullName: {\n          read(_, { readField }) {\n            const firstName = readField<string>('firstName');\n            const lastName = readField<string>('lastName');\n            return `${firstName} ${lastName}`;\n          },\n        },\n\n        // Relation avec arguments\n        posts: {\n          keyArgs: ['status'],\n          merge(existing = [], incoming) {\n            return [...existing, ...incoming];\n          },\n        },\n      },\n    },\n\n    Post: {\n      fields: {\n        // Cache TTL\n        content: {\n          read(content) {\n            return content;\n          },\n        },\n      },\n    },\n  },\n\n  // Données possiblement non-normalisables\n  possibleTypes: {\n    Node: ['User', 'Post', 'Comment'],\n    Content: ['Post', 'Comment'],\n  },\n});\n\nconst client = new ApolloClient({\n  uri: '/graphql',\n  cache,\n  defaultOptions: {\n    watchQuery: {\n      fetchPolicy: 'cache-and-network',\n      nextFetchPolicy: 'cache-first',\n    },\n    query: {\n      fetchPolicy: 'cache-first',\n    },\n    mutate: {\n      fetchPolicy: 'no-cache',\n    },\n  },\n});\n```\n\n## Optimistic UI\n\n```typescript\nimport { useMutation, gql } from '@apollo/client';\n\nconst ADD_TODO = gql`\n  mutation AddTodo($title: String!) {\n    addTodo(title: $title) {\n      id\n      title\n      completed\n    }\n  }\n`;\n\nconst GET_TODOS = gql`\n  query GetTodos {\n    todos {\n      id\n      title\n      completed\n    }\n  }\n`;\n\nfunction TodoList() {\n  const [addTodo] = useMutation(ADD_TODO, {\n    optimisticResponse: (vars) => ({\n      addTodo: {\n        __typename: 'Todo',\n        id: 'temp-' + Date.now(),\n        title: vars.title,\n        completed: false,\n      },\n    }),\n    update: (cache, { data }) => {\n      const existing = cache.readQuery({ query: GET_TODOS });\n      cache.writeQuery({\n        query: GET_TODOS,\n        data: {\n          todos: [...existing.todos, data.addTodo],\n        },\n      });\n    },\n    // Rollback on error\n    onError: (error) => {\n      console.error('Failed to add todo:', error);\n    },\n  });\n\n  const handleAdd = (title: string) => {\n    addTodo({ variables: { title } });\n  };\n\n  // ...\n}\n```\n\n## Subscriptions avec Reconnection\n\n```typescript\nimport { split, HttpLink, WebSocketLink } from '@apollo/client';\nimport { getMainDefinition } from '@apollo/client/utilities';\nimport { GraphQLWsLink } from '@apollo/client/link/subscriptions';\nimport { createClient } from 'graphql-ws';\n\n// WebSocket link avec reconnection\nconst wsLink = new GraphQLWsLink(\n  createClient({\n    url: 'wss://api.example.com/graphql',\n    connectionParams: () => ({\n      authorization: getAuthToken(),\n    }),\n    retryAttempts: 5,\n    retryWait: async (retries) => {\n      await new Promise((resolve) => \n        setTimeout(resolve, Math.min(1000 * 2 ** retries, 30000))\n      );\n    },\n    on: {\n      connected: () => console.log('WebSocket connected'),\n      closed: () => console.log('WebSocket closed'),\n      error: (error) => console.error('WebSocket error:', error),\n    },\n  })\n);\n\n// HTTP link pour queries/mutations\nconst httpLink = new HttpLink({\n  uri: '/graphql',\n});\n\n// Split entre HTTP et WebSocket\nconst splitLink = split(\n  ({ query }) => {\n    const definition = getMainDefinition(query);\n    return (\n      definition.kind === 'OperationDefinition' &&\n      definition.operation === 'subscription'\n    );\n  },\n  wsLink,\n  httpLink\n);\n\nconst client = new ApolloClient({\n  link: splitLink,\n  cache: new InMemoryCache(),\n});\n```",
      "terminal_task": {
        "command": "npm run test:graphql",
        "expected_output": "✓ Cache normalization and optimistic updates work",
        "hint": "Configurez typePolicies pour la pagination, utilisez optimisticResponse pour l'UI instantanée, et GraphQLWsLink pour les subscriptions"
      }
    },
    {
      "id": "frontend-008-css-architecture",
      "track": "FRONTEND_ELITE",
      "level": "Expert",
      "title": "Architecture CSS à Grande Échelle",
      "duration": "5h30",
      "description": "CSS-in-JS performant, design tokens, et systèmes de composants.",
      "markdown_content": "# Architecture CSS à Grande Échelle\n\n## Architecture CSS-in-JS\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    CSS ARCHITECTURE                         │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  Design Tokens                                              │\n│  ├── colors: { primary: '#007bff', ... }                   │\n│  ├── spacing: { sm: '4px', md: '8px', ... }                │\n│  └── typography: { fontSize: { sm: '12px', ... } }         │\n│                                                             │\n│  Theme Provider                                             │\n│  └── CSS Variables ──▶ Components                          │\n│                                                             │\n│  Component Styles                                           │\n│  ├── Base styles (static)                                   │\n│  ├── Variant styles (dynamic)                               │\n│  └── State styles (hover, focus, disabled)                  │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Panda CSS (Zero-Runtime)\n\n```typescript\n// panda.config.ts\nimport { defineConfig } from '@pandacss/dev';\n\nexport default defineConfig({\n  preflight: true,\n  include: ['./src/**/*.{js,jsx,ts,tsx}'],\n  exclude: [],\n\n  theme: {\n    extend: {\n      tokens: {\n        colors: {\n          primary: { value: '#007bff' },\n          secondary: { value: '#6c757d' },\n          success: { value: '#28a745' },\n          danger: { value: '#dc3545' },\n        },\n        spacing: {\n          xs: { value: '4px' },\n          sm: { value: '8px' },\n          md: { value: '16px' },\n          lg: { value: '24px' },\n          xl: { value: '32px' },\n        },\n        fonts: {\n          sans: { value: 'system-ui, sans-serif' },\n          mono: { value: 'monospace' },\n        },\n      },\n\n      semanticTokens: {\n        colors: {\n          background: {\n            value: { base: '{colors.white}', _dark: '{colors.gray.900}' },\n          },\n          foreground: {\n            value: { base: '{colors.gray.900}', _dark: '{colors.white}' },\n          },\n        },\n      },\n\n      recipes: {\n        button: {\n          base: {\n            display: 'inline-flex',\n            alignItems: 'center',\n            justifyContent: 'center',\n            borderRadius: 'md',\n            fontWeight: 'semibold',\n            transition: 'all 0.2s',\n            cursor: 'pointer',\n            _disabled: {\n              opacity: 0.5,\n              cursor: 'not-allowed',\n            },\n          },\n          variants: {\n            size: {\n              sm: { px: '3', py: '1.5', fontSize: 'sm' },\n              md: { px: '4', py: '2', fontSize: 'md' },\n              lg: { px: '6', py: '3', fontSize: 'lg' },\n            },\n            variant: {\n              solid: {},\n              outline: {\n                border: '2px solid',\n                background: 'transparent',\n              },\n              ghost: {\n                background: 'transparent',\n                _hover: { background: 'gray.100' },\n              },\n            },\n            color: {\n              primary: {},\n              secondary: {},\n              danger: {},\n            },\n          },\n          compoundVariants: [\n            {\n              variant: 'solid',\n              color: 'primary',\n              css: {\n                background: 'primary',\n                color: 'white',\n                _hover: { background: 'primary.dark' },\n              },\n            },\n            {\n              variant: 'outline',\n              color: 'primary',\n              css: {\n                borderColor: 'primary',\n                color: 'primary',\n                _hover: { background: 'primary', color: 'white' },\n              },\n            },\n          ],\n          defaultVariants: {\n            size: 'md',\n            variant: 'solid',\n            color: 'primary',\n          },\n        },\n      },\n    },\n  },\n\n  outdir: 'styled-system',\n});\n```\n\n## Composants avec Panda\n\n```typescript\n// src/components/Button.tsx\nimport { styled } from '../styled-system/jsx';\nimport { button } from '../styled-system/recipes';\nimport type { RecipeVariantProps } from '../styled-system/types';\n\ntype ButtonVariantProps = RecipeVariantProps<typeof button>;\n\ninterface ButtonProps extends ButtonVariantProps {\n  children: React.ReactNode;\n  onClick?: () => void;\n  disabled?: boolean;\n  loading?: boolean;\n}\n\nconst StyledButton = styled('button', button);\n\nexport function Button({\n  children,\n  loading,\n  disabled,\n  ...props\n}: ButtonProps) {\n  return (\n    <StyledButton\n      disabled={disabled || loading}\n      {...props}\n    >\n      {loading && <Spinner size=\"sm\" mr=\"2\" />}\n      {children}\n    </StyledButton>\n  );\n}\n\n// Usage\n<Button size=\"lg\" variant=\"outline\" color=\"danger\">\n  Delete\n</Button>\n```\n\n## CSS Variables Dynamiques\n\n```typescript\n// Theme provider avec CSS variables\nimport { createContext, useContext, useEffect, useState } from 'react';\n\ninterface Theme {\n  colors: Record<string, string>;\n  spacing: Record<string, string>;\n}\n\nconst ThemeContext = createContext<Theme | null>(null);\n\nexport function ThemeProvider({\n  theme,\n  children,\n}: {\n  theme: Theme;\n  children: React.ReactNode;\n}) {\n  useEffect(() => {\n    const root = document.documentElement;\n\n    // Set CSS variables\n    Object.entries(theme.colors).forEach(([key, value]) => {\n      root.style.setProperty(`--color-${key}`, value);\n    });\n\n    Object.entries(theme.spacing).forEach(([key, value]) => {\n      root.style.setProperty(`--spacing-${key}`, value);\n    });\n  }, [theme]);\n\n  return (\n    <ThemeContext.Provider value={theme}>\n      {children}\n    </ThemeContext.Provider>\n  );\n}\n\n// Hook pour utiliser les tokens\nexport function useToken(path: string): string {\n  const theme = useContext(ThemeContext);\n  if (!theme) throw new Error('useToken must be used within ThemeProvider');\n\n  const [category, key] = path.split('.');\n  return theme[category as keyof Theme]?.[key] || '';\n}\n```",
      "terminal_task": {
        "command": "npm run test:css-architecture",
        "expected_output": "✓ CSS tokens and recipes work correctly",
        "hint": "Utilisez Panda CSS pour le zero-runtime, des recipes pour les variants de composants, et CSS variables pour le theming dynamique"
      }
    },
    {
      "id": "frontend-009-testing-strategy",
      "track": "FRONTEND_ELITE",
      "level": "Expert",
      "title": "Stratégie de Testing: Unit à E2E",
      "duration": "6h00",
      "description": "Testing Library, MSW, Playwright, et TDD avancé.",
      "markdown_content": "# Stratégie de Testing\n\n## Architecture de Testing\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    TESTING PYRAMID                          │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│                    ┌─────────┐                              │\n│                    │   E2E   │  ← Playwright               │\n│                    │  (few)  │     (critical paths)        │\n│                   ┌┴─────────┴┐                             │\n│                   │Integration│  ← React Testing Library   │\n│                   │  (some)   │     + MSW                   │\n│                  ┌┴───────────┴┐                            │\n│                  │    Unit     │  ← Vitest + RTL            │\n│                  │   (many)    │     (logic, hooks)          │\n│                  └─────────────┘                            │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Unit Testing avec Vitest\n\n```typescript\n// src/utils/calculateTotal.test.ts\nimport { describe, it, expect } from 'vitest';\nimport { calculateTotal } from './calculateTotal';\n\ndescribe('calculateTotal', () => {\n  it('calculates total with tax', () => {\n    const items = [\n      { price: 10, quantity: 2 },\n      { price: 20, quantity: 1 },\n    ];\n\n    const result = calculateTotal(items, 0.1);\n\n    expect(result).toBe({\n      subtotal: 40,\n      tax: 4,\n      total: 44,\n    });\n  });\n\n  it('handles empty cart', () => {\n    const result = calculateTotal([], 0.1);\n\n    expect(result).toBe({\n      subtotal: 0,\n      tax: 0,\n      total: 0,\n    });\n  });\n\n  it('rounds to 2 decimal places', () => {\n    const items = [{ price: 10.999, quantity: 1 }];\n\n    const result = calculateTotal(items, 0.1);\n\n    expect(result.total).toBe(12.1);\n  });\n});\n\n// src/hooks/useCounter.test.ts\nimport { describe, it, expect } from 'vitest';\nimport { renderHook, act } from '@testing-library/react';\nimport { useCounter } from './useCounter';\n\ndescribe('useCounter', () => {\n  it('increments counter', () => {\n    const { result } = renderHook(() => useCounter());\n\n    act(() => {\n      result.current.increment();\n    });\n\n    expect(result.current.count).toBe(1);\n  });\n\n  it('respects max value', () => {\n    const { result } = renderHook(() => useCounter({ max: 5 }));\n\n    act(() => {\n      for (let i = 0; i < 10; i++) {\n        result.current.increment();\n      }\n    });\n\n    expect(result.current.count).toBe(5);\n  });\n});\n```\n\n## Integration Testing avec MSW\n\n```typescript\n// src/mocks/handlers.ts\nimport { http, HttpResponse } from 'msw';\n\nexport const handlers = [\n  http.get('/api/users', () => {\n    return HttpResponse.json({\n      users: [\n        { id: '1', name: 'Alice' },\n        { id: '2', name: 'Bob' },\n      ],\n    });\n  }),\n\n  http.post('/api/users', async ({ request }) => {\n    const body = await request.json();\n\n    return HttpResponse.json(\n      { id: '3', ...body },\n      { status: 201 }\n    );\n  }),\n\n  http.get('/api/users/:id', ({ params }) => {\n    const { id } = params;\n\n    if (id === '999') {\n      return HttpResponse.json(\n        { error: 'User not found' },\n        { status: 404 }\n      );\n    }\n\n    return HttpResponse.json({ id, name: 'Alice' });\n  }),\n];\n\n// src/mocks/server.ts\nimport { setupServer } from 'msw/node';\nimport { handlers } from './handlers';\n\nexport const server = setupServer(...handlers);\n\n// vitest.setup.ts\nimport { beforeAll, afterAll, afterEach } from 'vitest';\nimport { server } from './src/mocks/server';\n\nbeforeAll(() => server.listen({ onUnhandledRequest: 'error' }));\nafterEach(() => server.resetHandlers());\nafterAll(() => server.close());\n\n// Component test\nimport { describe, it, expect } from 'vitest';\nimport { render, screen, waitFor } from '@testing-library/react';\nimport userEvent from '@testing-library/user-event';\nimport { UserList } from './UserList';\n\ndescribe('UserList', () => {\n  it('displays users from API', async () => {\n    render(<UserList />);\n\n    await waitFor(() => {\n      expect(screen.getByText('Alice')).toBeInTheDocument();\n      expect(screen.getByText('Bob')).toBeInTheDocument();\n    });\n  });\n\n  it('adds new user', async () => {\n    const user = userEvent.setup();\n    render(<UserList />);\n\n    const input = screen.getByPlaceholderText('Enter name');\n    const button = screen.getByText('Add User');\n\n    await user.type(input, 'Charlie');\n    await user.click(button);\n\n    await waitFor(() => {\n      expect(screen.getByText('Charlie')).toBeInTheDocument();\n    });\n  });\n});\n```\n\n## E2E Testing avec Playwright\n\n```typescript\n// e2e/critical-paths.spec.ts\nimport { test, expect } from '@playwright/test';\n\ntest.describe('Critical Paths', () => {\n  test('user can complete purchase', async ({ page }) => {\n    // Login\n    await page.goto('/login');\n    await page.fill('[data-testid=\"email\"]', 'user@example.com');\n    await page.fill('[data-testid=\"password\"]', 'password');\n    await page.click('[data-testid=\"login-button\"]');\n\n    // Navigate to products\n    await page.waitForURL('/products');\n\n    // Add to cart\n    await page.click('[data-testid=\"product-1\"]');\n    await page.click('[data-testid=\"add-to-cart\"]');\n\n    // Go to cart\n    await page.click('[data-testid=\"cart-link\"]');\n    await page.waitForURL('/cart');\n\n    // Checkout\n    await page.click('[data-testid=\"checkout-button\"]');\n    await page.fill('[data-testid=\"card-number\"]', '4242424242424242');\n    await page.fill('[data-testid=\"expiry\"]', '12/25');\n    await page.fill('[data-testid=\"cvc\"]', '123');\n    await page.click('[data-testid=\"pay-button\"]');\n\n    // Verify success\n    await page.waitForSelector('[data-testid=\"success-message\"]');\n    expect(await page.textContent('[data-testid=\"success-message\"]'))\n      .toContain('Thank you for your purchase');\n  });\n\n  test('handles network errors gracefully', async ({ page }) => {\n    // Simulate network error\n    await page.route('/api/products', (route) => {\n      route.abort('failed');\n    });\n\n    await page.goto('/products');\n\n    // Should show error message\n    await page.waitForSelector('[data-testid=\"error-message\"]');\n    expect(await page.textContent('[data-testid=\"error-message\"]'))\n      .toContain('Failed to load products');\n  });\n});\n\n// playwright.config.ts\nimport { defineConfig, devices } from '@playwright/test';\n\nexport default defineConfig({\n  testDir: './e2e',\n  fullyParallel: true,\n  forbidOnly: !!process.env.CI,\n  retries: process.env.CI ? 2 : 0,\n  workers: process.env.CI ? 1 : undefined,\n  reporter: 'html',\n\n  use: {\n    baseURL: 'http://localhost:5173',\n    trace: 'on-first-retry',\n    screenshot: 'only-on-failure',\n  },\n\n  projects: [\n    {\n      name: 'chromium',\n      use: { ...devices['Desktop Chrome'] },\n    },\n    {\n      name: 'firefox',\n      use: { ...devices['Desktop Firefox'] },\n    },\n    {\n      name: 'webkit',\n      use: { ...devices['Desktop Safari'] },\n    },\n    {\n      name: 'Mobile Chrome',\n      use: { ...devices['Pixel 5'] },\n    },\n    {\n      name: 'Mobile Safari',\n      use: { ...devices['iPhone 12'] },\n    },\n  ],\n\n  webServer: {\n    command: 'npm run dev',\n    url: 'http://localhost:5173',\n    reuseExistingServer: !process.env.CI,\n  },\n});\n```",
      "terminal_task": {
        "command": "npm run test:all",
        "expected_output": "✓ All tests passed (142 tests)",
        "hint": "Utilisez Vitest pour les tests unitaires, MSW pour le mocking API, et Playwright pour les tests E2E cross-browser"
      }
    },
    {
      "id": "frontend-010-edge-computing",
      "track": "FRONTEND_ELITE",
      "level": "Expert",
      "title": "Edge Computing: Vercel Edge et Cloudflare Workers",
      "duration": "7h00",
      "description": "Functions edge, streaming, et patterns de cache distribué.",
      "markdown_content": "# Edge Computing\n\n## Architecture Edge\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    EDGE ARCHITECTURE                        │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  User Request                                               │\n│       │                                                     │\n│       ▼                                                     │\n│  ┌─────────────┐    ┌─────────────┐    ┌─────────────────┐ │\n│  │   CDN       │───▶│  Edge       │───▶│  Origin         │ │\n│  │  (static)   │    │  Function   │    │  (if needed)    │ │\n│  └─────────────┘    └──────┬──────┘    └─────────────────┘ │\n│                            │                                │\n│                            ▼                                │\n│                   ┌─────────────────┐                       │\n│                   │  Cache Edge     │                       │\n│                   │  (KV/Redis)     │                       │\n│                   └─────────────────┘                       │\n│                                                             │\n│  Latency: 10-50ms (edge) vs 100-500ms (origin)             │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Vercel Edge Function\n\n```typescript\n// app/api/realtime/route.ts\nexport const runtime = 'edge';\n\nimport { NextRequest, NextResponse } from 'next/server';\n\nexport async function GET(request: NextRequest) {\n  const { searchParams } = new URL(request.url);\n  const city = searchParams.get('city') || 'Paris';\n\n  // Edge KV pour le cache\n  const cached = await fetch(\n    `${process.env.EDGE_KV_URL}/weather/${city}`,\n    {\n      headers: { Authorization: `Bearer ${process.env.EDGE_KV_TOKEN}` },\n    }\n  );\n\n  if (cached.ok) {\n    const data = await cached.json();\n    return NextResponse.json(data, {\n      headers: { 'X-Cache': 'HIT' },\n    });\n  }\n\n  // Fetch from API\n  const response = await fetch(\n    `https://api.weather.com/v1/current?city=${city}`,\n    {\n      headers: { Authorization: `Bearer ${process.env.WEATHER_API_KEY}` },\n    }\n  );\n\n  const data = await response.json();\n\n  // Cache for 5 minutes\n  await fetch(`${process.env.EDGE_KV_URL}/weather/${city}`, {\n    method: 'PUT',\n    headers: {\n      Authorization: `Bearer ${process.env.EDGE_KV_TOKEN}`,\n      'Content-Type': 'application/json',\n    },\n    body: JSON.stringify(data),\n  });\n\n  return NextResponse.json(data, {\n    headers: { 'X-Cache': 'MISS' },\n  });\n}\n\n// Streaming response\nexport async function POST(request: NextRequest) {\n  const { prompt } = await request.json();\n\n  const stream = await fetch('https://api.openai.com/v1/chat/completions', {\n    method: 'POST',\n    headers: {\n      Authorization: `Bearer ${process.env.OPENAI_API_KEY}`,\n      'Content-Type': 'application/json',\n    },\n    body: JSON.stringify({\n      model: 'gpt-4',\n      messages: [{ role: 'user', content: prompt }],\n      stream: true,\n    }),\n  });\n\n  return new Response(stream.body, {\n    headers: {\n      'Content-Type': 'text/event-stream',\n      'Cache-Control': 'no-cache',\n      Connection: 'keep-alive',\n    },\n  });\n}\n```\n\n## Cloudflare Worker\n\n```typescript\n// src/index.ts\nexport interface Env {\n  WEATHER_CACHE: KVNamespace;\n  RATE_LIMIT: DurableObjectNamespace;\n}\n\nexport default {\n  async fetch(request: Request, env: Env, ctx: ExecutionContext): Promise<Response> {\n    const url = new URL(request.url);\n\n    // Rate limiting\n    const clientId = request.headers.get('CF-Connecting-IP') || 'unknown';\n    const rateLimitKey = `rate_limit:${clientId}`;\n\n    const current = parseInt(await env.WEATHER_CACHE.get(rateLimitKey) || '0');\n    if (current > 100) {\n      return new Response('Rate limit exceeded', { status: 429 });\n    }\n\n    await env.WEATHER_CACHE.put(rateLimitKey, String(current + 1), {\n      expirationTtl: 60,\n    });\n\n    // Cache check\n    const city = url.searchParams.get('city');\n    if (city) {\n      const cached = await env.WEATHER_CACHE.get(`weather:${city}`);\n      if (cached) {\n        return new Response(cached, {\n          headers: {\n            'Content-Type': 'application/json',\n            'X-Cache': 'HIT',\n          },\n        });\n      }\n\n      // Fetch and cache\n      const response = await fetch(`https://api.weather.com/current?city=${city}`);\n      const data = await response.text();\n\n      ctx.waitUntil(\n        env.WEATHER_CACHE.put(`weather:${city}`, data, {\n          expirationTtl: 300,\n        })\n      );\n\n      return new Response(data, {\n        headers: {\n          'Content-Type': 'application/json',\n          'X-Cache': 'MISS',\n        },\n      });\n    }\n\n    return new Response('Missing city parameter', { status: 400 });\n  },\n};\n```\n\n## Edge Middleware\n\n```typescript\n// middleware.ts\nimport { NextResponse } from 'next/server';\nimport type { NextRequest } from 'next/server';\n\nexport function middleware(request: NextRequest) {\n  // Geo-based routing\n  const country = request.geo?.country || 'US';\n\n  // A/B testing\n  const experiment = request.cookies.get('experiment')?.value;\n  if (!experiment) {\n    const variant = Math.random() > 0.5 ? 'A' : 'B';\n    const response = NextResponse.next();\n    response.cookies.set('experiment', variant);\n    return response;\n  }\n\n  // Authentication check\n  const token = request.headers.get('authorization');\n  if (request.nextUrl.pathname.startsWith('/api/protected')) {\n    if (!token) {\n      return NextResponse.json({ error: 'Unauthorized' }, { status: 401 });\n    }\n\n    // Verify JWT at edge\n    try {\n      const payload = verifyJWT(token);\n      request.headers.set('x-user-id', payload.sub);\n    } catch {\n      return NextResponse.json({ error: 'Invalid token' }, { status: 401 });\n    }\n  }\n\n  return NextResponse.next();\n}\n\nexport const config = {\n  matcher: ['/api/:path*', '/protected/:path*'],\n};\n```",
      "terminal_task": {
        "command": "npm run test:edge",
        "expected_output": "✓ Edge functions respond in <50ms",
        "hint": "Utilisez KV pour le cache edge, streaming pour les réponses temps réel, et geo/headers pour la personnalisation"
      }
    },
    {
      "id": "ia-001-ollama-local",
      "track": "IA_AGENTIC",
      "level": "Expert",
      "title": "Ollama: LLMs Locaux et Optimisation",
      "duration": "8h00",
      "description": "Déploiement de LLMs locaux, quantification GGUF, et optimisation VRAM.",
      "markdown_content": "# Ollama: LLMs Locaux\n\n## Architecture Ollama\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    OLLAMA ARCHITECTURE                      │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  ┌─────────────┐    ┌─────────────┐    ┌─────────────────┐ │\n│  │   Client    │───▶│   Ollama    │───▶│   llama.cpp     │ │\n│  │  (HTTP)     │    │   Server    │    │   (inference)   │ │\n│  └─────────────┘    └──────┬──────┘    └─────────────────┘ │\n│                            │                                │\n│                            ▼                                │\n│                   ┌─────────────────┐                       │\n│                   │  GGUF Models    │                       │\n│                   │  (quantized)    │                       │\n│                   └─────────────────┘                       │\n│                                                             │\n│  Quantization: Q4_0 (4-bit) → 50% size, 95% quality        │\n│                Q5_K_M (5-bit) → 62% size, 98% quality        │\n│                Q8_0 (8-bit) → 100% size, 99.9% quality       │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Client Ollama en Rust\n\n```rust\nuse reqwest::Client;\nuse serde::{Deserialize, Serialize};\nuse futures::StreamExt;\n\npub struct OllamaClient {\n    base_url: String,\n    client: Client,\n}\n\n#[derive(Debug, Serialize)]\npub struct GenerateRequest {\n    pub model: String,\n    pub prompt: String,\n    pub stream: bool,\n    pub options: Option<GenerationOptions>,\n}\n\n#[derive(Debug, Serialize, Default)]\npub struct GenerationOptions {\n    pub temperature: Option<f32>,\n    pub top_p: Option<f32>,\n    pub top_k: Option<i32>,\n    pub num_predict: Option<i32>,\n    pub stop: Option<Vec<String>>,\n}\n\n#[derive(Debug, Deserialize)]\npub struct GenerateResponse {\n    pub response: String,\n    pub done: bool,\n    pub context: Option<Vec<i32>>,\n    pub total_duration: Option<u64>,\n    pub load_duration: Option<u64>,\n}\n\nimpl OllamaClient {\n    pub fn new(base_url: &str) -> Self {\n        Self {\n            base_url: base_url.to_string(),\n            client: Client::new(),\n        }\n    }\n\n    pub async fn generate(\n        &self,\n        model: &str,\n        prompt: &str,\n        options: Option<GenerationOptions>,\n    ) -> Result<GenerateResponse, reqwest::Error> {\n        let request = GenerateRequest {\n            model: model.to_string(),\n            prompt: prompt.to_string(),\n            stream: false,\n            options,\n        };\n\n        let response = self.client\n            .post(format!(\"{}/api/generate\", self.base_url))\n            .json(&request)\n            .send()\n            .await?;\n\n        response.json().await\n    }\n\n    pub async fn generate_stream(\n        &self,\n        model: &str,\n        prompt: &str,\n        options: Option<GenerationOptions>,\n    ) -> Result<impl StreamExt<Item = Result<GenerateResponse, reqwest::Error>>, reqwest::Error> {\n        let request = GenerateRequest {\n            model: model.to_string(),\n            prompt: prompt.to_string(),\n            stream: true,\n            options,\n        };\n\n        let response = self.client\n            .post(format!(\"{}/api/generate\", self.base_url))\n            .json(&request)\n            .send()\n            .await?;\n\n        Ok(response.bytes_stream().map(|chunk| {\n            // Parse streaming JSON\n            match chunk {\n                Ok(bytes) => {\n                    let text = String::from_utf8_lossy(&bytes);\n                    serde_json::from_str(&text)\n                        .map_err(|e| reqwest::Error::from(e))\n                }\n                Err(e) => Err(e),\n            }\n        }))\n    }\n\n    pub async fn list_models(&self) -> Result<Vec<ModelInfo>, reqwest::Error> {\n        let response = self.client\n            .get(format!(\"{}/api/tags\", self.base_url))\n            .send()\n            .await?;\n\n        #[derive(Deserialize)]\n        struct ModelsResponse {\n            models: Vec<ModelInfo>,\n        }\n\n        let data: ModelsResponse = response.json().await?;\n        Ok(data.models)\n    }\n\n    pub async fn pull_model(&self, model: &str) -> Result<(), reqwest::Error> {\n        let request = serde_json::json!({\n            \"name\": model,\n        });\n\n        let _ = self.client\n            .post(format!(\"{}/api/pull\", self.base_url))\n            .json(&request)\n            .send()\n            .await?;\n\n        Ok(())\n    }\n}\n\n#[derive(Debug, Deserialize)]\npub struct ModelInfo {\n    pub name: String,\n    pub size: u64,\n    pub digest: String,\n    pub details: ModelDetails,\n}\n\n#[derive(Debug, Deserialize)]\npub struct ModelDetails {\n    pub format: String,\n    pub family: String,\n    pub parameter_size: String,\n    pub quantization_level: String,\n}\n```\n\n## Gestion VRAM\n\n```rust\npub struct VRAMManager {\n    total_vram: usize,\n    used_vram: Arc<Mutex<usize>>,\n    models: Arc<Mutex<HashMap<String, ModelVRAM>>>,\n}\n\n#[derive(Clone)]\nstruct ModelVRAM {\n    name: String,\n    vram_usage: usize,\n    loaded_at: Instant,\n    last_used: Instant,\n}\n\nimpl VRAMManager {\n    pub fn new(total_vram_gb: usize) -> Self {\n        Self {\n            total_vram: total_vram_gb * 1024 * 1024 * 1024,\n            used_vram: Arc::new(Mutex::new(0)),\n            models: Arc::new(Mutex::new(HashMap::new())),\n        }\n    }\n\n    pub async fn can_load(&self, model_size: usize) -> bool {\n        let used = *self.used_vram.lock().await;\n        used + model_size <= self.total_vram\n    }\n\n    pub async fn load_model(&self, name: &str, size: usize) -> Result<(), String> {\n        if !self.can_load(size).await {\n            // Try to unload least recently used\n            self.unload_lru(size).await?;\n        }\n\n        let mut models = self.models.lock().await;\n        let mut used = self.used_vram.lock().await;\n\n        models.insert(name.to_string(), ModelVRAM {\n            name: name.to_string(),\n            vram_usage: size,\n            loaded_at: Instant::now(),\n            last_used: Instant::now(),\n        });\n\n        *used += size;\n\n        Ok(())\n    }\n\n    async fn unload_lru(&self, needed: usize) -> Result<(), String> {\n        let mut models = self.models.lock().await;\n        let mut used = self.used_vram.lock().await;\n\n        let mut model_list: Vec<_> = models.values().cloned().collect();\n        model_list.sort_by_key(|m| m.last_used);\n\n        let mut freed = 0;\n        for model in model_list {\n            if freed >= needed {\n                break;\n            }\n\n            // Unload via Ollama API\n            // ...\n\n            freed += model.vram_usage;\n            *used -= model.vram_usage;\n            models.remove(&model.name);\n        }\n\n        if freed < needed {\n            return Err(\"Not enough VRAM even after unloading\".to_string());\n        }\n\n        Ok(())\n    }\n\n    pub async fn get_stats(&self) -> VRAMStats {\n        let used = *self.used_vram.lock().await;\n        let models = self.models.lock().await;\n\n        VRAMStats {\n            total: self.total_vram,\n            used,\n            free: self.total_vram - used,\n            loaded_models: models.len(),\n        }\n    }\n}\n\npub struct VRAMStats {\n    pub total: usize,\n    pub used: usize,\n    pub free: usize,\n    pub loaded_models: usize,\n}\n```",
      "terminal_task": {
        "command": "cargo test ollama -- --nocapture",
        "expected_output": "test ollama::tests::test_generate ... ok",
        "hint": "Utilisez reqwest pour l'API HTTP, futures::StreamExt pour le streaming, et Arc<Mutex> pour la gestion VRAM thread-safe"
      }
    },
    {
      "id": "ia-002-rag-implementation",
      "track": "IA_AGENTIC",
      "level": "Expert",
      "title": "RAG: Retrieval-Augmented Generation",
      "duration": "10h00",
      "description": "Implémentation complète de RAG avec vector DB, embeddings, et chunking.",
      "markdown_content": "# RAG: Retrieval-Augmented Generation\n\n## Architecture RAG\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    RAG PIPELINE                             │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  INGESTION                                                  │\n│  Documents ──▶ Chunking ──▶ Embeddings ──▶ Vector DB        │\n│                                                             │\n│  QUERY                                                      │\n│  Question ──▶ Embedding ──▶ Similarity Search ──▶ Context   │\n│                                                             │\n│  GENERATION                                                 │\n│  Context + Question ──▶ LLM ──▶ Answer                      │\n│                                                             │\n│  ┌─────────────┐    ┌─────────────┐    ┌─────────────────┐ │\n│  │  Embedding  │    │  Vector DB  │    │  LLM + Context  │ │\n│  │  (384-4096d)│    │  (cosine)   │    │  (prompt eng)   │ │\n│  └─────────────┘    └─────────────┘    └─────────────────┘ │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Chunking Stratégique\n\n```rust\npub struct Chunker {\n    chunk_size: usize,\n    overlap: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct Chunk {\n    pub content: String,\n    pub metadata: ChunkMetadata,\n}\n\n#[derive(Debug, Clone)]\npub struct ChunkMetadata {\n    pub source: String,\n    pub start_line: usize,\n    pub end_line: usize,\n    pub index: usize,\n}\n\nimpl Chunker {\n    pub fn new(chunk_size: usize, overlap: usize) -> Self {\n        Self {\n            chunk_size,\n            overlap,\n        }\n    }\n\n    pub fn chunk_text(&self, text: &str, source: &str) -> Vec<Chunk> {\n        let sentences: Vec<&str> = text\n            .split(|c: char| c == '.' || c == '!' || c == '?')\n            .filter(|s| !s.trim().is_empty())\n            .collect();\n\n        let mut chunks = vec![];\n        let mut current_chunk = String::new();\n        let mut start_line = 0;\n        let mut line_count = 0;\n\n        for (i, sentence) in sentences.iter().enumerate() {\n            let sentence = sentence.trim();\n\n            if current_chunk.len() + sentence.len() > self.chunk_size \n                && !current_chunk.is_empty() {\n                // Save current chunk\n                chunks.push(Chunk {\n                    content: current_chunk.trim().to_string(),\n                    metadata: ChunkMetadata {\n                        source: source.to_string(),\n                        start_line,\n                        end_line: line_count,\n                        index: chunks.len(),\n                    },\n                });\n\n                // Start new chunk with overlap\n                let overlap_text = self.get_overlap(&chunks.last().unwrap().content);\n                current_chunk = overlap_text + sentence + \". \";\n                start_line = line_count;\n            } else {\n                current_chunk.push_str(sentence);\n                current_chunk.push_str(\". \");\n            }\n\n            line_count += sentence.matches('\n').count() + 1;\n        }\n\n        // Don't forget the last chunk\n        if !current_chunk.is_empty() {\n            chunks.push(Chunk {\n                content: current_chunk.trim().to_string(),\n                metadata: ChunkMetadata {\n                    source: source.to_string(),\n                    start_line,\n                    end_line: line_count,\n                    index: chunks.len(),\n                },\n            });\n        }\n\n        chunks\n    }\n\n    fn get_overlap(&self, text: &str) -> String {\n        let words: Vec<&str> = text.split_whitespace().collect();\n        let overlap_words = words.iter().rev().take(self.overlap / 5).rev();\n        overlap_words.collect::<Vec<_>>().join(\" \") + \" \"\n    }\n\n    pub fn chunk_code(&self, code: &str, source: &str) -> Vec<Chunk> {\n        // Code-aware chunking\n        let lines: Vec<&str> = code.lines().collect();\n        let mut chunks = vec![];\n        let mut current_chunk = vec![];\n        let mut bracket_depth = 0;\n        let mut start_line = 0;\n\n        for (i, line) in lines.iter().enumerate() {\n            let trimmed = line.trim();\n\n            // Track block depth\n            bracket_depth += line.matches('{').count() as i32;\n            bracket_depth -= line.matches('}').count() as i32;\n\n            current_chunk.push(*line);\n\n            // Split on function boundaries or size limit\n            let should_split = (trimmed.starts_with(\"fn \") \n                || trimmed.starts_with(\"pub fn \")\n                || trimmed.starts_with(\"function \"))\n                && bracket_depth == 0\n                && !current_chunk.is_empty();\n\n            let chunk_text = current_chunk.join(\"\\n\");\n            if should_split || chunk_text.len() > self.chunk_size {\n                chunks.push(Chunk {\n                    content: chunk_text,\n                    metadata: ChunkMetadata {\n                        source: source.to_string(),\n                        start_line,\n                        end_line: i,\n                        index: chunks.len(),\n                    },\n                });\n                current_chunk.clear();\n                start_line = i + 1;\n            }\n        }\n\n        // Last chunk\n        if !current_chunk.is_empty() {\n            chunks.push(Chunk {\n                content: current_chunk.join(\"\\n\"),\n                metadata: ChunkMetadata {\n                    source: source.to_string(),\n                    start_line,\n                    end_line: lines.len(),\n                    index: chunks.len(),\n                },\n            });\n        }\n\n        chunks\n    }\n}\n```\n\n## Vector Database (Qdrant)\n\n```rust\nuse qdrant_client::prelude::*;\nuse qdrant_client::qdrant::{\n    vectors_config::Config, CreateCollection, Distance, VectorParams, VectorsConfig,\n};\n\npub struct VectorStore {\n    client: Qdrant,\n    collection_name: String,\n}\n\nimpl VectorStore {\n    pub async fn new(url: &str, collection_name: &str) -> Result<Self, QdrantError> {\n        let client = Qdrant::from_url(url).build()?;\n\n        // Create collection if not exists\n        let collections = client.list_collections().await?;\n        let exists = collections.collections.iter()\n            .any(|c| c.name == collection_name);\n\n        if !exists {\n            client\n                .create_collection(&CreateCollection {\n                    collection_name: collection_name.to_string(),\n                    vectors_config: Some(VectorsConfig {\n                        config: Some(Config::Params(VectorParams {\n                            size: 384, // nomic-embed-text dimension\n                            distance: Distance::Cosine.into(),\n                            ..Default::default()\n                        })),\n                    }),\n                    ..Default::default()\n                })\n                .await?;\n        }\n\n        Ok(Self {\n            client,\n            collection_name: collection_name.to_string(),\n        })\n    }\n\n    pub async fn upsert(\n        &self,\n        id: &str,\n        vector: Vec<f32>,\n        payload: serde_json::Value,\n    ) -> Result<(), QdrantError> {\n        let points = vec![PointStruct::new(\n            id.to_string(),\n            vector,\n            payload,\n        )];\n\n        self.client\n            .upsert_points_blocking(&self.collection_name, None, points, None)\n            .await?;\n\n        Ok(())\n    }\n\n    pub async fn search(\n        &self,\n        vector: Vec<f32>,\n        limit: u64,\n    ) -> Result<Vec<ScoredPoint>, QdrantError> {\n        let results = self.client\n            .search_points(&SearchPoints {\n                collection_name: self.collection_name.clone(),\n                vector,\n                limit,\n                with_payload: Some(true.into()),\n                ..Default::default()\n            })\n            .await?;\n\n        Ok(results.result)\n    }\n}\n```\n\n## Pipeline RAG Complète\n\n```rust\npub struct RAGPipeline {\n    embedder: Embedder,\n    vector_store: VectorStore,\n    llm: OllamaClient,\n    chunker: Chunker,\n}\n\nimpl RAGPipeline {\n    pub async fn ingest_document(&self, content: &str, source: &str) -> Result<(), String> {\n        // 1. Chunk\n        let chunks = self.chunker.chunk_text(content, source);\n\n        // 2. Embed and store\n        for chunk in chunks {\n            let embedding = self.embedder.embed(&chunk.content).await?;\n\n            let payload = serde_json::json!({\n                \"content\": chunk.content,\n                \"source\": chunk.metadata.source,\n                \"start_line\": chunk.metadata.start_line,\n                \"end_line\": chunk.metadata.end_line,\n            });\n\n            let id = format!(\"{}-{}\", source, chunk.metadata.index);\n            self.vector_store.upsert(&id, embedding, payload).await\n                .map_err(|e| e.to_string())?;\n        }\n\n        Ok(())\n    }\n\n    pub async fn query(&self, question: &str) -> Result<String, String> {\n        // 1. Embed question\n        let query_embedding = self.embedder.embed(question).await?;\n\n        // 2. Retrieve relevant chunks\n        let results = self.vector_store.search(query_embedding, 5).await\n            .map_err(|e| e.to_string())?;\n\n        // 3. Build context\n        let context = results.iter()\n            .map(|r| {\n                let payload = r.payload.as_ref()?;\n                let content = payload.get(\"content\")?.as_str()?;\n                Some(content.to_string())\n            })\n            .filter_map(|x| x)\n            .collect::<Vec<_>>()\n            .join(\"\\n\\n\");\n\n        // 4. Generate answer\n        let prompt = format!(\n            \"Context:\\n{}\\n\\nQuestion: {}\\n\\nAnswer based on the context provided:\",\n            context, question\n        );\n\n        let response = self.llm.generate(\"llama3.2\", &prompt, None).await\n            .map_err(|e| e.to_string())?;\n\n        Ok(response.response)\n    }\n}\n```",
      "terminal_task": {
        "command": "cargo test rag -- --nocapture",
        "expected_output": "test rag::tests::test_ingest_and_query ... ok",
        "hint": "Utilisez qdrant-client pour la vector DB, un chunker avec overlap pour la cohérence, et cosine similarity pour la recherche"
      }
    },
    {
      "id": "ia-003-agent-swarm",
      "track": "IA_AGENTIC",
      "level": "Expert",
      "title": "Agent Swarm: Orchestration Multi-Agents",
      "duration": "9h00",
      "description": "Système multi-agents, coordination, et résolution de problèmes complexes.",
      "markdown_content": "# Agent Swarm: Orchestration Multi-Agents\n\n## Architecture Swarm\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    AGENT SWARM                              │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│                    ┌─────────────┐                          │\n│                    │  Orchestrator│                         │\n│                    │  (dispatcher)│                          │\n│                    └──────┬──────┘                          │\n│                           │                                 │\n│        ┌──────────────────┼──────────────────┐             │\n│        │                  │                  │              │\n│        ▼                  ▼                  ▼              │\n│  ┌──────────┐      ┌──────────┐      ┌──────────┐         │\n│  │  Agent   │      │  Agent   │      │  Agent   │         │\n│  │  Code    │      │  Review  │      │  Test    │         │\n│  │          │      │          │      │          │         │\n│  └────┬─────┘      └────┬─────┘      └────┬─────┘         │\n│       │                 │                 │                │\n│       └─────────────────┼─────────────────┘                │\n│                         │                                   │\n│                         ▼                                   │\n│                  ┌─────────────┐                           │\n│                  │  Shared     │                           │\n│                  │  Memory     │                           │\n│                  └─────────────┘                           │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Agent Base et Spécialisés\n\n```rust\nuse async_trait::async_trait;\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\n\n#[async_trait]\npub trait Agent: Send + Sync {\n    fn name(&self) -> &str;\n    fn capabilities(&self) -> Vec<Capability>;\n\n    async fn execute(\n        &self,\n        task: Task,\n        context: &SharedContext,\n    ) -> Result<TaskResult, AgentError>;\n}\n\n#[derive(Debug, Clone)]\npub enum Capability {\n    CodeGeneration,\n    CodeReview,\n    Testing,\n    Documentation,\n    Architecture,\n    Debugging,\n}\n\n#[derive(Debug, Clone)]\npub struct Task {\n    pub id: String,\n    pub description: String,\n    pub task_type: TaskType,\n    pub priority: Priority,\n    pub dependencies: Vec<String>,\n}\n\n#[derive(Debug, Clone)]\npub enum TaskType {\n    GenerateCode { language: String, requirements: String },\n    ReviewCode { code: String, criteria: Vec<String> },\n    WriteTests { code: String, coverage_target: f32 },\n    FixBug { code: String, error: String },\n    Refactor { code: String, goals: Vec<String> },\n}\n\n#[derive(Debug, Clone)]\npub enum Priority {\n    Low,\n    Medium,\n    High,\n    Critical,\n}\n\n#[derive(Debug, Clone)]\npub struct TaskResult {\n    pub task_id: String,\n    pub output: String,\n    pub artifacts: Vec<Artifact>,\n    pub confidence: f32,\n}\n\n#[derive(Debug, Clone)]\npub struct Artifact {\n    pub name: String,\n    pub content: String,\n    pub artifact_type: ArtifactType,\n}\n\n#[derive(Debug, Clone)]\npub enum ArtifactType {\n    Code,\n    Documentation,\n    Test,\n    Configuration,\n}\n\n#[derive(Debug)]\npub struct AgentError {\n    pub message: String,\n    pub retryable: bool,\n}\n\n// Shared Context\npub struct SharedContext {\n    memory: Arc<RwLock<HashMap<String, String>>>,\n    message_bus: Arc<MessageBus>,\n}\n\nimpl SharedContext {\n    pub fn store(&self, key: &str, value: &str) {\n        let mut memory = self.memory.blocking_write();\n        memory.insert(key.to_string(), value.to_string());\n    }\n\n    pub fn retrieve(&self, key: &str) -> Option<String> {\n        let memory = self.memory.blocking_read();\n        memory.get(key).cloned()\n    }\n\n    pub async fn broadcast(&self, message: Message) {\n        self.message_bus.send(message).await;\n    }\n}\n\n// Code Agent\npub struct CodeAgent {\n    name: String,\n    llm: Arc<dyn LLM>,\n}\n\n#[async_trait]\nimpl Agent for CodeAgent {\n    fn name(&self) -> &str {\n        &self.name\n    }\n\n    fn capabilities(&self) -> Vec<Capability> {\n        vec![\n            Capability::CodeGeneration,\n            Capability::Refactoring,\n            Capability::Debugging,\n        ]\n    }\n\n    async fn execute(\n        &self,\n        task: Task,\n        context: &SharedContext,\n    ) -> Result<TaskResult, AgentError> {\n        match task.task_type {\n            TaskType::GenerateCode { language, requirements } => {\n                let prompt = format!(\n                    \"Generate {} code for: {}\\n\\nRequirements:\\n{}\",\n                    language, task.description, requirements\n                );\n\n                let code = self.llm.generate(&prompt).await\n                    .map_err(|e| AgentError {\n                        message: e.to_string(),\n                        retryable: true,\n                    })?;\n\n                Ok(TaskResult {\n                    task_id: task.id,\n                    output: code.clone(),\n                    artifacts: vec![Artifact {\n                        name: format!(\"generated.{}\", language),\n                        content: code,\n                        artifact_type: ArtifactType::Code,\n                    }],\n                    confidence: 0.85,\n                })\n            }\n            _ => Err(AgentError {\n                message: \"Unsupported task type\".to_string(),\n                retryable: false,\n            }),\n        }\n    }\n}\n```\n\n## Orchestrateur\n\n```rust\npub struct SwarmOrchestrator {\n    agents: Vec<Box<dyn Agent>>,\n    task_queue: Arc<Mutex<Vec<Task>>>,\n    results: Arc<Mutex<HashMap<String, TaskResult>>>,\n    max_concurrent: usize,\n}\n\nimpl SwarmOrchestrator {\n    pub fn new(max_concurrent: usize) -> Self {\n        Self {\n            agents: vec![],\n            task_queue: Arc::new(Mutex::new(vec![])),\n            results: Arc::new(Mutex::new(HashMap::new())),\n            max_concurrent,\n        }\n    }\n\n    pub fn register_agent(&mut self, agent: Box<dyn Agent>) {\n        self.agents.push(agent);\n    }\n\n    pub async fn submit_task(&self, task: Task) {\n        let mut queue = self.task_queue.lock().await;\n        queue.push(task);\n    }\n\n    pub async fn run(&self) {\n        let semaphore = Arc::new(Semaphore::new(self.max_concurrent));\n        let context = Arc::new(SharedContext::new());\n\n        loop {\n            let task = {\n                let mut queue = self.task_queue.lock().await;\n                queue.pop()\n            };\n\n            if let Some(task) = task {\n                let permit = semaphore.clone().acquire_owned().await.unwrap();\n                let agents = self.agents.clone();\n                let results = self.results.clone();\n                let ctx = context.clone();\n\n                tokio::spawn(async move {\n                    let _permit = permit;\n\n                    // Find best agent\n                    let agent = Self::select_agent(&agents, &task);\n\n                    if let Some(agent) = agent {\n                        match agent.execute(task.clone(), &ctx).await {\n                            Ok(result) => {\n                                let mut res = results.lock().await;\n                                res.insert(task.id.clone(), result);\n                            }\n                            Err(e) => {\n                                eprintln!(\"Task {} failed: {}\", task.id, e.message);\n                                // Retry or escalate\n                            }\n                        }\n                    }\n                });\n            } else {\n                tokio::time::sleep(Duration::from_millis(100)).await;\n            }\n        }\n    }\n\n    fn select_agent(agents: &[Box<dyn Agent>], task: &Task) -> Option<&Box<dyn Agent>> {\n        // Simple selection based on capabilities\n        agents.iter().max_by_key(|agent| {\n            let caps = agent.capabilities();\n            let required_cap = match task.task_type {\n                TaskType::GenerateCode { .. } => Capability::CodeGeneration,\n                TaskType::ReviewCode { .. } => Capability::CodeReview,\n                TaskType::WriteTests { .. } => Capability::Testing,\n                TaskType::FixBug { .. } => Capability::Debugging,\n                TaskType::Refactor { .. } => Capability::CodeGeneration,\n            };\n\n            if caps.contains(&required_cap) { 1 } else { 0 }\n        })\n    }\n\n    pub async fn get_result(&self, task_id: &str) -> Option<TaskResult> {\n        let results = self.results.lock().await;\n        results.get(task_id).cloned()\n    }\n}\n```",
      "terminal_task": {
        "command": "cargo test swarm -- --nocapture",
        "expected_output": "test swarm::tests::test_multi_agent_execution ... ok",
        "hint": "Utilisez async_trait pour les agents, Arc<Semaphore> pour la concurrence, et un orchestrateur avec task queue"
      }
    },
    {
      "id": "ia-004-fine-tuning",
      "track": "IA_AGENTIC",
      "level": "Expert",
      "title": "Fine-Tuning et Transfer Learning",
      "duration": "7h30",
      "description": "Fine-tuning de modèles, LoRA, QLoRA, et datasets personnalisés.",
      "markdown_content": "# Fine-Tuning et Transfer Learning\n\n## Architecture Fine-Tuning\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    FINE-TUNING PIPELINE                     │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  Base Model (7B params)                                     │\n│       │                                                     │\n│       ▼                                                     │\n│  ┌─────────────┐    ┌─────────────┐    ┌─────────────────┐ │\n│  │  Dataset    │───▶│  LoRA/QLoRA │───▶│  Fine-tuned     │ │\n│  │  (custom)   │    │  adapters   │    │  Model          │ │\n│  └─────────────┘    └─────────────┘    └─────────────────┘ │\n│                                                             │\n│  LoRA: Low-Rank Adaptation                                  │\n│  - Only train adapter layers (1% of params)                 │\n│  - Base model frozen                                        │\n│  - Much faster, less memory                                 │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Préparation Dataset\n\n```rust\nuse serde::{Deserialize, Serialize};\nuse std::fs::File;\nuse std::io::{BufRead, BufReader, Write};\n\n#[derive(Debug, Serialize, Deserialize)]\npub struct TrainingExample {\n    pub instruction: String,\n    pub input: String,\n    pub output: String,\n}\n\n#[derive(Debug, Serialize, Deserialize)]\npub struct ConversationTurn {\n    pub role: String, // \"system\", \"user\", \"assistant\"\n    pub content: String,\n}\n\n#[derive(Debug, Serialize, Deserialize)]\npub struct Conversation {\n    pub messages: Vec<ConversationTurn>,\n}\n\npub struct DatasetBuilder {\n    examples: Vec<TrainingExample>,\n    format: DatasetFormat,\n}\n\n#[derive(Clone)]\npub enum DatasetFormat {\n    Alpaca,\n    ShareGPT,\n    OpenAI,\n}\n\nimpl DatasetBuilder {\n    pub fn new(format: DatasetFormat) -> Self {\n        Self {\n            examples: vec![],\n            format,\n        }\n    }\n\n    pub fn add_example(&mut self, instruction: &str, input: &str, output: &str) {\n        self.examples.push(TrainingExample {\n            instruction: instruction.to_string(),\n            input: input.to_string(),\n            output: output.to_string(),\n        });\n    }\n\n    pub fn add_conversation(&mut self, messages: Vec<ConversationTurn>) {\n        // Convert conversation to training format\n        let instruction = messages.iter()\n            .find(|m| m.role == \"system\")\n            .map(|m| m.content.clone())\n            .unwrap_or_default();\n\n        let user_msgs: Vec<_> = messages.iter()\n            .filter(|m| m.role == \"user\")\n            .collect();\n\n        let assistant_msgs: Vec<_> = messages.iter()\n            .filter(|m| m.role == \"assistant\")\n            .collect();\n\n        for (i, (user, assistant)) in user_msgs.iter().zip(assistant_msgs.iter()).enumerate() {\n            let context: String = messages.iter()\n                .take_while(|m| m.role != \"user\" || messages.iter().position(|x| x == *m).unwrap() <= i * 2 + 1)\n                .map(|m| format!(\"{}: {}\", m.role, m.content))\n                .collect::<Vec<_>>()\n                .join(\"\\n\");\n\n            self.examples.push(TrainingExample {\n                instruction: if i == 0 { instruction.clone() } else { \"\".to_string() },\n                input: context,\n                output: assistant.content.clone(),\n            });\n        }\n    }\n\n    pub fn save(&self, path: &str) -> Result<(), std::io::Error> {\n        let mut file = File::create(path)?;\n\n        for example in &self.examples {\n            let formatted = match self.format {\n                DatasetFormat::Alpaca => self.format_alpaca(example),\n                DatasetFormat::ShareGPT => self.format_sharegpt(example),\n                DatasetFormat::OpenAI => self.format_openai(example),\n            };\n\n            writeln!(file, \"{}\", formatted)?;\n        }\n\n        Ok(())\n    }\n\n    fn format_alpaca(&self, example: &TrainingExample) -> String {\n        serde_json::json!({\n            \"instruction\": example.instruction,\n            \"input\": example.input,\n            \"output\": example.output,\n        }).to_string()\n    }\n\n    fn format_sharegpt(&self, example: &TrainingExample) -> String {\n        serde_json::json!({\n            \"conversations\": [\n                { \"from\": \"system\", \"value\": example.instruction },\n                { \"from\": \"human\", \"value\": example.input },\n                { \"from\": \"gpt\", \"value\": example.output },\n            ]\n        }).to_string()\n    }\n\n    fn format_openai(&self, example: &TrainingExample) -> String {\n        serde_json::json!({\n            \"messages\": [\n                { \"role\": \"system\", \"content\": example.instruction },\n                { \"role\": \"user\", \"content\": example.input },\n                { \"role\": \"assistant\", \"content\": example.output },\n            ]\n        }).to_string()\n    }\n\n    pub fn split(&self, train_ratio: f32) -> (Vec<TrainingExample>, Vec<TrainingExample>) {\n        let train_size = (self.examples.len() as f32 * train_ratio) as usize;\n\n        let mut shuffled = self.examples.clone();\n        // Shuffle\n        use rand::seq::SliceRandom;\n        shuffled.shuffle(&mut rand::thread_rng());\n\n        let train = shuffled[..train_size].to_vec();\n        let eval = shuffled[train_size..].to_vec();\n\n        (train, eval)\n    }\n}\n```\n\n## LoRA Configuration\n\n```rust\n#[derive(Debug, Clone)]\npub struct LoRAConfig {\n    pub r: i32,                    // Rank (4-64)\n    pub lora_alpha: i32,           // Scaling factor\n    pub lora_dropout: f32,         // Dropout rate\n    pub target_modules: Vec<String>, // Which layers to adapt\n    pub bias: String,              // \"none\", \"all\", \"lora_only\"\n    pub task_type: String,         // \"CAUSAL_LM\", \"SEQ_2_SEQ_LM\", etc.\n}\n\nimpl Default for LoRAConfig {\n    fn default() -> Self {\n        Self {\n            r: 16,\n            lora_alpha: 32,\n            lora_dropout: 0.05,\n            target_modules: vec![\n                \"q_proj\".to_string(),\n                \"v_proj\".to_string(),\n                \"k_proj\".to_string(),\n                \"o_proj\".to_string(),\n            ],\n            bias: \"none\".to_string(),\n            task_type: \"CAUSAL_LM\".to_string(),\n        }\n    }\n}\n\npub struct FineTuner {\n    config: LoRAConfig,\n    output_dir: String,\n}\n\nimpl FineTuner {\n    pub fn new(config: LoRAConfig, output_dir: &str) -> Self {\n        Self {\n            config,\n            output_dir: output_dir.to_string(),\n        }\n    }\n\n    pub fn generate_training_script(&self, model_name: &str, dataset_path: &str) -> String {\n        format!(r#\"\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\nfrom datasets import load_dataset\nimport torch\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"{}\",\n    load_in_8bit=True,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\n# Prepare for training\nmodel = prepare_model_for_kbit_training(model)\n\n# LoRA config\nlora_config = LoraConfig(\n    r={},\n    lora_alpha={},\n    target_modules={:?},\n    lora_dropout={},\n    bias=\"{}\",\n    task_type=\"{}\",\n)\n\nmodel = get_peft_model(model, lora_config)\n\n# Load dataset\ndataset = load_dataset(\"json\", data_files=\"{}\", split=\"train\")\n\n# Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"{}\")\ntokenizer.pad_token = tokenizer.eos_token\n\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[\"instruction\"] + \"\\n\" + examples[\"input\"],\n        truncation=True,\n        max_length=512,\n        padding=\"max_length\",\n    )\n\ntokenized_dataset = dataset.map(tokenize_function, batched=True)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"{}\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-4,\n    save_steps=100,\n    logging_steps=10,\n    fp16=True,\n    optim=\"paged_adamw_8bit\",\n)\n\n# Train\nfrom trl import SFTTrainer\n\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n)\n\ntrainer.train()\n\n# Save\nmodel.save_pretrained(\"{}/final\")\n\"#,\n            model_name,\n            self.config.r,\n            self.config.lora_alpha,\n            self.config.target_modules,\n            self.config.lora_dropout,\n            self.config.bias,\n            self.config.task_type,\n            dataset_path,\n            model_name,\n            self.output_dir,\n            self.output_dir,\n        )\n    }\n}\n```",
      "terminal_task": {
        "command": "cargo test fine_tuning -- --nocapture",
        "expected_output": "test fine_tuning::tests::test_dataset_builder ... ok",
        "hint": "Utilisez LoRA pour le fine-tuning efficace, préparez les datasets en formats standards (Alpaca, ShareGPT), et utilisez 8-bit quantization"
      }
    },
    {
      "id": "ia-005-embedding-models",
      "track": "IA_AGENTIC",
      "level": "Expert",
      "title": "Modèles d'Embeddings et Similarité",
      "duration": "6h00",
      "description": "Modèles d'embeddings, similarité cosinus, et indexing vectoriel.",
      "markdown_content": "# Modèles d'Embeddings\n\n## Architecture Embedding\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    EMBEDDING MODEL                          │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  Text Input ──▶ Tokenizer ──▶ Transformer ──▶ Pooling      │\n│                                                             │\n│  \"Hello world\"  [101, 7592,   [hidden        Mean/CLS       │\n│                 2088, 102]    states]        pooling       │\n│                                                             │\n│  Output: 384-dimensional vector                             │\n│  [0.023, -0.156, 0.892, ...]                               │\n│                                                             │\n│  Similarity: cos(A,B) = (A·B) / (||A|| × ||B||)            │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Client Embedding\n\n```rust\npub struct Embedder {\n    client: reqwest::Client,\n    endpoint: String,\n    model: String,\n    dimension: usize,\n}\n\nimpl Embedder {\n    pub fn new(endpoint: &str, model: &str, dimension: usize) -> Self {\n        Self {\n            client: reqwest::Client::new(),\n            endpoint: endpoint.to_string(),\n            model: model.to_string(),\n            dimension,\n        }\n    }\n\n    pub async fn embed(&self, text: &str) -> Result<Vec<f32>, String> {\n        let request = serde_json::json!({\n            \"model\": self.model,\n            \"input\": text,\n        });\n\n        let response = self.client\n            .post(&self.endpoint)\n            .json(&request)\n            .send()\n            .await\n            .map_err(|e| e.to_string())?;\n\n        let data: EmbeddingResponse = response\n            .json()\n            .await\n            .map_err(|e| e.to_string())?;\n\n        Ok(data.embeddings[0].clone())\n    }\n\n    pub async fn embed_batch(&self, texts: &[String]) -> Result<Vec<Vec<f32>>, String> {\n        let request = serde_json::json!({\n            \"model\": self.model,\n            \"input\": texts,\n        });\n\n        let response = self.client\n            .post(&self.endpoint)\n            .json(&request)\n            .send()\n            .await\n            .map_err(|e| e.to_string())?;\n\n        let data: EmbeddingResponse = response\n            .json()\n            .await\n            .map_err(|e| e.to_string())?;\n\n        Ok(data.embeddings)\n    }\n}\n\n#[derive(Debug, Deserialize)]\nstruct EmbeddingResponse {\n    embeddings: Vec<Vec<f32>>,\n}\n\n// Similarity calculations\npub fn cosine_similarity(a: &[f32], b: &[f32]) -> f32 {\n    let dot_product: f32 = a.iter().zip(b.iter()).map(|(x, y)| x * y).sum();\n    let norm_a: f32 = a.iter().map(|x| x * x).sum::<f32>().sqrt();\n    let norm_b: f32 = b.iter().map(|x| x * x).sum::<f32>().sqrt();\n\n    if norm_a == 0.0 || norm_b == 0.0 {\n        return 0.0;\n    }\n\n    dot_product / (norm_a * norm_b)\n}\n\npub fn euclidean_distance(a: &[f32], b: &[f32]) -> f32 {\n    a.iter()\n        .zip(b.iter())\n        .map(|(x, y)| (x - y).powi(2))\n        .sum::<f32>()\n        .sqrt()\n}\n\npub fn dot_product(a: &[f32], b: &[f32]) -> f32 {\n    a.iter().zip(b.iter()).map(|(x, y)| x * y).sum()\n}\n\n// Normalize vector\npub fn normalize(v: &mut [f32]) {\n    let norm: f32 = v.iter().map(|x| x * x).sum::<f32>().sqrt();\n    if norm > 0.0 {\n        for x in v.iter_mut() {\n            *x /= norm;\n        }\n    }\n}\n```\n\n## Index Vectoriel Local\n\n```rust\nuse std::collections::BinaryHeap;\nuse std::cmp::Ordering;\n\npub struct VectorIndex {\n    vectors: Vec<(String, Vec<f32>)>, // id, vector\n    dimension: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct SearchResult {\n    pub id: String,\n    pub score: f32,\n}\n\n// For min-heap (we want highest scores)\n#[derive(Debug)]\nstruct ScoredItem {\n    score: f32,\n    id: String,\n}\n\nimpl Ord for ScoredItem {\n    fn cmp(&self, other: &Self) -> Ordering {\n        self.score.partial_cmp(&other.score)\n            .unwrap_or(Ordering::Equal)\n            .reverse() // Min-heap\n    }\n}\n\nimpl PartialOrd for ScoredItem {\n    fn partial_cmp(&self, other: &Self) -> Option<Ordering> {\n        Some(self.cmp(other))\n    }\n}\n\nimpl PartialEq for ScoredItem {\n    fn eq(&self, other: &Self) -> bool {\n        self.score == other.score\n    }\n}\n\nimpl Eq for ScoredItem {}\n\nimpl VectorIndex {\n    pub fn new(dimension: usize) -> Self {\n        Self {\n            vectors: vec![],\n            dimension,\n        }\n    }\n\n    pub fn add(&mut self, id: &str, vector: Vec<f32>) {\n        assert_eq!(vector.len(), self.dimension);\n        self.vectors.push((id.to_string(), vector));\n    }\n\n    pub fn search(&self, query: &[f32], k: usize) -> Vec<SearchResult> {\n        assert_eq!(query.len(), self.dimension);\n\n        let mut heap: BinaryHeap<ScoredItem> = BinaryHeap::new();\n\n        for (id, vector) in &self.vectors {\n            let score = cosine_similarity(query, vector);\n\n            if heap.len() < k {\n                heap.push(ScoredItem {\n                    score,\n                    id: id.clone(),\n                });\n            } else if let Some(min) = heap.peek() {\n                if score > min.score {\n                    heap.pop();\n                    heap.push(ScoredItem {\n                        score,\n                        id: id.clone(),\n                    });\n                }\n            }\n        }\n\n        let mut results: Vec<_> = heap.into_iter()\n            .map(|item| SearchResult {\n                id: item.id,\n                score: item.score,\n            })\n            .collect();\n\n        results.sort_by(|a, b| b.score.partial_cmp(&a.score).unwrap());\n        results\n    }\n\n    // Approximate search with LSH (Locality Sensitive Hashing)\n    pub fn build_lsh_index(&self, num_hashes: usize, num_bands: usize) -> LSHIndex {\n        let mut lsh = LSHIndex::new(self.dimension, num_hashes, num_bands);\n\n        for (id, vector) in &self.vectors {\n            lsh.add(id, vector);\n        }\n\n        lsh\n    }\n}\n\n// LSH for approximate nearest neighbors\npub struct LSHIndex {\n    dimension: usize,\n    num_hashes: usize,\n    num_bands: usize,\n    random_vectors: Vec<Vec<f32>>,\n    buckets: Vec<std::collections::HashMap<u64, Vec<String>>>,\n}\n\nimpl LSHIndex {\n    pub fn new(dimension: usize, num_hashes: usize, num_bands: usize) -> Self {\n        use rand::Rng;\n        let mut rng = rand::thread_rng();\n\n        let random_vectors: Vec<Vec<f32>> = (0..num_hashes)\n            .map(|_| {\n                (0..dimension)\n                    .map(|_| rng.gen::<f32>() * 2.0 - 1.0)\n                    .collect()\n            })\n            .collect();\n\n        Self {\n            dimension,\n            num_hashes,\n            num_bands,\n            random_vectors,\n            buckets: vec![std::collections::HashMap::new(); num_bands],\n        }\n    }\n\n    pub fn add(&mut self, id: &str, vector: &[f32]) {\n        let hashes: Vec<bool> = self.random_vectors.iter()\n            .map(|rv| dot_product(vector, rv) > 0.0)\n            .collect();\n\n        let rows_per_band = self.num_hashes / self.num_bands;\n\n        for band in 0..self.num_bands {\n            let start = band * rows_per_band;\n            let end = start + rows_per_band;\n\n            let band_signature = Self::hash_band(&hashes[start..end]);\n\n            self.buckets[band]\n                .entry(band_signature)\n                .or_insert_with(Vec::new)\n                .push(id.to_string());\n        }\n    }\n\n    fn hash_band(band: &[bool]) -> u64 {\n        use std::collections::hash_map::DefaultHasher;\n        use std::hash::{Hash, Hasher};\n\n        let mut hasher = DefaultHasher::new();\n        band.hash(&mut hasher);\n        hasher.finish()\n    }\n\n    pub fn query(&self, vector: &[f32], k: usize) -> Vec<String> {\n        let hashes: Vec<bool> = self.random_vectors.iter()\n            .map(|rv| dot_product(vector, rv) > 0.0)\n            .collect();\n\n        let rows_per_band = self.num_hashes / self.num_bands;\n        let mut candidates = std::collections::HashSet::new();\n\n        for band in 0..self.num_bands {\n            let start = band * rows_per_band;\n            let end = start + rows_per_band;\n\n            let band_signature = Self::hash_band(&hashes[start..end]);\n\n            if let Some(bucket) = self.buckets[band].get(&band_signature) {\n                for id in bucket {\n                    candidates.insert(id.clone());\n                }\n            }\n        }\n\n        candidates.into_iter().take(k).collect()\n    }\n}\n```",
      "terminal_task": {
        "command": "cargo test embeddings -- --nocapture",
        "expected_output": "test embeddings::tests::test_cosine_similarity ... ok",
        "hint": "Utilisez cosine_similarity pour la recherche sémantique, BinaryHeap pour les k-plus-proches, et LSH pour l'approximation"
      }
    },
    {
      "id": "ia-006-prompt-engineering",
      "track": "IA_AGENTIC",
      "level": "Expert",
      "title": "Prompt Engineering Avancé",
      "duration": "5h30",
      "description": "Chain-of-thought, few-shot prompting, et patterns de prompts.",
      "markdown_content": "# Prompt Engineering Avancé\n\n## Patterns de Prompts\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    PROMPT PATTERNS                          │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  Chain-of-Thought                                           │\n│  \"Let's think step by step...\"                              │\n│                                                             │\n│  Few-Shot                                                   │\n│  Input: X → Output: Y                                       │\n│  Input: A → Output: B                                       │\n│  Input: ? → Output:                                         │\n│                                                             │\n│  ReAct (Reasoning + Acting)                                 │\n│  Thought → Action → Observation → ...                       │\n│                                                             │\n│  Tree-of-Thoughts                                           │\n│  Explore multiple reasoning paths                           │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Prompt Builder\n\n```rust\npub struct PromptBuilder {\n    system_prompt: String,\n    examples: Vec<Example>,\n    context: Vec<String>,\n    instructions: Vec<String>,\n    output_format: Option<String>,\n    constraints: Vec<String>,\n}\n\n#[derive(Clone)]\npub struct Example {\n    pub input: String,\n    pub output: String,\n    pub reasoning: Option<String>,\n}\n\nimpl PromptBuilder {\n    pub fn new() -> Self {\n        Self {\n            system_prompt: String::new(),\n            examples: vec![],\n            context: vec![],\n            instructions: vec![],\n            output_format: None,\n            constraints: vec![],\n        }\n    }\n\n    pub fn system(mut self, prompt: &str) -> Self {\n        self.system_prompt = prompt.to_string();\n        self\n    }\n\n    pub fn example(mut self, input: &str, output: &str) -> Self {\n        self.examples.push(Example {\n            input: input.to_string(),\n            output: output.to_string(),\n            reasoning: None,\n        });\n        self\n    }\n\n    pub fn example_with_reasoning(\n        mut self,\n        input: &str,\n        reasoning: &str,\n        output: &str,\n    ) -> Self {\n        self.examples.push(Example {\n            input: input.to_string(),\n            output: output.to_string(),\n            reasoning: Some(reasoning.to_string()),\n        });\n        self\n    }\n\n    pub fn context(mut self, context: &str) -> Self {\n        self.context.push(context.to_string());\n        self\n    }\n\n    pub fn instruction(mut self, instruction: &str) -> Self {\n        self.instructions.push(instruction.to_string());\n        self\n    }\n\n    pub fn output_format(mut self, format: &str) -> Self {\n        self.output_format = Some(format.to_string());\n        self\n    }\n\n    pub fn constraint(mut self, constraint: &str) -> Self {\n        self.constraints.push(constraint.to_string());\n        self\n    }\n\n    pub fn build(self) -> String {\n        let mut prompt = String::new();\n\n        // System prompt\n        if !self.system_prompt.is_empty() {\n            prompt.push_str(&format!(\"System: {}\\n\\n\", self.system_prompt));\n        }\n\n        // Context\n        if !self.context.is_empty() {\n            prompt.push_str(\"Context:\\n\");\n            for ctx in &self.context {\n                prompt.push_str(&format!(\"- {}\\n\", ctx));\n            }\n            prompt.push_str(\"\\n\");\n        }\n\n        // Examples (few-shot)\n        if !self.examples.is_empty() {\n            prompt.push_str(\"Examples:\\n\");\n            for (i, ex) in self.examples.iter().enumerate() {\n                prompt.push_str(&format!(\"Example {}:\\n\", i + 1));\n                prompt.push_str(&format!(\"Input: {}\\n\", ex.input));\n                if let Some(ref reasoning) = ex.reasoning {\n                    prompt.push_str(&format!(\"Reasoning: {}\\n\", reasoning));\n                }\n                prompt.push_str(&format!(\"Output: {}\\n\\n\", ex.output));\n            }\n        }\n\n        // Instructions\n        if !self.instructions.is_empty() {\n            prompt.push_str(\"Instructions:\\n\");\n            for (i, inst) in self.instructions.iter().enumerate() {\n                prompt.push_str(&format!(\"{}. {}\\n\", i + 1, inst));\n            }\n            prompt.push_str(\"\\n\");\n        }\n\n        // Output format\n        if let Some(format) = self.output_format {\n            prompt.push_str(&format!(\"Output format: {}\\n\\n\", format));\n        }\n\n        // Constraints\n        if !self.constraints.is_empty() {\n            prompt.push_str(\"Constraints:\\n\");\n            for constraint in &self.constraints {\n                prompt.push_str(&format!(\"- {}\\n\", constraint));\n            }\n            prompt.push_str(\"\\n\");\n        }\n\n        prompt.push_str(\"Now, provide your response:\");\n\n        prompt\n    }\n\n    // Pre-built patterns\n    pub fn chain_of_thought() -> Self {\n        Self::new()\n            .instruction(\"Think step by step and explain your reasoning\")\n            .instruction(\"Break down complex problems into smaller steps\")\n            .output_format(\"Step-by-step reasoning followed by final answer\")\n    }\n\n    pub fn code_review() -> Self {\n        Self::new()\n            .system(\"You are an expert code reviewer\")\n            .instruction(\"Analyze the code for bugs, security issues, and performance problems\")\n            .instruction(\"Suggest specific improvements with code examples\")\n            .output_format(\"JSON with fields: issues (array), suggestions (array), improved_code (string)\")\n    }\n\n    pub fn json_extractor() -> Self {\n        Self::new()\n            .instruction(\"Extract structured information from the text\")\n            .constraint(\"Output valid JSON only\")\n            .constraint(\"Do not include markdown formatting\")\n    }\n}\n\n// Response parsers\npub fn extract_json(response: &str) -> Result<serde_json::Value, String> {\n    // Try to find JSON in markdown code blocks\n    if let Some(start) = response.find(\"```json\") {\n        if let Some(end) = response[start..].find(\"```\") {\n            let json_str = &response[start + 7..start + end];\n            return serde_json::from_str(json_str.trim())\n                .map_err(|e| e.to_string());\n        }\n    }\n\n    // Try direct parse\n    serde_json::from_str(response.trim())\n        .map_err(|e| e.to_string())\n}\n\npub fn extract_code_blocks(response: &str, language: Option<&str>) -> Vec<String> {\n    let mut blocks = vec![];\n    let pattern = if let Some(lang) = language {\n        format!(\"```{}\\n\", lang)\n    } else {\n        \"```\\n\".to_string()\n    };\n\n    let mut start = 0;\n    while let Some(block_start) = response[start..].find(&pattern) {\n        let block_start = start + block_start + pattern.len();\n        if let Some(block_end) = response[block_start..].find(\"```\") {\n            blocks.push(response[block_start..block_start + block_end].to_string());\n            start = block_start + block_end + 3;\n        } else {\n            break;\n        }\n    }\n\n    blocks\n}\n```",
      "terminal_task": {
        "command": "cargo test prompt_engineering -- --nocapture",
        "expected_output": "test prompt_engineering::tests::test_prompt_builder ... ok",
        "hint": "Utilisez un builder pattern pour les prompts, few-shot pour la cohérence, et chain-of-thought pour le raisonnement"
      }
    },
    {
      "id": "ia-007-llm-evaluation",
      "track": "IA_AGENTIC",
      "level": "Expert",
      "title": "Évaluation et Benchmarking de LLMs",
      "duration": "5h00",
      "description": "Métriques d'évaluation, benchmarks, et comparaison de modèles.",
      "markdown_content": "# Évaluation de LLMs\n\n## Métriques d'Évaluation\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    LLM EVALUATION                           │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  Perplexity                                                 │\n│  ↓ = meilleure prédiction                                   │\n│                                                             │\n│  BLEU / ROUGE                                               │\n│  ↑ = similarité avec référence                              │\n│                                                             │\n│  HumanEval / MBPP                                           │\n│  % de code qui compile et passe les tests                   │\n│                                                             │\n│  MMLU / Hellaswag                                           │\n│  % de réponses correctes sur benchmarks académiques         │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Évaluateur de Code\n\n```rust\npub struct CodeEvaluator {\n    temp_dir: PathBuf,\n    timeout: Duration,\n}\n\n#[derive(Debug)]\npub struct EvaluationResult {\n    pub passed: bool,\n    pub compile_output: String,\n    pub run_output: String,\n    pub execution_time: Duration,\n    pub memory_usage: usize,\n}\n\nimpl CodeEvaluator {\n    pub fn new() -> Result<Self, std::io::Error> {\n        let temp_dir = tempfile::tempdir()?.into_path();\n\n        Ok(Self {\n            temp_dir,\n            timeout: Duration::from_secs(10),\n        })\n    }\n\n    pub async fn evaluate_rust(\n        &self,\n        code: &str,\n        test_cases: &[TestCase],\n    ) -> Result<EvaluationResult, String> {\n        // Write code to file\n        let code_path = self.temp_dir.join(\"main.rs\");\n        std::fs::write(&code_path, code).map_err(|e| e.to_string())?;\n\n        // Create Cargo.toml\n        let cargo_toml = r#\"\n[package]\nname = \"eval\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\"#;\n        std::fs::write(self.temp_dir.join(\"Cargo.toml\"), cargo_toml)\n            .map_err(|e| e.to_string())?;\n\n        // Compile\n        let compile_output = Command::new(\"cargo\")\n            .args(&[\"build\", \"--release\"])\n            .current_dir(&self.temp_dir)\n            .output()\n            .await\n            .map_err(|e| e.to_string())?;\n\n        let compile_stderr = String::from_utf8_lossy(&compile_output.stderr).to_string();\n\n        if !compile_output.status.success() {\n            return Ok(EvaluationResult {\n                passed: false,\n                compile_output: compile_stderr,\n                run_output: String::new(),\n                execution_time: Duration::ZERO,\n                memory_usage: 0,\n            });\n        }\n\n        // Run tests\n        let start = Instant::now();\n        let run_output = Command::new(\"cargo\")\n            .args(&[\"test\", \"--release\"])\n            .current_dir(&self.temp_dir)\n            .timeout(self.timeout)\n            .output()\n            .await\n            .map_err(|e| e.to_string())?;\n\n        let execution_time = start.elapsed();\n\n        let run_stdout = String::from_utf8_lossy(&run_output.stdout).to_string();\n        let passed = run_output.status.success();\n\n        Ok(EvaluationResult {\n            passed,\n            compile_output: compile_stderr,\n            run_output: run_stdout,\n            execution_time,\n            memory_usage: 0,\n        })\n    }\n\n    pub async fn evaluate_with_humaneval(&self, model: &dyn LLM) -> HumanevalResult {\n        let problems = self.load_humaneval_problems().await;\n        let mut passed = 0;\n        let mut total = 0;\n\n        for problem in problems {\n            let prompt = format!(\"{}\\n\\n{}\", problem.prompt, problem.canonical_solution);\n            let generated = model.generate(&prompt).await.unwrap_or_default();\n\n            let code = format!(\"{}\\n{}\", problem.prompt, generated);\n            let result = self.evaluate_rust(&code, &problem.test_cases).await;\n\n            if result.map(|r| r.passed).unwrap_or(false) {\n                passed += 1;\n            }\n            total += 1;\n        }\n\n        HumanevalResult {\n            pass_at_1: passed as f32 / total as f32,\n            total_problems: total,\n            passed_problems: passed,\n        }\n    }\n}\n\n#[derive(Debug)]\npub struct HumanevalResult {\n    pub pass_at_1: f32,\n    pub total_problems: usize,\n    pub passed_problems: usize,\n}\n\n#[derive(Debug)]\npub struct TestCase {\n    pub input: String,\n    pub expected_output: String,\n}\n\n#[derive(Debug)]\npub struct HumanevalProblem {\n    pub task_id: String,\n    pub prompt: String,\n    pub entry_point: String,\n    pub canonical_solution: String,\n    pub test_cases: Vec<TestCase>,\n}\n```\n\n## Benchmark Runner\n\n```rust\npub struct BenchmarkRunner {\n    models: Vec<Box<dyn LLM>>,\n    datasets: Vec<Box<dyn BenchmarkDataset>>,\n}\n\n#[async_trait]\npub trait BenchmarkDataset: Send + Sync {\n    fn name(&self) -> &str;\n    async fn load(&self) -> Vec<BenchmarkSample>;\n    fn evaluate(&self, prediction: &str, reference: &str) -> f32;\n}\n\n#[derive(Debug)]\npub struct BenchmarkSample {\n    pub input: String,\n    pub reference: String,\n    pub metadata: serde_json::Value,\n}\n\n#[derive(Debug)]\npub struct BenchmarkReport {\n    pub model_name: String,\n    pub dataset_name: String,\n    pub score: f32,\n    pub samples_evaluated: usize,\n    pub per_sample_scores: Vec<f32>,\n    pub metadata: serde_json::Value,\n}\n\nimpl BenchmarkRunner {\n    pub async fn run(&self) -> Vec<BenchmarkReport> {\n        let mut reports = vec![];\n\n        for model in &self.models {\n            for dataset in &self.datasets {\n                let samples = dataset.load().await;\n                let mut scores = vec![];\n\n                for sample in &samples {\n                    let prediction = model.generate(&sample.input).await\n                        .unwrap_or_default();\n                    let score = dataset.evaluate(&prediction, &sample.reference);\n                    scores.push(score);\n                }\n\n                let avg_score = scores.iter().sum::<f32>() / scores.len() as f32;\n\n                reports.push(BenchmarkReport {\n                    model_name: model.name().to_string(),\n                    dataset_name: dataset.name().to_string(),\n                    score: avg_score,\n                    samples_evaluated: samples.len(),\n                    per_sample_scores: scores,\n                    metadata: serde_json::json!({}),\n                });\n            }\n        }\n\n        reports\n    }\n\n    pub fn compare_models(&self, reports: &[BenchmarkReport]) -> ComparisonTable {\n        let mut table = ComparisonTable::default();\n\n        for report in reports {\n            table.add_score(&report.model_name, &report.dataset_name, report.score);\n        }\n\n        table\n    }\n}\n\n#[derive(Default)]\npub struct ComparisonTable {\n    rows: HashMap<String, HashMap<String, f32>>,\n}\n\nimpl ComparisonTable {\n    fn add_score(&mut self, model: &str, dataset: &str, score: f32) {\n        self.rows\n            .entry(model.to_string())\n            .or_insert_with(HashMap::new)\n            .insert(dataset.to_string(), score);\n    }\n\n    pub fn to_markdown(&self) -> String {\n        let mut md = String::new();\n\n        // Header\n        let datasets: Vec<_> = self.rows.values()\n            .flat_map(|m| m.keys())\n            .collect::<std::collections::HashSet<_>>()\n            .into_iter()\n            .collect();\n\n        md.push_str(\"| Model |\");\n        for dataset in &datasets {\n            md.push_str(&format!(\" {} |\", dataset));\n        }\n        md.push_str(\"\\n\");\n\n        md.push_str(\"|-------|\");\n        for _ in &datasets {\n            md.push_str(\"-------|\");\n        }\n        md.push_str(\"\\n\");\n\n        // Rows\n        for (model, scores) in &self.rows {\n            md.push_str(&format!(\"| {} |\", model));\n            for dataset in &datasets {\n                let score = scores.get(*dataset).unwrap_or(&0.0);\n                md.push_str(&format!(\" {:.2} |\", score));\n            }\n            md.push_str(\"\\n\");\n        }\n\n        md\n    }\n}\n```",
      "terminal_task": {
        "command": "cargo test llm_evaluation -- --nocapture",
        "expected_output": "test llm_evaluation::tests::test_code_eval ... ok",
        "hint": "Utilisez des benchmarks standards (HumanEval, MMLU), exécutez le code dans un sandbox, et comparez avec métriques objectives"
      }
    },
    {
      "id": "ia-008-function-calling",
      "track": "IA_AGENTIC",
      "level": "Expert",
      "title": "Function Calling et Tool Use",
      "duration": "6h00",
      "description": "Appels de fonctions par LLM, définition d'outils, et exécution.",
      "markdown_content": "# Function Calling\n\n## Architecture Function Calling\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    FUNCTION CALLING                         │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  User Request                                               │\n│       │                                                     │\n│       ▼                                                     │\n│  ┌─────────────┐    ┌─────────────┐    ┌─────────────────┐ │\n│  │   LLM       │───▶│  Parse      │───▶│  Execute Tool   │ │\n│  │  (decide)   │    │  Function   │    │                 │ │\n│  └─────────────┘    │  Call       │    └────────┬────────┘ │\n│                     └─────────────┘             │          │\n│                                                 ▼          │\n│                                          ┌─────────────┐   │\n│                                          │  Return     │   │\n│                                          │  Result     │   │\n│                                          └──────┬──────┘   │\n│                                                 │          │\n│                                          ┌──────┴──────┐   │\n│                                          │  LLM Answer │   │\n│                                          └─────────────┘   │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Définition d'Outils\n\n```rust\nuse serde::{Deserialize, Serialize};\nuse serde_json::Value;\nuse std::collections::HashMap;\n\n#[derive(Debug, Clone, Serialize)]\npub struct Tool {\n    pub name: String,\n    pub description: String,\n    pub parameters: ToolParameters,\n}\n\n#[derive(Debug, Clone, Serialize)]\npub struct ToolParameters {\n    pub type_name: String,\n    pub properties: HashMap<String, ParameterSchema>,\n    pub required: Vec<String>,\n}\n\n#[derive(Debug, Clone, Serialize)]\npub struct ParameterSchema {\n    pub type_name: String,\n    pub description: String,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub enum_values: Option<Vec<String>>,\n}\n\n#[derive(Debug, Clone, Deserialize)]\npub struct FunctionCall {\n    pub name: String,\n    pub arguments: Value,\n}\n\n// Tool registry\npub struct ToolRegistry {\n    tools: HashMap<String, Tool>,\n    handlers: HashMap<String, Box<dyn ToolHandler>>,\n}\n\n#[async_trait]\npub trait ToolHandler: Send + Sync {\n    async fn execute(&self, arguments: Value) -> Result<Value, String>;\n}\n\nimpl ToolRegistry {\n    pub fn new() -> Self {\n        Self {\n            tools: HashMap::new(),\n            handlers: HashMap::new(),\n        }\n    }\n\n    pub fn register<H: ToolHandler + 'static>(\n        &mut self,\n        tool: Tool,\n        handler: H,\n    ) {\n        let name = tool.name.clone();\n        self.tools.insert(name.clone(), tool);\n        self.handlers.insert(name, Box::new(handler));\n    }\n\n    pub fn get_tool_definitions(&self) -> Vec<Tool> {\n        self.tools.values().cloned().collect()\n    }\n\n    pub async fn execute(&self, call: &FunctionCall) -> Result<Value, String> {\n        let handler = self.handlers.get(&call.name)\n            .ok_or_else(|| format!(\"Unknown tool: {}\", call.name))?;\n\n        handler.execute(call.arguments.clone()).await\n    }\n}\n\n// Built-in tools\npub struct CalculatorTool;\n\n#[async_trait]\nimpl ToolHandler for CalculatorTool {\n    async fn execute(&self, arguments: Value) -> Result<Value, String> {\n        let expression = arguments.get(\"expression\")\n            .and_then(|v| v.as_str())\n            .ok_or(\"Missing expression\")?;\n\n        // Simple eval (use proper parser in production)\n        let result = eval_expression(expression)?;\n\n        Ok(serde_json::json!({ \"result\": result }))\n    }\n}\n\nfn eval_expression(expr: &str) -> Result<f64, String> {\n    // Use meval or similar\n    meval::eval_str(expr).map_err(|e| e.to_string())\n}\n\npub struct WeatherTool {\n    api_key: String,\n}\n\n#[async_trait]\nimpl ToolHandler for WeatherTool {\n    async fn execute(&self, arguments: Value) -> Result<Value, String> {\n        let location = arguments.get(\"location\")\n            .and_then(|v| v.as_str())\n            .ok_or(\"Missing location\")?;\n\n        let client = reqwest::Client::new();\n        let response = client\n            .get(\"https://api.weather.com/v1/current\")\n            .query(&[(\"location\", location), (\"api_key\", &self.api_key)])\n            .send()\n            .await\n            .map_err(|e| e.to_string())?;\n\n        let data: serde_json::Value = response.json().await\n            .map_err(|e| e.to_string())?;\n\n        Ok(serde_json::json!({\n            \"temperature\": data[\"temp\"],\n            \"conditions\": data[\"conditions\"],\n            \"humidity\": data[\"humidity\"],\n        }))\n    }\n}\n\n// Setup\npub fn setup_tools() -> ToolRegistry {\n    let mut registry = ToolRegistry::new();\n\n    registry.register(\n        Tool {\n            name: \"calculator\".to_string(),\n            description: \"Evaluate mathematical expressions\".to_string(),\n            parameters: ToolParameters {\n                type_name: \"object\".to_string(),\n                properties: {\n                    let mut props = HashMap::new();\n                    props.insert(\n                        \"expression\".to_string(),\n                        ParameterSchema {\n                            type_name: \"string\".to_string(),\n                            description: \"The mathematical expression to evaluate\".to_string(),\n                            enum_values: None,\n                        },\n                    );\n                    props\n                },\n                required: vec![\"expression\".to_string()],\n            },\n        },\n        CalculatorTool,\n    );\n\n    registry.register(\n        Tool {\n            name: \"get_weather\".to_string(),\n            description: \"Get current weather for a location\".to_string(),\n            parameters: ToolParameters {\n                type_name: \"object\".to_string(),\n                properties: {\n                    let mut props = HashMap::new();\n                    props.insert(\n                        \"location\".to_string(),\n                        ParameterSchema {\n                            type_name: \"string\".to_string(),\n                            description: \"City name or coordinates\".to_string(),\n                            enum_values: None,\n                        },\n                    );\n                    props\n                },\n                required: vec![\"location\".to_string()],\n            },\n        },\n        WeatherTool { api_key: std::env::var(\"WEATHER_API_KEY\").unwrap() },\n    );\n\n    registry\n}\n```\n\n## Function Calling avec Ollama\n\n```rust\npub struct FunctionCallingClient {\n    ollama: OllamaClient,\n    tools: ToolRegistry,\n}\n\nimpl FunctionCallingClient {\n    pub fn new(ollama: OllamaClient, tools: ToolRegistry) -> Self {\n        Self { ollama, tools }\n    }\n\n    pub async fn chat(&self, message: &str) -> Result<String, String> {\n        // Build prompt with tools\n        let tools_json = serde_json::to_string(&self.tools.get_tool_definitions())\n            .map_err(|e| e.to_string())?;\n\n        let prompt = format!(r#\"\nYou have access to the following tools:\n{}\n\nTo use a tool, respond with a JSON object in this format:\n{{\"tool\": \"tool_name\", \"arguments\": {{\"arg1\": \"value1\"}}}}\n\nIf no tool is needed, respond directly.\n\nUser: {}\n\"#, tools_json, message);\n\n        let response = self.ollama.generate(\"llama3.2\", &prompt, None).await\n            .map_err(|e| e.to_string())?;\n\n        // Try to parse as function call\n        if let Ok(call) = serde_json::from_str::<FunctionCall>(&response.response) {\n            // Execute tool\n            let result = self.tools.execute(&call).await?;\n\n            // Get final answer with tool result\n            let final_prompt = format!(r#\"\nTool result: {}\n\nProvide a natural language response to the user.\n\"#, serde_json::to_string(&result).unwrap());\n\n            let final_response = self.ollama.generate(\"llama3.2\", &final_prompt, None).await\n                .map_err(|e| e.to_string())?;\n\n            Ok(final_response.response)\n        } else {\n            // Direct response\n            Ok(response.response)\n        }\n    }\n}\n```",
      "terminal_task": {
        "command": "cargo test function_calling -- --nocapture",
        "expected_output": "test function_calling::tests::test_tool_execution ... ok",
        "hint": "Définissez les outils avec JSON Schema, utilisez un registry pour les handlers, et parsez les appels de fonction du LLM"
      }
    },
    {
      "id": "ia-009-observability",
      "track": "IA_AGENTIC",
      "level": "Expert",
      "title": "Observabilité et Monitoring LLM",
      "duration": "5h00",
      "description": "Tracing, logging, et métriques pour systèmes LLM.",
      "markdown_content": "# Observabilité LLM\n\n## Architecture d'Observabilité\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    LLM OBSERVABILITY                        │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  Application                                                │\n│       │                                                     │\n│       ▼                                                     │\n│  ┌─────────────┐    ┌─────────────┐    ┌─────────────────┐ │\n│  │   Tracing   │───▶│   Logging   │───▶│   Metrics       │ │\n│  │  (spans)    │    │  (events)   │    │  (prometheus)   │ │\n│  └─────────────┘    └─────────────┘    └─────────────────┘ │\n│                                                             │\n│  Dashboards:                                                │\n│  - Latency P50/P95/P99                                      │\n│  - Token usage per request                                  │\n│  - Cost tracking                                            │\n│  - Error rates                                              │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## LLM Tracer\n\n```rust\nuse tracing::{info, span, Level, Span};\nuse std::time::Instant;\n\npub struct LLMTracer {\n    span: Span,\n    start: Instant,\n    model: String,\n    prompt_tokens: usize,\n    completion_tokens: usize,\n}\n\nimpl LLMTracer {\n    pub fn new(model: &str, prompt: &str) -> Self {\n        let span = span!(Level::INFO, \"llm_request\",\n            model = model,\n            prompt_length = prompt.len(),\n        );\n\n        let _enter = span.enter();\n        info!(\"LLM request started\");\n\n        Self {\n            span,\n            start: Instant::now(),\n            model: model.to_string(),\n            prompt_tokens: 0,\n            completion_tokens: 0,\n        }\n    }\n\n    pub fn set_token_counts(&mut self, prompt: usize, completion: usize) {\n        self.prompt_tokens = prompt;\n        self.completion_tokens = completion;\n    }\n\n    pub fn finish(self, success: bool) {\n        let duration = self.start.elapsed();\n        let total_tokens = self.prompt_tokens + self.completion_tokens;\n\n        let _enter = self.span.enter();\n\n        info!(\n            duration_ms = duration.as_millis() as u64,\n            prompt_tokens = self.prompt_tokens,\n            completion_tokens = self.completion_tokens,\n            total_tokens = total_tokens,\n            success = success,\n            \"LLM request completed\"\n        );\n\n        // Record metrics\n        metrics::histogram!(\"llm_request_duration\", duration.as_secs_f64());\n        metrics::counter!(\"llm_tokens_total\", total_tokens as u64);\n        metrics::gauge!(\"llm_prompt_tokens\", self.prompt_tokens as f64);\n        metrics::gauge!(\"llm_completion_tokens\", self.completion_tokens as f64);\n\n        if success {\n            metrics::counter!(\"llm_requests_success\", 1);\n        } else {\n            metrics::counter!(\"llm_requests_failed\", 1);\n        }\n    }\n}\n\n// Cost tracking\npub struct CostTracker {\n    model_pricing: HashMap<String, Pricing>,\n    total_cost: Arc<Mutex<f64>>,\n}\n\n#[derive(Clone)]\npub struct Pricing {\n    pub prompt_price_per_1k: f64,    // USD\n    pub completion_price_per_1k: f64, // USD\n}\n\nimpl CostTracker {\n    pub fn new() -> Self {\n        let mut pricing = HashMap::new();\n\n        // OpenAI pricing (example)\n        pricing.insert(\"gpt-4\".to_string(), Pricing {\n            prompt_price_per_1k: 0.03,\n            completion_price_per_1k: 0.06,\n        });\n\n        pricing.insert(\"gpt-3.5-turbo\".to_string(), Pricing {\n            prompt_price_per_1k: 0.0015,\n            completion_price_per_1k: 0.002,\n        });\n\n        Self {\n            model_pricing: pricing,\n            total_cost: Arc::new(Mutex::new(0.0)),\n        }\n    }\n\n    pub fn calculate_cost(&self, model: &str, prompt_tokens: usize, completion_tokens: usize) -> f64 {\n        let pricing = self.model_pricing.get(model)?;\n\n        let prompt_cost = (prompt_tokens as f64 / 1000.0) * pricing.prompt_price_per_1k;\n        let completion_cost = (completion_tokens as f64 / 1000.0) * pricing.completion_price_per_1k;\n\n        prompt_cost + completion_cost\n    }\n\n    pub async fn track(&self, model: &str, prompt_tokens: usize, completion_tokens: usize) {\n        if let Some(cost) = self.calculate_cost(model, prompt_tokens, completion_tokens) {\n            let mut total = self.total_cost.lock().await;\n            *total += cost;\n\n            metrics::counter!(\"llm_cost_usd\", (cost * 1_000_000.0) as u64);\n        }\n    }\n\n    pub async fn get_total_cost(&self) -> f64 {\n        *self.total_cost.lock().await\n    }\n}\n\n// Request logging\npub struct RequestLogger {\n    log_file: Arc<Mutex<File>>,\n}\n\n#[derive(Serialize)]\npub struct LogEntry {\n    pub timestamp: String,\n    pub model: String,\n    pub prompt: String,\n    pub response: String,\n    pub prompt_tokens: usize,\n    pub completion_tokens: usize,\n    pub duration_ms: u64,\n    pub metadata: serde_json::Value,\n}\n\nimpl RequestLogger {\n    pub async fn new(path: &str) -> Result<Self, std::io::Error> {\n        let file = OpenOptions::new()\n            .create(true)\n            .append(true)\n            .open(path)\n            .await?;\n\n        Ok(Self {\n            log_file: Arc::new(Mutex::new(file)),\n        })\n    }\n\n    pub async fn log(&self, entry: LogEntry) -> Result<(), std::io::Error> {\n        let mut file = self.log_file.lock().await;\n        let line = serde_json::to_string(&entry)?;\n        file.write_all(line.as_bytes()).await?;\n        file.write_all(b\"\\n\").await?;\n        Ok(())\n    }\n}\n```",
      "terminal_task": {
        "command": "cargo test observability -- --nocapture",
        "expected_output": "test observability::tests::test_tracing ... ok",
        "hint": "Utilisez tracing pour les spans, metrics pour les compteurs, et un CostTracker pour le suivi des coûts"
      }
    },
    {
      "id": "ia-010-guardrails",
      "track": "IA_AGENTIC",
      "level": "Expert",
      "title": "Guardrails et Sécurité IA",
      "duration": "5h30",
      "description": "Détection de contenu dangereux, PII filtering, et safety layers.",
      "markdown_content": "# Guardrails et Sécurité IA\n\n## Architecture de Sécurité\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    AI SAFETY LAYERS                         │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  Input ──▶ Filter ──▶ LLM ──▶ Output Filter ──▶ User       │\n│                                                             │\n│  Filters:                                                   │\n│  - PII detection (email, phone, SSN)                        │\n│  - Toxicity detection                                       │\n│  - Prompt injection detection                               │\n│  - Jailbreak detection                                      │\n│                                                             │\n│  Output:                                                    │\n│  - Content moderation                                       │\n│  - Fact checking (optional)                                 │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Input Validator\n\n```rust\nuse regex::Regex;\n\npub struct InputValidator {\n    pii_patterns: Vec<(Regex, PIIType)>,\n    blocked_keywords: Vec<String>,\n    max_length: usize,\n}\n\n#[derive(Debug, Clone)]\npub enum PIIType {\n    Email,\n    Phone,\n    SSN,\n    CreditCard,\n    IPAddress,\n}\n\n#[derive(Debug)]\npub struct ValidationResult {\n    pub is_valid: bool,\n    pub violations: Vec<Violation>,\n    pub sanitized_input: String,\n}\n\n#[derive(Debug)]\npub struct Violation {\n    pub violation_type: ViolationType,\n    pub message: String,\n    pub position: Option<(usize, usize)>,\n}\n\n#[derive(Debug)]\npub enum ViolationType {\n    PII(PIIType),\n    BlockedKeyword,\n    PromptInjection,\n    TooLong,\n    Toxic,\n}\n\nimpl InputValidator {\n    pub fn new() -> Self {\n        let mut pii_patterns = vec![];\n\n        // Email\n        pii_patterns.push((\n            Regex::new(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\").unwrap(),\n            PIIType::Email,\n        ));\n\n        // Phone (US)\n        pii_patterns.push((\n            Regex::new(r\"\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b\").unwrap(),\n            PIIType::Phone,\n        ));\n\n        // SSN\n        pii_patterns.push((\n            Regex::new(r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\").unwrap(),\n            PIIType::SSN,\n        ));\n\n        // Credit card\n        pii_patterns.push((\n            Regex::new(r\"\\b\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}\\b\").unwrap(),\n            PIIType::CreditCard,\n        ));\n\n        Self {\n            pii_patterns,\n            blocked_keywords: vec![\n                \"ignore previous instructions\".to_string(),\n                \"disregard\".to_string(),\n                \"DAN\".to_string(),\n                \"jailbreak\".to_string(),\n            ],\n            max_length: 4000,\n        }\n    }\n\n    pub fn validate(&self, input: &str) -> ValidationResult {\n        let mut violations = vec![];\n        let mut sanitized = input.to_string();\n\n        // Check length\n        if input.len() > self.max_length {\n            violations.push(Violation {\n                violation_type: ViolationType::TooLong,\n                message: format!(\"Input exceeds {} characters\", self.max_length),\n                position: None,\n            });\n        }\n\n        // Check PII\n        for (pattern, pii_type) in &self.pii_patterns {\n            for mat in pattern.find_iter(input) {\n                violations.push(Violation {\n                    violation_type: ViolationType::PII(pii_type.clone()),\n                    message: format!(\"Detected {:?}\", pii_type),\n                    position: Some((mat.start(), mat.end())),\n                });\n\n                // Sanitize\n                sanitized.replace_range(mat.range(), &\"[REDACTED]\");\n            }\n        }\n\n        // Check blocked keywords\n        for keyword in &self.blocked_keywords {\n            if input.to_lowercase().contains(keyword) {\n                violations.push(Violation {\n                    violation_type: ViolationType::BlockedKeyword,\n                    message: format!(\"Blocked keyword detected: {}\", keyword),\n                    position: None,\n                });\n            }\n        }\n\n        // Check for prompt injection patterns\n        let injection_patterns = [\n            r\"ignore (all |any )?previous (instruction|prompt)\",\n            r\"disregard (all |any )?previous\",\n            r\"you are (now |no longer )?\",\n            r\"new (instruction|prompt):\",\n        ];\n\n        for pattern in &injection_patterns {\n            let regex = Regex::new(pattern).unwrap();\n            if regex.is_match(&input.to_lowercase()) {\n                violations.push(Violation {\n                    violation_type: ViolationType::PromptInjection,\n                    message: \"Potential prompt injection detected\".to_string(),\n                    position: None,\n                });\n            }\n        }\n\n        ValidationResult {\n            is_valid: violations.is_empty(),\n            violations,\n            sanitized_input: sanitized,\n        }\n    }\n}\n```\n\n## Output Moderator\n\n```rust\npub struct OutputModerator {\n    toxicity_threshold: f32,\n    blocked_topics: Vec<String>,\n}\n\nimpl OutputModerator {\n    pub fn new() -> Self {\n        Self {\n            toxicity_threshold: 0.7,\n            blocked_topics: vec![\n                \"self-harm\".to_string(),\n                \"violence\".to_string(),\n                \"hate speech\".to_string(),\n            ],\n        }\n    }\n\n    pub async fn moderate(&self, output: &str) -> ModerationResult {\n        // Use external moderation API or local model\n        let moderation = self.call_moderation_api(output).await;\n\n        match moderation {\n            Ok(scores) => {\n                let flagged = scores.toxicity > self.toxicity_threshold\n                    || scores.self_harm > self.toxicity_threshold\n                    || scores.violence > self.toxicity_threshold;\n\n                ModerationResult {\n                    approved: !flagged,\n                    flagged_categories: self.get_flagged_categories(&scores),\n                    scores,\n                }\n            }\n            Err(e) => {\n                // Fail closed\n                ModerationResult {\n                    approved: false,\n                    flagged_categories: vec![\"moderation_error\".to_string()],\n                    scores: ModerationScores::default(),\n                }\n            }\n        }\n    }\n\n    async fn call_moderation_api(&self, text: &str) -> Result<ModerationScores, String> {\n        // Call OpenAI moderation or similar\n        // For local: use a small toxicity classifier\n        Ok(ModerationScores {\n            toxicity: 0.1,\n            self_harm: 0.0,\n            violence: 0.0,\n            hate: 0.0,\n        })\n    }\n\n    fn get_flagged_categories(&self, scores: &ModerationScores) -> Vec<String> {\n        let mut flagged = vec![];\n        if scores.toxicity > self.toxicity_threshold {\n            flagged.push(\"toxicity\".to_string());\n        }\n        if scores.self_harm > self.toxicity_threshold {\n            flagged.push(\"self-harm\".to_string());\n        }\n        if scores.violence > self.toxicity_threshold {\n            flagged.push(\"violence\".to_string());\n        }\n        flagged\n    }\n}\n\n#[derive(Debug, Default)]\npub struct ModerationScores {\n    pub toxicity: f32,\n    pub self_harm: f32,\n    pub violence: f32,\n    pub hate: f32,\n}\n\n#[derive(Debug)]\npub struct ModerationResult {\n    pub approved: bool,\n    pub flagged_categories: Vec<String>,\n    pub scores: ModerationScores,\n}\n```\n\n## Guardrails Pipeline\n\n```rust\npub struct Guardrails {\n    input_validator: InputValidator,\n    output_moderator: OutputModerator,\n    rate_limiter: RateLimiter,\n}\n\nimpl Guardrails {\n    pub fn new() -> Self {\n        Self {\n            input_validator: InputValidator::new(),\n            output_moderator: OutputModerator::new(),\n            rate_limiter: RateLimiter::new(100, Duration::from_minutes(1)),\n        }\n    }\n\n    pub async fn process_input(&self, user_id: &str, input: &str) -> Result<String, GuardrailError> {\n        // Rate limiting\n        if !self.rate_limiter.allow(user_id).await {\n            return Err(GuardrailError::RateLimited);\n        }\n\n        // Input validation\n        let validation = self.input_validator.validate(input);\n        if !validation.is_valid {\n            return Err(GuardrailError::InvalidInput(validation.violations));\n        }\n\n        Ok(validation.sanitized_input)\n    }\n\n    pub async fn process_output(&self, output: &str) -> Result<String, GuardrailError> {\n        let moderation = self.output_moderator.moderate(output).await;\n\n        if !moderation.approved {\n            return Err(GuardrailError::ContentFlagged(moderation.flagged_categories));\n        }\n\n        Ok(output.to_string())\n    }\n}\n\n#[derive(Debug)]\npub enum GuardrailError {\n    RateLimited,\n    InvalidInput(Vec<Violation>),\n    ContentFlagged(Vec<String>),\n}\n```",
      "terminal_task": {
        "command": "cargo test guardrails -- --nocapture",
        "expected_output": "test guardrails::tests::test_pii_detection ... ok",
        "hint": "Utilisez regex pour la détection PII, un rate limiter pour la protection, et modération pour le contenu de sortie"
      }
    },
    {
      "id": "cyber-001-sandboxing",
      "track": "CYBER_OPS",
      "level": "Expert",
      "title": "Sandboxing: Isolation de Code Non Fiable",
      "duration": "8h00",
      "description": "Techniques de sandboxing, seccomp, namespaces, et isolation de processus.",
      "markdown_content": "# Sandboxing: Isolation de Code\n\n## Architecture de Sandboxing\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    SANDBOX ARCHITECTURE                     │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  ┌─────────────┐    ┌─────────────┐    ┌─────────────────┐ │\n│  │   Parent    │───▶│  Namespace  │───▶│   Child         │ │\n│  │  Process    │    │  (isolated) │    │   (untrusted)   │ │\n│  └─────────────┘    └──────┬──────┘    └─────────────────┘ │\n│                            │                                │\n│                            ▼                                │\n│                   ┌─────────────────┐                       │\n│                   │  seccomp-bpf    │                       │\n│                   │  (syscall filt) │                       │\n│                   └─────────────────┘                       │\n│                                                             │\n│  Restrictions:                                              │\n│  - Network: deny all                                        │\n│  - Filesystem: read-only, tmpfs only                        │\n│  - Syscalls: whitelist only                                 │\n│  - Resources: CPU/memory limits                             │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Sandbox en Rust (Linux)\n\n```rust\nuse nix::sched::{clone, unshare, CloneFlags};\nuse nix::sys::wait::waitpid;\nuse nix::unistd::{chroot, fork, ForkResult, Pid};\nuse std::fs;\nuse std::os::unix::process::CommandExt;\nuse std::process::Command;\n\npub struct SandboxConfig {\n    pub root_dir: String,\n    pub memory_limit_mb: usize,\n    pub cpu_limit_percent: usize,\n    pub timeout_seconds: u64,\n    pub allow_network: bool,\n    pub allowed_syscalls: Vec<String>,\n}\n\nimpl Default for SandboxConfig {\n    fn default() -> Self {\n        Self {\n            root_dir: \"/tmp/sandbox\".to_string(),\n            memory_limit_mb: 64,\n            cpu_limit_percent: 50,\n            timeout_seconds: 5,\n            allow_network: false,\n            allowed_syscalls: vec![\n                \"read\".to_string(),\n                \"write\".to_string(),\n                \"exit\".to_string(),\n                \"exit_group\".to_string(),\n            ],\n        }\n    }\n}\n\npub struct Sandbox {\n    config: SandboxConfig,\n}\n\nimpl Sandbox {\n    pub fn new(config: SandboxConfig) -> Self {\n        Self { config }\n    }\n\n    pub fn setup(&self) -> Result<(), String> {\n        // Create sandbox root\n        fs::create_dir_all(&self.config.root_dir)\n            .map_err(|e| format!(\"Failed to create sandbox dir: {}\", e))?;\n\n        // Create minimal filesystem\n        self.create_minimal_fs()?;\n\n        Ok(())\n    }\n\n    fn create_minimal_fs(&self) -> Result<(), String> {\n        let dirs = [\"bin\", \"lib\", \"lib64\", \"tmp\", \"dev\", \"proc\"];\n\n        for dir in &dirs {\n            let path = format!(\"{}/{}\", self.config.root_dir, dir);\n            fs::create_dir_all(&path)\n                .map_err(|e| format!(\"Failed to create {}: {}\", dir, e))?;\n        }\n\n        // Copy essential binaries\n        self.copy_binary(\"/bin/sh\", \"bin/sh\")?;\n        self.copy_binary(\"/bin/ls\", \"bin/ls\")?;\n\n        // Copy libraries\n        self.copy_libs(\"/bin/sh\")?;\n\n        // Create device nodes\n        self.create_dev_nodes()?;\n\n        Ok(())\n    }\n\n    fn copy_binary(&self, src: &str, dest: &str) -> Result<(), String> {\n        let dest_path = format!(\"{}/{}\", self.config.root_dir, dest);\n        fs::copy(src, &dest_path)\n            .map_err(|e| format!(\"Failed to copy {}: {}\", src, e))?;\n        Ok(())\n    }\n\n    fn copy_libs(&self, binary: &str) -> Result<(), String> {\n        // Use ldd to find dependencies\n        let output = Command::new(\"ldd\")\n            .arg(binary)\n            .output()\n            .map_err(|e| e.to_string())?;\n\n        let libs = String::from_utf8_lossy(&output.stdout);\n\n        for line in libs.lines() {\n            if let Some(lib_path) = line.split_whitespace().nth(2) {\n                if lib_path.starts_with('/') {\n                    let lib_name = lib_path.split('/').last().unwrap();\n                    let dest = format!(\"{}/lib/{}\", self.config.root_dir, lib_name);\n                    let _ = fs::copy(lib_path, dest);\n                }\n            }\n        }\n\n        Ok(())\n    }\n\n    fn create_dev_nodes(&self) -> Result<(), String> {\n        // Create minimal devices\n        let dev_path = format!(\"{}/dev\", self.config.root_dir);\n\n        // /dev/null\n        Command::new(\"mknod\")\n            .args(&[\"-m\", \"666\", &format!(\"{}/null\", dev_path), \"c\", \"1\", \"3\"])\n            .status()\n            .map_err(|e| e.to_string())?;\n\n        // /dev/zero\n        Command::new(\"mknod\")\n            .args(&[\"-m\", \"666\", &format!(\"{}/zero\", dev_path), \"c\", \"1\", \"5\"])\n            .status()\n            .map_err(|e| e.to_string())?;\n\n        // /dev/random\n        Command::new(\"mknod\")\n            .args(&[\"-m\", \"666\", &format!(\"{}/random\", dev_path), \"c\", \"1\", \"8\"])\n            .status()\n            .map_err(|e| e.to_string())?;\n\n        Ok(())\n    }\n\n    pub fn run<F, T>(&self, f: F) -> Result<T, String>\n    where\n        F: FnOnce() -> T + Send + 'static,\n        T: Send + 'static,\n    {\n        match unsafe { fork() } {\n            Ok(ForkResult::Child) => {\n                // Child process - apply restrictions\n                self.apply_restrictions()?;\n\n                // Execute function\n                let result = f();\n                std::process::exit(0);\n            }\n            Ok(ForkResult::Parent { child }) => {\n                // Parent - wait for child\n                match waitpid(child, None) {\n                    Ok(_) => Ok(unsafe { std::mem::zeroed() }), // Placeholder\n                    Err(e) => Err(format!(\"Wait failed: {}\", e)),\n                }\n            }\n            Err(e) => Err(format!(\"Fork failed: {}\", e)),\n        }\n    }\n\n    fn apply_restrictions(&self) -> Result<(), String> {\n        // Unshare namespaces\n        unshare(\n            CloneFlags::CLONE_NEWNS |\n            CloneFlags::CLONE_NEWPID |\n            CloneFlags::CLONE_NEWNET |\n            CloneFlags::CLONE_NEWIPC |\n            CloneFlags::CLONE_NEWUTS |\n            CloneFlags::CLONE_NEWUSER\n        ).map_err(|e| format!(\"Unshare failed: {}\", e))?;\n\n        // Chroot\n        chroot(&self.config.root_dir)\n            .map_err(|e| format!(\"Chroot failed: {}\", e))?;\n\n        std::env::set_current_dir(\"/\")\n            .map_err(|e| format!(\"Chdir failed: {}\", e))?;\n\n        // Apply seccomp\n        self.apply_seccomp()?;\n\n        // Set resource limits\n        self.set_rlimits()?;\n\n        Ok(())\n    }\n\n    fn apply_seccomp(&self) -> Result<(), String> {\n        // Use libseccomp or write BPF directly\n        // This is a simplified version\n\n        #[cfg(target_os = \"linux\")]\n        {\n            use seccomp::*;\n\n            let mut ctx = Context::new(Action::Allow)\n                .map_err(|e| e.to_string())?;\n\n            // Deny dangerous syscalls\n            let denied = vec![\n                ScmpSyscall::from_name(\"execve\"),\n                ScmpSyscall::from_name(\"fork\"),\n                ScmpSyscall::from_name(\"clone\"),\n                ScmpSyscall::from_name(\"ptrace\"),\n                ScmpSyscall::from_name(\"mount\"),\n                ScmpSyscall::from_name(\"umount2\"),\n            ];\n\n            for syscall in denied {\n                ctx.add_rule(ScmpAction::Errno(1), syscall)\n                    .map_err(|e| e.to_string())?;\n            }\n\n            ctx.load().map_err(|e| e.to_string())?;\n        }\n\n        Ok(())\n    }\n\n    fn set_rlimits(&self) -> Result<(), String> {\n        use nix::sys::resource::{setrlimit, Resource};\n\n        // Memory limit\n        let mem_limit = (self.config.memory_limit_mb * 1024 * 1024) as u64;\n        setrlimit(Resource::RLIMIT_AS, mem_limit, mem_limit)\n            .map_err(|e| format!(\"Failed to set memory limit: {}\", e))?;\n\n        // CPU time limit\n        let cpu_limit = self.config.timeout_seconds;\n        setrlimit(Resource::RLIMIT_CPU, cpu_limit, cpu_limit)\n            .map_err(|e| format!(\"Failed to set CPU limit: {}\", e))?;\n\n        // File size limit\n        setrlimit(Resource::RLIMIT_FSIZE, 1024 * 1024, 1024 * 1024)\n            .map_err(|e| format!(\"Failed to set file size limit: {}\", e))?;\n\n        // No core dumps\n        setrlimit(Resource::RLIMIT_CORE, 0, 0)\n            .map_err(|e| format!(\"Failed to disable core dumps: {}\", e))?;\n\n        Ok(())\n    }\n}\n```",
      "terminal_task": {
        "command": "cargo test sandbox -- --nocapture",
        "expected_output": "test sandbox::tests::test_isolated_execution ... ok",
        "hint": "Utilisez namespaces Linux pour l'isolation, seccomp-bpf pour le filtrage syscalls, et rlimits pour les ressources"
      }
    },
    {
      "id": "cyber-002-csp-hardening",
      "track": "CYBER_OPS",
      "level": "Expert",
      "title": "CSP Hardening et Sécurité Frontend",
      "duration": "6h00",
      "description": "Content Security Policy avancée, subresource integrity, et protection XSS.",
      "markdown_content": "# CSP Hardening\n\n## Architecture CSP\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    CSP DIRECTIVES                           │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  default-src 'self'                                         │\n│  script-src 'self' 'nonce-{random}'                         │\n│  style-src 'self' 'unsafe-inline'                           │\n│  img-src 'self' data: https:                                │\n│  connect-src 'self' https://api.example.com                 │\n│  font-src 'self'                                            │\n│  frame-src 'none'                                           │\n│  object-src 'none'                                          │\n│  base-uri 'self'                                            │\n│  form-action 'self'                                         │\n│  upgrade-insecure-requests                                  │\n│                                                             │\n│  report-uri /csp-report                                     │\n│  report-to csp-endpoint                                     │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## CSP Generator\n\n```rust\nuse std::collections::HashMap;\n\n#[derive(Debug, Clone, Default)]\npub struct CSPPolicy {\n    directives: HashMap<String, Vec<String>>,\n}\n\nimpl CSPPolicy {\n    pub fn new() -> Self {\n        Self::default()\n    }\n\n    pub fn default_src(mut self, sources: &[&str]) -> Self {\n        self.directives.insert(\n            \"default-src\".to_string(),\n            sources.iter().map(|s| s.to_string()).collect(),\n        );\n        self\n    }\n\n    pub fn script_src(mut self, sources: &[&str]) -> Self {\n        self.directives.insert(\n            \"script-src\".to_string(),\n            sources.iter().map(|s| s.to_string()).collect(),\n        );\n        self\n    }\n\n    pub fn style_src(mut self, sources: &[&str]) -> Self {\n        self.directives.insert(\n            \"style-src\".to_string(),\n            sources.iter().map(|s| s.to_string()).collect(),\n        );\n        self\n    }\n\n    pub fn img_src(mut self, sources: &[&str]) -> Self {\n        self.directives.insert(\n            \"img-src\".to_string(),\n            sources.iter().map(|s| s.to_string()).collect(),\n        );\n        self\n    }\n\n    pub fn connect_src(mut self, sources: &[&str]) -> Self {\n        self.directives.insert(\n            \"connect-src\".to_string(),\n            sources.iter().map(|s| s.to_string()).collect(),\n        );\n        self\n    }\n\n    pub fn frame_src(mut self, sources: &[&str]) -> Self {\n        self.directives.insert(\n            \"frame-src\".to_string(),\n            sources.iter().map(|s| s.to_string()).collect(),\n        );\n        self\n    }\n\n    pub fn object_src(mut self, sources: &[&str]) -> Self {\n        self.directives.insert(\n            \"object-src\".to_string(),\n            sources.iter().map(|s| s.to_string()).collect(),\n        );\n        self\n    }\n\n    pub fn upgrade_insecure_requests(mut self) -> Self {\n        self.directives.insert(\n            \"upgrade-insecure-requests\".to_string(),\n            vec![],\n        );\n        self\n    }\n\n    pub fn report_uri(mut self, uri: &str) -> Self {\n        self.directives.insert(\n            \"report-uri\".to_string(),\n            vec![uri.to_string()],\n        );\n        self\n    }\n\n    pub fn build(&self) -> String {\n        let mut parts = vec![];\n\n        for (directive, sources) in &self.directives {\n            if sources.is_empty() {\n                parts.push(directive.clone());\n            } else {\n                parts.push(format!(\"{} {}\", directive, sources.join(\" \")));\n            }\n        }\n\n        parts.join(\"; \")\n    }\n\n    // Pre-configured policies\n    pub fn strict() -> Self {\n        Self::new()\n            .default_src(&[\"'self'\"])\n            .script_src(&[\"'self'\", \"'nonce-{nonce}'\"])\n            .style_src(&[\"'self'\", \"'unsafe-inline'\"])\n            .img_src(&[\"'self'\", \"data:\", \"https:\"])\n            .connect_src(&[\"'self'\"])\n            .font_src(&[\"'self'\"])\n            .frame_src(&[\"'none'\"])\n            .object_src(&[\"'none'\"])\n            .upgrade_insecure_requests()\n    }\n\n    pub fn report_only(&self) -> String {\n        format!(\"Content-Security-Policy-Report-Only: {}\", self.build())\n    }\n\n    pub fn enforce(&self) -> String {\n        format!(\"Content-Security-Policy: {}\", self.build())\n    }\n}\n\n// Nonce generator\npub struct NonceGenerator;\n\nimpl NonceGenerator {\n    pub fn generate() -> String {\n        use rand::Rng;\n        let mut rng = rand::thread_rng();\n        let nonce: Vec<u8> = (0..16).map(|_| rng.gen()).collect();\n        base64::encode(&nonce)\n    }\n}\n\n// HTML generator with nonces\npub struct SecureHTML {\n    nonce: String,\n    csp: CSPPolicy,\n}\n\nimpl SecureHTML {\n    pub fn new() -> Self {\n        Self {\n            nonce: NonceGenerator::generate(),\n            csp: CSPPolicy::strict(),\n        }\n    }\n\n    pub fn script(&self, content: &str) -> String {\n        format!(\n            r#\"<script nonce=\"{}\">{}</script>\"#,\n            self.nonce, content\n        )\n    }\n\n    pub fn external_script(&self, src: &str, integrity: &str) -> String {\n        format!(\n            r#\"<script src=\"{}\" nonce=\"{}\" integrity=\"{}\" crossorigin=\"anonymous\"></script>\"#,\n            src, self.nonce, integrity\n        )\n    }\n\n    pub fn get_csp_header(&self) -> String {\n        let csp_with_nonce = self.csp.clone()\n            .script_src(&[&format!(\"'nonce-{}'\", self.nonce)]);\n        csp_with_nonce.enforce()\n    }\n}\n```\n\n## Subresource Integrity\n\n```rust\nuse sha2::{Sha256, Sha384, Digest};\n\npub struct IntegrityHash;\n\nimpl IntegrityHash {\n    pub fn sha256(content: &[u8]) -> String {\n        let mut hasher = Sha256::new();\n        hasher.update(content);\n        let result = hasher.finalize();\n        format!(\"sha256-{}\", base64::encode(&result))\n    }\n\n    pub fn sha384(content: &[u8]) -> String {\n        let mut hasher = Sha384::new();\n        hasher.update(content);\n        let result = hasher.finalize();\n        format!(\"sha384-{}\", base64::encode(&result))\n    }\n\n    pub fn generate_for_file(path: &str) -> Result<String, std::io::Error> {\n        let content = std::fs::read(path)?;\n        Ok(Self::sha384(&content))\n    }\n\n    pub fn verify(content: &[u8], integrity: &str) -> bool {\n        let parts: Vec<&str> = integrity.split('-').collect();\n        if parts.len() != 2 {\n            return false;\n        }\n\n        let algorithm = parts[0];\n        let expected_hash = parts[1];\n\n        let actual_hash = match algorithm {\n            \"sha256\" => base64::encode(Sha256::digest(content)),\n            \"sha384\" => base64::encode(Sha384::digest(content)),\n            _ => return false,\n        };\n\n        actual_hash == expected_hash\n    }\n}\n\n// Build-time integrity generation\npub fn generate_build_integrity() -> Result<serde_json::Value, std::io::Error> {\n    let mut integrity_map = serde_json::Map::new();\n\n    for entry in std::fs::read_dir(\"dist\")? {\n        let entry = entry?;\n        let path = entry.path();\n\n        if path.extension().map(|e| e == \"js\" || e == \"css\").unwrap_or(false) {\n            let hash = IntegrityHash::generate_for_file(path.to_str().unwrap())?;\n            let filename = path.file_name().unwrap().to_str().unwrap();\n            integrity_map.insert(filename.to_string(), serde_json::Value::String(hash));\n        }\n    }\n\n    Ok(serde_json::Value::Object(integrity_map))\n}\n```",
      "terminal_task": {
        "command": "cargo test csp -- --nocapture",
        "expected_output": "test csp::tests::test_csp_generation ... ok",
        "hint": "Utilisez nonces pour les scripts inline, SRI pour les ressources externes, et CSP strict pour la protection XSS"
      }
    },
    {
      "id": "cyber-003-crypto-rust",
      "track": "CYBER_OPS",
      "level": "Expert",
      "title": "Cryptographie en Rust: Implémentations Sécurisées",
      "duration": "9h00",
      "description": "Chiffrement, signatures, hashing, et gestion sécurisée des secrets.",
      "markdown_content": "# Cryptographie en Rust\n\n## Architecture Cryptographique\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    CRYPTOGRAPHY STACK                       │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  ┌─────────────┐    ┌─────────────┐    ┌─────────────────┐ │\n│  │   AEAD      │    │   Signatures│    │   KDF / HKDF    │ │\n│  │  (AES-GCM)  │    │  (Ed25519)  │    │  (Argon2)       │ │\n│  └─────────────┘    └─────────────┘    └─────────────────┘ │\n│                                                             │\n│  ┌─────────────┐    ┌─────────────┐    ┌─────────────────┐ │\n│  │   Hashing   │    │   RNG       │    │   Key Exchange  │ │\n│  │  (BLAKE3)   │    │  (OS)       │    │  (X25519)       │ │\n│  └─────────────┘    └─────────────┘    └─────────────────┘ │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Chiffrement AEAD\n\n```rust\nuse aes_gcm::{\n    aead::{Aead, KeyInit},\n    Aes256Gcm, Nonce,\n};\nuse rand::RngCore;\n\npub struct EncryptedData {\n    pub ciphertext: Vec<u8>,\n    pub nonce: [u8; 12],\n}\n\npub struct CryptoProvider;\n\nimpl CryptoProvider {\n    pub fn encrypt(plaintext: &[u8], key: &[u8; 32]) -> EncryptedData {\n        let cipher = Aes256Gcm::new_from_slice(key).unwrap();\n\n        // Generate random nonce\n        let mut nonce_bytes = [0u8; 12];\n        rand::thread_rng().fill_bytes(&mut nonce_bytes);\n        let nonce = Nonce::from_slice(&nonce_bytes);\n\n        let ciphertext = cipher.encrypt(nonce, plaintext)\n            .expect(\"encryption failure\");\n\n        EncryptedData {\n            ciphertext,\n            nonce: nonce_bytes,\n        }\n    }\n\n    pub fn decrypt(encrypted: &EncryptedData, key: &[u8; 32]) -> Result<Vec<u8>, String> {\n        let cipher = Aes256Gcm::new_from_slice(key).unwrap();\n        let nonce = Nonce::from_slice(&encrypted.nonce);\n\n        cipher.decrypt(nonce, encrypted.ciphertext.as_ref())\n            .map_err(|e| format!(\"Decryption failed: {:?}\", e))\n    }\n}\n\n// Key derivation\nuse argon2::{Argon2, PasswordHash, PasswordHasher, PasswordVerifier};\nuse argon2::password_hash::SaltString;\n\npub struct PasswordCrypto;\n\nimpl PasswordCrypto {\n    pub fn hash_password(password: &str) -> Result<String, String> {\n        let salt = SaltString::generate(&mut rand::thread_rng());\n        let argon2 = Argon2::default();\n\n        let password_hash = argon2\n            .hash_password(password.as_bytes(), &salt)\n            .map_err(|e| e.to_string())?;\n\n        Ok(password_hash.to_string())\n    }\n\n    pub fn verify_password(password: &str, hash: &str) -> Result<bool, String> {\n        let parsed_hash = PasswordHash::new(hash)\n            .map_err(|e| e.to_string())?;\n\n        let argon2 = Argon2::default();\n\n        Ok(argon2.verify_password(password.as_bytes(), &parsed_hash).is_ok())\n    }\n\n    pub fn derive_key(password: &str, salt: &[u8]) -> [u8; 32] {\n        let argon2 = Argon2::default();\n        let mut key = [0u8; 32];\n\n        argon2.hash_password_into(\n            password.as_bytes(),\n            salt,\n            &mut key,\n        ).expect(\"Key derivation failed\");\n\n        key\n    }\n}\n```\n\n## Signatures Digitales\n\n```rust\nuse ed25519_dalek::{Signer, SigningKey, VerifyingKey, Signature};\nuse rand::rngs::OsRng;\n\npub struct KeyPair {\n    pub signing_key: SigningKey,\n    pub verifying_key: VerifyingKey,\n}\n\nimpl KeyPair {\n    pub fn generate() -> Self {\n        let mut csprng = OsRng;\n        let signing_key = SigningKey::generate(&mut csprng);\n        let verifying_key = signing_key.verifying_key();\n\n        Self {\n            signing_key,\n            verifying_key,\n        }\n    }\n\n    pub fn sign(&self, message: &[u8]) -> Signature {\n        self.signing_key.sign(message)\n    }\n\n    pub fn verify(&self, message: &[u8], signature: &Signature) -> bool {\n        self.verifying_key.verify(message, signature).is_ok()\n    }\n\n    pub fn to_bytes(&self) -> ([u8; 32], [u8; 32]) {\n        (\n            self.signing_key.to_bytes(),\n            self.verifying_key.to_bytes(),\n        )\n    }\n\n    pub fn from_bytes(signing: &[u8; 32], verifying: &[u8; 32]) -> Result<Self, String> {\n        let signing_key = SigningKey::from_bytes(signing);\n        let verifying_key = VerifyingKey::from_bytes(verifying)\n            .map_err(|e| e.to_string())?;\n\n        Ok(Self {\n            signing_key,\n            verifying_key,\n        })\n    }\n}\n\n// Secure secret storage\npub struct SecretVault {\n    master_key: [u8; 32],\n}\n\nimpl SecretVault {\n    pub fn new(master_password: &str) -> Self {\n        let salt = b\"fixed_salt_should_be_random_in_production\";\n        let key = PasswordCrypto::derive_key(master_password, salt);\n\n        Self { master_key: key }\n    }\n\n    pub fn seal(&self, secret: &str) -> EncryptedData {\n        CryptoProvider::encrypt(secret.as_bytes(), &self.master_key)\n    }\n\n    pub fn unseal(&self, encrypted: &EncryptedData) -> Result<String, String> {\n        let plaintext = CryptoProvider::decrypt(encrypted, &self.master_key)?;\n        String::from_utf8(plaintext)\n            .map_err(|e| e.to_string())\n    }\n}\n```\n\n## Hashing et HMAC\n\n```rust\nuse blake3::Hasher;\nuse hmac::{Hmac, Mac};\nuse sha2::Sha256;\n\ntype HmacSha256 = Hmac<Sha256>;\n\npub struct HashProvider;\n\nimpl HashProvider {\n    pub fn blake3(data: &[u8]) -> String {\n        let hash = blake3::hash(data);\n        hash.to_hex().to_string()\n    }\n\n    pub fn blake3_file(path: &str) -> Result<String, std::io::Error> {\n        let mut hasher = Hasher::new();\n        let mut file = std::fs::File::open(path)?;\n        std::io::copy(&mut file, &mut hasher)?;\n        Ok(hasher.finalize().to_hex().to_string())\n    }\n\n    pub fn hmac(key: &[u8], data: &[u8]) -> Vec<u8> {\n        let mut mac = HmacSha256::new_from_slice(key)\n            .expect(\"HMAC can take key of any size\");\n        mac.update(data);\n        mac.finalize().into_bytes().to_vec()\n    }\n\n    pub fn verify_hmac(key: &[u8], data: &[u8], tag: &[u8]) -> bool {\n        let mut mac = HmacSha256::new_from_slice(key)\n            .expect(\"HMAC can take key of any size\");\n        mac.update(data);\n        mac.verify_slice(tag).is_ok()\n    }\n}\n\n// Constant-time comparison\npub fn constant_time_eq(a: &[u8], b: &[u8]) -> bool {\n    if a.len() != b.len() {\n        return false;\n    }\n\n    let mut result = 0u8;\n    for (x, y) in a.iter().zip(b.iter()) {\n        result |= x ^ y;\n    }\n\n    result == 0\n}\n```",
      "terminal_task": {
        "command": "cargo test crypto -- --nocapture",
        "expected_output": "test crypto::tests::test_encrypt_decrypt ... ok",
        "hint": "Utilisez AES-256-GCM pour le chiffrement, Argon2 pour le KDF, Ed25519 pour les signatures, et constant_time_eq pour la comparaison"
      }
    },
    {
      "id": "cyber-004-secure-communication",
      "track": "CYBER_OPS",
      "level": "Expert",
      "title": "Communication Sécurisée: TLS et mTLS",
      "duration": "7h00",
      "description": "Configuration TLS, certificate pinning, et mutual TLS.",
      "markdown_content": "# Communication Sécurisée\n\n## Architecture TLS/mTLS\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    TLS HANDSHAKE                            │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  Client                              Server                 │\n│    │                                   │                    │\n│    │ ───── ClientHello ─────────────▶ │                    │\n│    │                                   │                    │\n│    │ ◀──── ServerHello ────────────── │                    │\n│    │ ◀──── Certificate  ───────────── │                    │\n│    │ ◀──── [CertificateRequest] ───── │  (mTLS)            │\n│    │ ◀──── ServerHelloDone ────────── │                    │\n│    │                                   │                    │\n│    │ ───── [Certificate] ──────────▶ │  (mTLS)            │\n│    │ ───── ClientKeyExchange ──────▶ │                    │\n│    │ ───── [CertificateVerify] ────▶ │  (mTLS)            │\n│    │ ───── ChangeCipherSpec ───────▶ │                    │\n│    │ ───── Finished ───────────────▶ │                    │\n│    │                                   │                    │\n│    │ ◀──── ChangeCipherSpec ───────── │                    │\n│    │ ◀──── Finished ───────────────── │                    │\n│    │                                   │                    │\n│    │ ◀───▶ Encrypted Application Data │                    │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Client TLS en Rust\n\n```rust\nuse rustls::{ClientConfig, RootCertStore, ServerName};\nuse std::sync::Arc;\nuse tokio_rustls::TlsConnector;\n\npub struct SecureClient {\n    config: Arc<ClientConfig>,\n}\n\nimpl SecureClient {\n    pub fn new() -> Result<Self, String> {\n        let mut root_store = RootCertStore::empty();\n\n        // Add system certificates\n        root_store.add_trust_anchors(\n            webpki_roots::TLS_SERVER_ROOTS.iter()\n                .map(|ta| {\n                    rustls::OwnedTrustAnchor::from_subject_spki_name_constraints(\n                        ta.subject,\n                        ta.spki,\n                        ta.name_constraints,\n                    )\n                })\n        );\n\n        let config = ClientConfig::builder()\n            .with_safe_defaults()\n            .with_root_certificates(root_store)\n            .with_no_client_auth();\n\n        Ok(Self {\n            config: Arc::new(config),\n        })\n    }\n\n    pub fn with_custom_ca(ca_cert_path: &str) -> Result<Self, String> {\n        let ca_cert = std::fs::read(ca_cert_path)\n            .map_err(|e| e.to_string())?;\n\n        let ca_cert = rustls_pemfile::certs(&mut ca_cert.as_slice())\n            .next()\n            .ok_or(\"No certificate found\")?\n            .map_err(|e| e.to_string())?;\n\n        let mut root_store = RootCertStore::empty();\n        root_store.add(&ca_cert).map_err(|e| e.to_string())?;\n\n        let config = ClientConfig::builder()\n            .with_safe_defaults()\n            .with_root_certificates(root_store)\n            .with_no_client_auth();\n\n        Ok(Self {\n            config: Arc::new(config),\n        })\n    }\n\n    pub fn with_certificate_pinning(\n        expected_fingerprint: &str,\n    ) -> Result<Self, String> {\n        let mut root_store = RootCertStore::empty();\n        root_store.add_trust_anchors(\n            webpki_roots::TLS_SERVER_ROOTS.iter()\n                .map(|ta| {\n                    rustls::OwnedTrustAnchor::from_subject_spki_name_constraints(\n                        ta.subject,\n                        ta.spki,\n                        ta.name_constraints,\n                    )\n                })\n        );\n\n        let verifier = PinningCertVerifier::new(expected_fingerprint)?;\n\n        let config = ClientConfig::builder()\n            .with_safe_defaults()\n            .with_custom_certificate_verifier(Arc::new(verifier))\n            .with_no_client_auth();\n\n        Ok(Self {\n            config: Arc::new(config),\n        })\n    }\n\n    pub async fn connect(&self, host: &str, port: u16) -> Result<tokio_rustls::client::TlsStream<tokio::net::TcpStream>, String> {\n        let connector = TlsConnector::from(self.config.clone());\n        let server_name = ServerName::try_from(host)\n            .map_err(|e| e.to_string())?;\n\n        let stream = tokio::net::TcpStream::connect((host, port))\n            .await\n            .map_err(|e| e.to_string())?;\n\n        connector.connect(server_name, stream)\n            .await\n            .map_err(|e| e.to_string())\n    }\n}\n\n// Certificate pinning verifier\nstruct PinningCertVerifier {\n    expected_fingerprint: Vec<u8>,\n}\n\nimpl PinningCertVerifier {\n    fn new(fingerprint: &str) -> Result<Self, String> {\n        let fingerprint = hex::decode(fingerprint)\n            .map_err(|e| e.to_string())?;\n\n        Ok(Self {\n            expected_fingerprint: fingerprint,\n        })\n    }\n\n    fn cert_fingerprint(cert: &rustls::Certificate) -> Vec<u8> {\n        use sha2::{Sha256, Digest};\n        let mut hasher = Sha256::new();\n        hasher.update(&cert.0);\n        hasher.finalize().to_vec()\n    }\n}\n\nimpl rustls::client::ServerCertVerifier for PinningCertVerifier {\n    fn verify_server_cert(\n        &self,\n        end_entity: &rustls::Certificate,\n        _intermediates: &[rustls::Certificate],\n        _server_name: &ServerName,\n        _scts: &mut dyn Iterator<Item = &[u8]>,\n        _ocsp_response: &[u8],\n        _now: std::time::SystemTime,\n    ) -> Result<rustls::client::ServerCertVerified, rustls::Error> {\n        let fingerprint = Self::cert_fingerprint(end_entity);\n\n        if fingerprint == self.expected_fingerprint {\n            Ok(rustls::client::ServerCertVerified::assertion())\n        } else {\n            Err(rustls::Error::General(\"Certificate pinning failed\".to_string()))\n        }\n    }\n}\n```\n\n## mTLS (Mutual TLS)\n\n```rust\npub struct MtlsClient {\n    config: Arc<ClientConfig>,\n}\n\nimpl MtlsClient {\n    pub fn new(\n        cert_path: &str,\n        key_path: &str,\n        ca_cert_path: &str,\n    ) -> Result<Self, String> {\n        // Load client certificate\n        let cert = std::fs::read(cert_path)\n            .map_err(|e| e.to_string())?;\n        let cert = rustls_pemfile::certs(&mut cert.as_slice())\n            .next()\n            .ok_or(\"No certificate found\")?\n            .map_err(|e| e.to_string())?;\n\n        // Load client key\n        let key = std::fs::read(key_path)\n            .map_err(|e| e.to_string())?;\n        let key = rustls_pemfile::pkcs8_private_keys(&mut key.as_slice())\n            .next()\n            .ok_or(\"No key found\")?\n            .map_err(|e| e.to_string())?;\n\n        let key = rustls::PrivateKey(key.secret_pkcs8_der().to_vec());\n\n        // Load CA certificate\n        let ca_cert = std::fs::read(ca_cert_path)\n            .map_err(|e| e.to_string())?;\n        let ca_cert = rustls_pemfile::certs(&mut ca_cert.as_slice())\n            .next()\n            .ok_or(\"No CA certificate found\")?\n            .map_err(|e| e.to_string())?;\n\n        let mut root_store = RootCertStore::empty();\n        root_store.add(&ca_cert).map_err(|e| e.to_string())?;\n\n        let config = ClientConfig::builder()\n            .with_safe_defaults()\n            .with_root_certificates(root_store)\n            .with_client_auth_cert(vec![cert], key)\n            .map_err(|e| e.to_string())?;\n\n        Ok(Self {\n            config: Arc::new(config),\n        })\n    }\n}\n\n// Server mTLS\npub struct MtlsServer {\n    config: Arc<rustls::ServerConfig>,\n}\n\nimpl MtlsServer {\n    pub fn new(\n        cert_path: &str,\n        key_path: &str,\n        ca_cert_path: &str,\n    ) -> Result<Self, String> {\n        // Load server certificate\n        let cert = std::fs::read(cert_path)\n            .map_err(|e| e.to_string())?;\n        let cert = rustls_pemfile::certs(&mut cert.as_slice())\n            .next()\n            .ok_or(\"No certificate found\")?\n            .map_err(|e| e.to_string())?;\n\n        // Load server key\n        let key = std::fs::read(key_path)\n            .map_err(|e| e.to_string())?;\n        let key = rustls_pemfile::pkcs8_private_keys(&mut key.as_slice())\n            .next()\n            .ok_or(\"No key found\")?\n            .map_err(|e| e.to_string())?;\n\n        let key = rustls::PrivateKey(key.secret_pkcs8_der().to_vec());\n\n        // Load CA for client verification\n        let ca_cert = std::fs::read(ca_cert_path)\n            .map_err(|e| e.to_string())?;\n        let ca_cert = rustls_pemfile::certs(&mut ca_cert.as_slice())\n            .next()\n            .ok_or(\"No CA certificate found\")?\n            .map_err(|e| e.to_string())?;\n\n        let mut client_auth_roots = RootCertStore::empty();\n        client_auth_roots.add(&ca_cert).map_err(|e| e.to_string())?;\n\n        let client_auth = rustls::server::AllowAnyAuthenticatedClient::new(\n            client_auth_roots,\n        );\n\n        let config = rustls::ServerConfig::builder()\n            .with_safe_defaults()\n            .with_client_cert_verifier(client_auth)\n            .with_single_cert(vec![cert], key)\n            .map_err(|e| e.to_string())?;\n\n        Ok(Self {\n            config: Arc::new(config),\n        })\n    }\n\n    pub async fn accept(&self, stream: tokio::net::TcpStream) -> Result<tokio_rustls::server::TlsStream<tokio::net::TcpStream>, String> {\n        let acceptor = tokio_rustls::TlsAcceptor::from(self.config.clone());\n        acceptor.accept(stream)\n            .await\n            .map_err(|e| e.to_string())\n    }\n}\n```",
      "terminal_task": {
        "command": "cargo test tls -- --nocapture",
        "expected_output": "test tls::tests::test_mtls_handshake ... ok",
        "hint": "Utilisez rustls pour TLS, certificate pinning pour la sécurité supplémentaire, et mTLS pour l'authentification bidirectionnelle"
      }
    },
    {
      "id": "cyber-005-vulnerability-scanning",
      "track": "CYBER_OPS",
      "level": "Expert",
      "title": "Scanning de Vulnérabilités",
      "duration": "6h30",
      "description": "Détection de vulnérabilités, audit de dépendances, et analyse statique.",
      "markdown_content": "# Scanning de Vulnérabilités\n\n## Architecture de Scanning\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    VULNERABILITY SCANNER                    │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  ┌─────────────┐    ┌─────────────┐    ┌─────────────────┐ │\n│  │  Dependency │───▶│    SAST     │───▶│   DAST          │ │\n│  │  Audit      │    │  (code)     │    │  (runtime)      │ │\n│  └─────────────┘    └─────────────┘    └─────────────────┘ │\n│                                                             │\n│  Sources:                                                   │\n│  - Cargo.lock / package-lock.json                           │\n│  - GitHub Security Advisories                               │\n│  - RustSec Advisory Database                                │\n│  - OWASP Dependency Check                                   │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Dependency Scanner\n\n```rust\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\n\n#[derive(Debug, Deserialize)]\npub struct CargoLock {\n    pub package: Vec<Package>,\n}\n\n#[derive(Debug, Deserialize, Clone)]\npub struct Package {\n    pub name: String,\n    pub version: String,\n    pub source: Option<String>,\n    pub checksum: Option<String>,\n}\n\n#[derive(Debug, Serialize)]\npub struct VulnerabilityReport {\n    pub package: String,\n    pub version: String,\n    pub vulnerabilities: Vec<Vulnerability>,\n}\n\n#[derive(Debug, Serialize)]\npub struct Vulnerability {\n    pub id: String,\n    pub title: String,\n    pub description: String,\n    pub severity: Severity,\n    pub patched_versions: Vec<String>,\n    pub references: Vec<String>,\n}\n\n#[derive(Debug, Serialize)]\npub enum Severity {\n    Low,\n    Medium,\n    High,\n    Critical,\n}\n\npub struct DependencyScanner {\n    advisory_db: AdvisoryDatabase,\n}\n\npub struct AdvisoryDatabase {\n    advisories: Vec<Vulnerability>,\n}\n\nimpl DependencyScanner {\n    pub async fn new() -> Result<Self, String> {\n        let db = Self::load_rustsec_db().await?;\n\n        Ok(Self {\n            advisory_db: AdvisoryDatabase { advisories: db },\n        })\n    }\n\n    async fn load_rustsec_db() -> Result<Vec<Vulnerability>, String> {\n        // Load from RustSec advisory-db\n        let url = \"https://raw.githubusercontent.com/RustSec/advisory-db/main/crates/\";\n\n        // In practice, clone or fetch the repo\n        // For now, return empty\n        Ok(vec![])\n    }\n\n    pub fn scan_cargo_lock(&self, lock_file: &str) -> Result<Vec<VulnerabilityReport>, String> {\n        let lock: CargoLock = toml::from_str(lock_file)\n            .map_err(|e| e.to_string())?;\n\n        let mut reports = vec![];\n\n        for package in &lock.package {\n            let vulns = self.check_package(&package.name, &package.version);\n\n            if !vulns.is_empty() {\n                reports.push(VulnerabilityReport {\n                    package: package.name.clone(),\n                    version: package.version.clone(),\n                    vulnerabilities: vulns,\n                });\n            }\n        }\n\n        Ok(reports)\n    }\n\n    fn check_package(&self, name: &str, version: &str) -> Vec<Vulnerability> {\n        let mut vulns = vec![];\n\n        for advisory in &self.advisory_db.advisories {\n            if advisory.id.contains(name) {\n                // Check if version is affected\n                if self.is_version_affected(version, &advisory.patched_versions) {\n                    vulns.push(advisory.clone());\n                }\n            }\n        }\n\n        vulns\n    }\n\n    fn is_version_affected(&self, version: &str, patched: &[String]) -> bool {\n        // Parse semver and check\n        let version = semver::Version::parse(version).unwrap();\n\n        for patched_ver in patched {\n            if let Ok(patched) = semver::Version::parse(patched_ver) {\n                if version >= patched {\n                    return false;\n                }\n            }\n        }\n\n        true\n    }\n\n    pub fn generate_report(&self, reports: &[VulnerabilityReport]) -> String {\n        let mut output = String::new();\n\n        output.push_str(\"# Vulnerability Report\\n\\n\");\n\n        let critical: Vec<_> = reports.iter()\n            .flat_map(|r| &r.vulnerabilities)\n            .filter(|v| matches!(v.severity, Severity::Critical))\n            .collect();\n\n        let high: Vec<_> = reports.iter()\n            .flat_map(|r| &r.vulnerabilities)\n            .filter(|v| matches!(v.severity, Severity::High))\n            .collect();\n\n        output.push_str(&format!(\"Critical: {}\\n\", critical.len()));\n        output.push_str(&format!(\"High: {}\\n\", high.len()));\n        output.push_str(\"\\n\");\n\n        for report in reports {\n            output.push_str(&format!(\"## {} @ {}\\n\", report.package, report.version));\n\n            for vuln in &report.vulnerabilities {\n                output.push_str(&format!(\"- **{}**: {}\\n\", vuln.id, vuln.title));\n                output.push_str(&format!(\"  Severity: {:?}\\n\", vuln.severity));\n                output.push_str(&format!(\"  Patched: {}\\n\", vuln.patched_versions.join(\", \")));\n            }\n\n            output.push_str(\"\\n\");\n        }\n\n        output\n    }\n}\n```\n\n## SAST (Static Analysis)\n\n```rust\nuse syn::{visit::Visit, File, ItemFn, Expr};\n\npub struct SecurityVisitor {\n    findings: Vec<SecurityFinding>,\n}\n\n#[derive(Debug)]\npub struct SecurityFinding {\n    pub severity: Severity,\n    pub message: String,\n    pub line: usize,\n    pub category: FindingCategory,\n}\n\n#[derive(Debug)]\npub enum FindingCategory {\n    UnsafeCode,\n    PanicUsage,\n    UnwrapUsage,\n    ExpectUsage,\n    DangerousFunction,\n}\n\nimpl<'ast> Visit<'ast> for SecurityVisitor {\n    fn visit_item_fn(&mut self, node: &'ast ItemFn) {\n        // Check for unsafe blocks\n        if node.sig.unsafety.is_some() {\n            self.findings.push(SecurityFinding {\n                severity: Severity::Medium,\n                message: format!(\"Unsafe function: {}\", node.sig.ident),\n                line: 0, // Would need span info\n                category: FindingCategory::UnsafeCode,\n            });\n        }\n\n        // Continue visiting\n        syn::visit::visit_item_fn(self, node);\n    }\n\n    fn visit_expr(&mut self, node: &'ast Expr) {\n        // Check for unwrap/expect\n        if let Expr::MethodCall(call) = node {\n            let method_name = call.method.to_string();\n\n            if method_name == \"unwrap\" {\n                self.findings.push(SecurityFinding {\n                    severity: Severity::Low,\n                    message: \"unwrap() can panic\".to_string(),\n                    line: 0,\n                    category: FindingCategory::UnwrapUsage,\n                });\n            } else if method_name == \"expect\" {\n                self.findings.push(SecurityFinding {\n                    severity: Severity::Low,\n                    message: \"expect() can panic\".to_string(),\n                    line: 0,\n                    category: FindingCategory::ExpectUsage,\n                });\n            }\n        }\n\n        syn::visit::visit_expr(self, node);\n    }\n}\n\npub fn analyze_file(source: &str) -> Result<Vec<SecurityFinding>, String> {\n    let syntax_tree: File = syn::parse_str(source)\n        .map_err(|e| e.to_string())?;\n\n    let mut visitor = SecurityVisitor {\n        findings: vec![],\n    };\n\n    visitor.visit_file(&syntax_tree);\n\n    Ok(visitor.findings)\n}\n```",
      "terminal_task": {
        "command": "cargo test vulnerability -- --nocapture",
        "expected_output": "test vulnerability::tests::test_dependency_scan ... ok",
        "hint": "Utilisez RustSec pour les advisories, syn pour l'analyse statique, et semver pour la comparaison de versions"
      }
    },
    {
      "id": "cyber-006-audit-logging",
      "track": "CYBER_OPS",
      "level": "Expert",
      "title": "Audit Logging et Forensics",
      "duration": "5h30",
      "description": "Journalisation sécurisée, immutabilité, et analyse forensique.",
      "markdown_content": "# Audit Logging\n\n## Architecture d'Audit\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    AUDIT LOGGING                            │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  Application ──▶ Structured Log ──▶ Tamper-proof Store     │\n│                                                             │\n│  ┌─────────────┐    ┌─────────────┐    ┌─────────────────┐ │\n│  │   Sign      │    │   Hash      │    │   Remote        │ │\n│  │  (HMAC)     │    │  (chain)    │    │   Storage       │ │\n│  └─────────────┘    └─────────────┘    └─────────────────┘ │\n│                                                             │\n│  Log Entry:                                                 │\n│  { timestamp, event, user, resource, action, result, hash } │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Audit Logger\n\n```rust\nuse serde::{Deserialize, Serialize};\nuse sha2::{Sha256, Digest};\nuse std::time::{SystemTime, UNIX_EPOCH};\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct AuditLogEntry {\n    pub timestamp: u64,\n    pub sequence: u64,\n    pub event_type: EventType,\n    pub user_id: String,\n    pub resource: String,\n    pub action: String,\n    pub result: ActionResult,\n    pub metadata: serde_json::Value,\n    pub prev_hash: String,\n    pub hash: String,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum EventType {\n    Authentication,\n    Authorization,\n    DataAccess,\n    DataModification,\n    SystemChange,\n    SecurityEvent,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum ActionResult {\n    Success,\n    Failure(String),\n}\n\npub struct AuditLogger {\n    key: [u8; 32],\n    sequence: u64,\n    last_hash: String,\n    storage: Box<dyn AuditStorage>,\n}\n\n#[async_trait]\npub trait AuditStorage: Send + Sync {\n    async fn append(&mut self, entry: &AuditLogEntry) -> Result<(), String>;\n    async fn get_range(&self, start: u64, end: u64) -> Result<Vec<AuditLogEntry>, String>;\n}\n\nimpl AuditLogger {\n    pub fn new(key: [u8; 32], storage: Box<dyn AuditStorage>) -> Self {\n        Self {\n            key,\n            sequence: 0,\n            last_hash: String::new(),\n            storage,\n        }\n    }\n\n    pub async fn log(\n        &mut self,\n        event_type: EventType,\n        user_id: &str,\n        resource: &str,\n        action: &str,\n        result: ActionResult,\n        metadata: serde_json::Value,\n    ) -> Result<(), String> {\n        let timestamp = SystemTime::now()\n            .duration_since(UNIX_EPOCH)\n            .unwrap()\n            .as_secs();\n\n        self.sequence += 1;\n\n        let mut entry = AuditLogEntry {\n            timestamp,\n            sequence: self.sequence,\n            event_type,\n            user_id: user_id.to_string(),\n            resource: resource.to_string(),\n            action: action.to_string(),\n            result,\n            metadata,\n            prev_hash: self.last_hash.clone(),\n            hash: String::new(),\n        };\n\n        // Calculate hash\n        entry.hash = self.calculate_hash(&entry);\n\n        // Sign entry\n        let signature = self.sign_entry(&entry);\n\n        // Store\n        self.storage.append(&entry).await?;\n\n        self.last_hash = entry.hash.clone();\n\n        Ok(())\n    }\n\n    fn calculate_hash(&self, entry: &AuditLogEntry) -> String {\n        let data = format!(\n            \"{}:{}:{}:{}:{}:{}:{}:{}\",\n            entry.timestamp,\n            entry.sequence,\n            entry.user_id,\n            entry.resource,\n            entry.action,\n            entry.metadata.to_string(),\n            entry.prev_hash,\n            std::any::type_name_of_val(&entry.event_type)\n        );\n\n        let mut hasher = Sha256::new();\n        hasher.update(data);\n        hasher.update(&self.key);\n        format!(\"{:x}\", hasher.finalize())\n    }\n\n    fn sign_entry(&self, entry: &AuditLogEntry) -> Vec<u8> {\n        use hmac::{Hmac, Mac};\n        use sha2::Sha256;\n\n        type HmacSha256 = Hmac<Sha256>;\n\n        let mut mac = HmacSha256::new_from_slice(&self.key).unwrap();\n        mac.update(entry.hash.as_bytes());\n        mac.finalize().into_bytes().to_vec()\n    }\n\n    pub async fn verify_chain(&self) -> Result<bool, String> {\n        let entries = self.storage.get_range(1, self.sequence).await?;\n\n        for i in 1..entries.len() {\n            let current = &entries[i];\n            let previous = &entries[i - 1];\n\n            // Verify chain\n            if current.prev_hash != previous.hash {\n                return Ok(false);\n            }\n\n            // Verify hash\n            let expected_hash = self.calculate_hash(current);\n            if current.hash != expected_hash {\n                return Ok(false);\n            }\n        }\n\n        Ok(true)\n    }\n}\n\n// File storage implementation\npub struct FileAuditStorage {\n    file: tokio::fs::File,\n}\n\n#[async_trait]\nimpl AuditStorage for FileAuditStorage {\n    async fn append(&mut self, entry: &AuditLogEntry) -> Result<(), String> {\n        use tokio::io::AsyncWriteExt;\n\n        let line = serde_json::to_string(entry).map_err(|e| e.to_string())?;\n        self.file.write_all(line.as_bytes()).await.map_err(|e| e.to_string())?;\n        self.file.write_all(b\"\\n\").await.map_err(|e| e.to_string())?;\n        self.file.flush().await.map_err(|e| e.to_string())?;\n\n        Ok(())\n    }\n\n    async fn get_range(&self, start: u64, end: u64) -> Result<Vec<AuditLogEntry>, String> {\n        // Read and parse\n        let content = tokio::fs::read_to_string(\"audit.log\").await\n            .map_err(|e| e.to_string())?;\n\n        let mut entries = vec![];\n        for line in content.lines() {\n            if let Ok(entry) = serde_json::from_str::<AuditLogEntry>(line) {\n                if entry.sequence >= start && entry.sequence <= end {\n                    entries.push(entry);\n                }\n            }\n        }\n\n        Ok(entries)\n    }\n}\n```",
      "terminal_task": {
        "command": "cargo test audit -- --nocapture",
        "expected_output": "test audit::tests::test_chain_verification ... ok",
        "hint": "Utilisez une chaîne de hachage pour l'immutabilité, HMAC pour la signature, et une séquence pour l'ordre"
      }
    },
    {
      "id": "cyber-007-reverse-engineering",
      "track": "CYBER_OPS",
      "level": "Expert",
      "title": "Reverse Engineering Basique",
      "duration": "8h00",
      "description": "Analyse de binaires, disassembly, et compréhension de code compilé.",
      "markdown_content": "# Reverse Engineering\n\n## Architecture d'Analyse\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    REVERSE ENGINEERING                      │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  Binary ──▶ Disassembler ──▶ Control Flow ──▶ Analysis     │\n│                                                             │\n│  ┌─────────────┐    ┌─────────────┐    ┌─────────────────┐ │\n│  │   ELF/PE    │    │   IDA/Ghidra│    │   Decompiler    │ │\n│  │  Parser     │    │   (x86/ARM) │    │   (pseudo-C)    │ │\n│  └─────────────┘    └─────────────┘    └─────────────────┘ │\n│                                                             │\n│  Techniques:                                                │\n│  - Static analysis (strings, imports, exports)              │\n│  - Dynamic analysis (debugging, tracing)                    │\n│  - Control flow reconstruction                              │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## ELF Parser\n\n```rust\nuse goblin::elf::Elf;\nuse goblin::Object;\n\npub struct BinaryAnalyzer {\n    data: Vec<u8>,\n}\n\n#[derive(Debug)]\npub struct BinaryInfo {\n    pub architecture: String,\n    pub entry_point: u64,\n    pub sections: Vec<SectionInfo>,\n    pub symbols: Vec<SymbolInfo>,\n    pub imports: Vec<ImportInfo>,\n    pub strings: Vec<String>,\n}\n\n#[derive(Debug)]\npub struct SectionInfo {\n    pub name: String,\n    pub address: u64,\n    pub size: u64,\n    pub flags: String,\n}\n\n#[derive(Debug)]\npub struct SymbolInfo {\n    pub name: String,\n    pub address: u64,\n    pub size: u64,\n    pub symbol_type: String,\n}\n\n#[derive(Debug)]\npub struct ImportInfo {\n    pub name: String,\n    pub library: String,\n}\n\nimpl BinaryAnalyzer {\n    pub fn new(data: Vec<u8>) -> Self {\n        Self { data }\n    }\n\n    pub fn analyze(&self) -> Result<BinaryInfo, String> {\n        match Object::parse(&self.data).map_err(|e| e.to_string())? {\n            Object::Elf(elf) => self.analyze_elf(&elf),\n            Object::PE(pe) => self.analyze_pe(&pe),\n            _ => Err(\"Unsupported format\".to_string()),\n        }\n    }\n\n    fn analyze_elf(&self, elf: &Elf) -> Result<BinaryInfo, String> {\n        let arch = match elf.header.e_machine {\n            0x3e => \"x86_64\",\n            0x03 => \"x86\",\n            0x28 => \"ARM\",\n            0xb7 => \"AArch64\",\n            _ => \"Unknown\",\n        };\n\n        let sections: Vec<_> = elf.section_headers.iter()\n            .filter_map(|sh| {\n                let name = elf.shdr_strtab.get_at(sh.sh_name)?;\n                Some(SectionInfo {\n                    name: name.to_string(),\n                    address: sh.sh_addr,\n                    size: sh.sh_size,\n                    flags: format!(\"{:?}\", sh.sh_flags),\n                })\n            })\n            .collect();\n\n        let symbols: Vec<_> = elf.syms.iter()\n            .filter_map(|sym| {\n                let name = elf.strtab.get_at(sym.st_name)?;\n                Some(SymbolInfo {\n                    name: name.to_string(),\n                    address: sym.st_value,\n                    size: sym.st_size,\n                    symbol_type: format!(\"{:?}\", sym.st_type()),\n                })\n            })\n            .collect();\n\n        let imports: Vec<_> = elf.imports().iter()\n            .map(|imp| ImportInfo {\n                name: imp.name.to_string(),\n                library: imp.dll.to_string(),\n            })\n            .collect();\n\n        let strings = self.extract_strings();\n\n        Ok(BinaryInfo {\n            architecture: arch.to_string(),\n            entry_point: elf.entry,\n            sections,\n            symbols,\n            imports,\n            strings,\n        })\n    }\n\n    fn analyze_pe(&self, pe: &goblin::pe::PE) -> Result<BinaryInfo, String> {\n        // Similar analysis for PE files\n        Ok(BinaryInfo {\n            architecture: \"PE\".to_string(),\n            entry_point: pe.entry,\n            sections: vec![],\n            symbols: vec![],\n            imports: vec![],\n            strings: self.extract_strings(),\n        })\n    }\n\n    fn extract_strings(&self) -> Vec<String> {\n        let mut strings = vec![];\n        let min_length = 4;\n\n        let mut current = String::new();\n        for &byte in &self.data {\n            if byte.is_ascii_graphic() || byte == b' ' {\n                current.push(byte as char);\n            } else {\n                if current.len() >= min_length {\n                    strings.push(current.clone());\n                }\n                current.clear();\n            }\n        }\n\n        strings\n    }\n\n    pub fn find_function_starts(&self) -> Vec<u64> {\n        let mut addresses = vec![];\n\n        // Simple pattern: look for function prologue\n        // push rbp; mov rbp, rsp (0x55 0x48 0x89 0xe5)\n        let prologue = [0x55, 0x48, 0x89, 0xe5];\n\n        for i in 0..self.data.len() - prologue.len() {\n            if &self.data[i..i + prologue.len()] == &prologue[..] {\n                addresses.push(i as u64);\n            }\n        }\n\n        addresses\n    }\n}\n```\n\n## Disassembly Basique\n\n```rust\nuse iced_x86::{Decoder, DecoderOptions, Instruction, Formatter, NasmFormatter};\n\npub struct Disassembler {\n    code: Vec<u8>,\n    base_address: u64,\n}\n\n#[derive(Debug)]\npub struct DisassembledInstruction {\n    pub address: u64,\n    pub bytes: Vec<u8>,\n    pub mnemonic: String,\n    pub operands: String,\n}\n\nimpl Disassembler {\n    pub fn new(code: Vec<u8>, base_address: u64) -> Self {\n        Self { code, base_address }\n    }\n\n    pub fn disassemble(&self) -> Vec<DisassembledInstruction> {\n        let mut instructions = vec![];\n        let mut decoder = Decoder::with_ip(\n            64,\n            &self.code,\n            self.base_address,\n            DecoderOptions::NONE,\n        );\n\n        let mut formatter = NasmFormatter::new();\n\n        while decoder.can_decode() {\n            let mut instruction = Instruction::default();\n            decoder.decode_out(&mut instruction);\n\n            let mut output = String::new();\n            formatter.format(&instruction, &mut output);\n\n            let bytes = self.code[instruction.ip() as usize - self.base_address as usize..\n                instruction.next_ip() as usize - self.base_address as usize]\n                .to_vec();\n\n            instructions.push(DisassembledInstruction {\n                address: instruction.ip(),\n                bytes,\n                mnemonic: instruction.mnemonic().to_string(),\n                operands: output,\n            });\n        }\n\n        instructions\n    }\n\n    pub fn find_calls(&self) -> Vec<u64> {\n        let mut calls = vec![];\n        let mut decoder = Decoder::with_ip(\n            64,\n            &self.code,\n            self.base_address,\n            DecoderOptions::NONE,\n        );\n\n        while decoder.can_decode() {\n            let instruction = decoder.decode();\n\n            if instruction.mnemonic() == iced_x86::Mnemonic::Call {\n                if let Some(target) = instruction.near_branch_target() {\n                    calls.push(target);\n                }\n            }\n        }\n\n        calls\n    }\n\n    pub fn build_cfg(&self) -> ControlFlowGraph {\n        let mut cfg = ControlFlowGraph::new();\n        let instructions = self.disassemble();\n\n        let mut current_block = BasicBlock::new();\n\n        for inst in &instructions {\n            current_block.instructions.push(inst.clone());\n\n            // Check if instruction is a branch\n            match inst.mnemonic.as_str() {\n                \"jmp\" | \"je\" | \"jne\" | \"jg\" | \"jl\" | \"call\" | \"ret\" => {\n                    cfg.blocks.push(current_block);\n                    current_block = BasicBlock::new();\n                }\n                _ => {}\n            }\n        }\n\n        if !current_block.instructions.is_empty() {\n            cfg.blocks.push(current_block);\n        }\n\n        cfg\n    }\n}\n\n#[derive(Debug)]\npub struct ControlFlowGraph {\n    pub blocks: Vec<BasicBlock>,\n    pub edges: Vec<(usize, usize)>,\n}\n\nimpl ControlFlowGraph {\n    fn new() -> Self {\n        Self {\n            blocks: vec![],\n            edges: vec![],\n        }\n    }\n}\n\n#[derive(Debug)]\npub struct BasicBlock {\n    pub instructions: Vec<DisassembledInstruction>,\n}\n\nimpl BasicBlock {\n    fn new() -> Self {\n        Self {\n            instructions: vec![],\n        }\n    }\n}\n```",
      "terminal_task": {
        "command": "cargo test reverse -- --nocapture",
        "expected_output": "test reverse::tests::test_elf_parse ... ok",
        "hint": "Utilisez goblin pour le parsing de binaires, iced_x86 pour le disassembly, et extrayez les strings pour l'analyse"
      }
    },
    {
      "id": "cyber-008-exploit-mitigation",
      "track": "CYBER_OPS",
      "level": "Expert",
      "title": "Mitigations contre les Exploits",
      "duration": "7h00",
      "description": "ASLR, DEP/NX, stack canaries, et protections modernes.",
      "markdown_content": "# Mitigations contre les Exploits\n\n## Protections Système\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    EXPLOIT MITIGATIONS                      │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  ┌─────────────┐    ┌─────────────┐    ┌─────────────────┐ │\n│  │    ASLR     │    │  DEP/NX     │    │  Stack Canary   │ │\n│  │  (random)   │    │  (no exec)  │    │  (cookie)       │ │\n│  └─────────────┘    └─────────────┘    └─────────────────┘ │\n│                                                             │\n│  ┌─────────────┐    ┌─────────────┐    ┌─────────────────┐ │\n│  │    PIE      │    │  RELRO      │    │  Fortify Source │ │\n│  │  (position) │    │  (GOT prot) │    │  (checks)       │ │\n│  └─────────────┘    └─────────────┘    └─────────────────┘ │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Vérification des Protections\n\n```rust\nuse goblin::elf::Elf;\nuse goblin::elf::program_header::{PT_GNU_STACK, PT_GNU_RELRO};\n\n#[derive(Debug)]\npub struct SecurityFeatures {\n    pub aslr: bool,\n    pub nx: bool,\n    pub canary: bool,\n    pub pie: bool,\n    pub relro: RelroLevel,\n    pub fortify: bool,\n}\n\n#[derive(Debug)]\npub enum RelroLevel {\n    None,\n    Partial,\n    Full,\n}\n\npub struct ExploitMitigationChecker;\n\nimpl ExploitMitigationChecker {\n    pub fn check_elf(elf: &Elf) -> SecurityFeatures {\n        SecurityFeatures {\n            aslr: Self::has_aslr(elf),\n            nx: Self::has_nx(elf),\n            canary: Self::has_canary(elf),\n            pie: Self::has_pie(elf),\n            relro: Self::check_relro(elf),\n            fortify: false, // Requires symbol analysis\n        }\n    }\n\n    fn has_aslr(elf: &Elf) -> bool {\n        // ASLR is an OS feature, but PIE is required\n        elf.header.e_type == 3 // ET_DYN\n    }\n\n    fn has_nx(elf: &Elf) -> bool {\n        // Check GNU_STACK program header\n        elf.program_headers.iter()\n            .any(|ph| {\n                ph.p_type == PT_GNU_STACK && ph.p_flags & 1 == 0\n            })\n    }\n\n    fn has_canary(elf: &Elf) -> bool {\n        // Check for __stack_chk_fail symbol\n        elf.syms.iter()\n            .any(|sym| {\n                if let Some(name) = elf.strtab.get_at(sym.st_name) {\n                    name.contains(\"__stack_chk_fail\")\n                } else {\n                    false\n                }\n            })\n    }\n\n    fn has_pie(elf: &Elf) -> bool {\n        elf.header.e_type == 3 // ET_DYN\n    }\n\n    fn check_relro(elf: &Elf) -> RelroLevel {\n        let has_relro = elf.program_headers.iter()\n            .any(|ph| ph.p_type == PT_GNU_RELRO);\n\n        let has_bind_now = elf.dynamic.as_ref()\n            .map(|d| d.dyns.iter()\n                .any(|dy| dy.d_tag == 0x1e)) // DF_BIND_NOW\n            .unwrap_or(false);\n\n        if has_relro && has_bind_now {\n            RelroLevel::Full\n        } else if has_relro {\n            RelroLevel::Partial\n        } else {\n            RelroLevel::None\n        }\n    }\n\n    pub fn report(features: &SecurityFeatures) -> String {\n        let mut report = String::new();\n\n        report.push_str(\"Security Features Report\\n\");\n        report.push_str(\"=========================\\n\\n\");\n\n        let aslr_status = if features.aslr { \"✓\" } else { \"✗\" };\n        report.push_str(&format!(\"ASLR: {}\\n\", aslr_status));\n\n        let nx_status = if features.nx { \"✓\" } else { \"✗\" };\n        report.push_str(&format!(\"NX/DEP: {}\\n\", nx_status));\n\n        let canary_status = if features.canary { \"✓\" } else { \"✗\" };\n        report.push_str(&format!(\"Stack Canary: {}\\n\", canary_status));\n\n        let pie_status = if features.pie { \"✓\" } else { \"✗\" };\n        report.push_str(&format!(\"PIE: {}\\n\", pie_status));\n\n        report.push_str(&format!(\"RELRO: {:?}\\n\", features.relro));\n\n        let fortify_status = if features.fortify { \"✓\" } else { \"✗\" };\n        report.push_str(&format!(\"Fortify Source: {}\\n\", fortify_status));\n\n        report\n    }\n}\n```\n\n## Sécurisation du Code Rust\n\n```rust\n// Cargo.toml configuration for security\n// [profile.release]\n// lto = true          # Link-time optimization\n// panic = \"abort\"     # Smaller binary, no unwinding\n// codegen-units = 1   # Better optimization\n// strip = true        # Remove symbols\n\n// Unsafe code guidelines\npub struct SafeBuffer {\n    data: Vec<u8>,\n}\n\nimpl SafeBuffer {\n    pub fn new(size: usize) -> Self {\n        Self {\n            data: vec![0; size],\n        }\n    }\n\n    pub fn write(&mut self, offset: usize, data: &[u8]) -> Result<(), String> {\n        // Bounds check\n        if offset + data.len() > self.data.len() {\n            return Err(\"Buffer overflow\".to_string());\n        }\n\n        self.data[offset..offset + data.len()].copy_from_slice(data);\n        Ok(())\n    }\n\n    pub fn read(&self, offset: usize, len: usize) -> Result<&[u8], String> {\n        if offset + len > self.data.len() {\n            return Err(\"Out of bounds read\".to_string());\n        }\n\n        Ok(&self.data[offset..offset + len])\n    }\n}\n\n// When unsafe is necessary\npub fn safe_memcpy(dst: &mut [u8], src: &[u8]) -> Result<(), String> {\n    if dst.len() < src.len() {\n        return Err(\"Destination too small\".to_string());\n    }\n\n    unsafe {\n        std::ptr::copy_nonoverlapping(\n            src.as_ptr(),\n            dst.as_mut_ptr(),\n            src.len()\n        );\n    }\n\n    Ok(())\n}\n\n// Constant-time operations\npub fn constant_time_compare(a: &[u8], b: &[u8]) -> bool {\n    if a.len() != b.len() {\n        return false;\n    }\n\n    let mut result = 0u8;\n    for (x, y) in a.iter().zip(b.iter()) {\n        result |= x ^ y;\n    }\n\n    result == 0\n}\n\n// Secure memory zeroing\npub fn secure_zero(memory: &mut [u8]) {\n    for byte in memory.iter_mut() {\n        unsafe {\n            std::ptr::write_volatile(byte, 0);\n        }\n    }\n    std::sync::atomic::fence(std::sync::atomic::Ordering::SeqCst);\n}\n```",
      "terminal_task": {
        "command": "cargo test mitigations -- --nocapture",
        "expected_output": "test mitigations::tests::test_security_features ... ok",
        "hint": "Vérifiez les program headers pour ASLR/NX/RELRO, les symboles pour les canaries, et utilisez des constant-time operations pour la crypto"
      }
    },
    {
      "id": "cyber-009-incident-response",
      "track": "CYBER_OPS",
      "level": "Expert",
      "title": "Incident Response et Forensics",
      "duration": "6h00",
      "description": "Procédures d'incident, collecte de preuves, et analyse post-mortem.",
      "markdown_content": "# Incident Response\n\n## Procédure d'Incident Response\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    INCIDENT RESPONSE                        │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  1. PREPARATION                                             │\n│     - Playbooks, tools, contacts                            │\n│                                                             │\n│  2. IDENTIFICATION                                          │\n│     - Detect, analyze, scope                                │\n│                                                             │\n│  3. CONTAINMENT                                             │\n│     - Short-term: isolate                                   │\n│     - Long-term: segment                                    │\n│                                                             │\n│  4. ERADICATION                                             │\n│     - Remove threat, patch                                  │\n│                                                             │\n│  5. RECOVERY                                                │\n│     - Restore systems, monitor                              │\n│                                                             │\n│  6. LESSONS LEARNED                                         │\n│     - Post-mortem, improve                                  │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Incident Manager\n\n```rust\nuse chrono::{DateTime, Utc};\nuse serde::{Deserialize, Serialize};\nuse uuid::Uuid;\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Incident {\n    pub id: String,\n    pub title: String,\n    pub description: String,\n    pub severity: IncidentSeverity,\n    pub status: IncidentStatus,\n    pub created_at: DateTime<Utc>,\n    pub detected_at: DateTime<Utc>,\n    pub resolved_at: Option<DateTime<Utc>>,\n    pub affected_systems: Vec<String>,\n    pub indicators: Vec<Indicator>,\n    pub timeline: Vec<TimelineEvent>,\n    pub assigned_to: Option<String>,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum IncidentSeverity {\n    Low,\n    Medium,\n    High,\n    Critical,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum IncidentStatus {\n    Detected,\n    Analyzing,\n    Contained,\n    Eradicated,\n    Recovered,\n    Closed,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Indicator {\n    pub indicator_type: IndicatorType,\n    pub value: String,\n    pub confidence: f32,\n    pub first_seen: DateTime<Utc>,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum IndicatorType {\n    IP,\n    Domain,\n    Hash,\n    FilePath,\n    RegistryKey,\n    ProcessName,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct TimelineEvent {\n    pub timestamp: DateTime<Utc>,\n    pub description: String,\n    pub actor: String,\n    pub evidence: Vec<String>,\n}\n\npub struct IncidentManager {\n    incidents: Vec<Incident>,\n    playbook: IncidentPlaybook,\n}\n\nimpl IncidentManager {\n    pub fn new() -> Self {\n        Self {\n            incidents: vec![],\n            playbook: IncidentPlaybook::default(),\n        }\n    }\n\n    pub fn create_incident(\n        &mut self,\n        title: &str,\n        description: &str,\n        severity: IncidentSeverity,\n        affected_systems: Vec<String>,\n    ) -> String {\n        let id = Uuid::new_v4().to_string();\n\n        let incident = Incident {\n            id: id.clone(),\n            title: title.to_string(),\n            description: description.to_string(),\n            severity,\n            status: IncidentStatus::Detected,\n            created_at: Utc::now(),\n            detected_at: Utc::now(),\n            resolved_at: None,\n            affected_systems,\n            indicators: vec![],\n            timeline: vec![TimelineEvent {\n                timestamp: Utc::now(),\n                description: \"Incident detected\".to_string(),\n                actor: \"system\".to_string(),\n                evidence: vec![],\n            }],\n            assigned_to: None,\n        };\n\n        self.incidents.push(incident);\n\n        // Trigger playbook\n        self.execute_playbook(&id);\n\n        id\n    }\n\n    fn execute_playbook(&self, incident_id: &str) {\n        let steps = match self.get_incident(incident_id) {\n            Some(inc) => match inc.severity {\n                IncidentSeverity::Critical => vec![\n                    \"Isolate affected systems\",\n                    \"Preserve evidence\",\n                    \"Notify security team\",\n                    \"Engage external IR team\",\n                ],\n                IncidentSeverity::High => vec![\n                    \"Isolate affected systems\",\n                    \"Preserve evidence\",\n                    \"Notify security team\",\n                ],\n                _ => vec![\n                    \"Investigate scope\",\n                    \"Document findings\",\n                ],\n            },\n            None => return,\n        };\n\n        for step in steps {\n            println!(\"Executing: {}\", step);\n        }\n    }\n\n    pub fn get_incident(&self, id: &str) -> Option<&Incident> {\n        self.incidents.iter().find(|i| i.id == id)\n    }\n\n    pub fn update_status(&mut self, id: &str, status: IncidentStatus) {\n        if let Some(incident) = self.incidents.iter_mut().find(|i| i.id == id) {\n            incident.status = status;\n            incident.timeline.push(TimelineEvent {\n                timestamp: Utc::now(),\n                description: format!(\"Status changed to {:?}\", status),\n                actor: \"analyst\".to_string(),\n                evidence: vec![],\n            });\n        }\n    }\n\n    pub fn add_indicator(&mut self, id: &str, indicator: Indicator) {\n        if let Some(incident) = self.incidents.iter_mut().find(|i| i.id == id) {\n            incident.indicators.push(indicator);\n        }\n    }\n\n    pub fn generate_report(&self, id: &str) -> Option<String> {\n        let incident = self.get_incident(id)?;\n\n        let mut report = String::new();\n\n        report.push_str(&format!(\"# Incident Report: {}\\n\", incident.id));\n        report.push_str(&format!(\"## {}\\n\\n\", incident.title));\n\n        report.push_str(&format!(\"**Severity:** {:?}\\n\", incident.severity));\n        report.push_str(&format!(\"**Status:** {:?}\\n\", incident.status));\n        report.push_str(&format!(\"**Created:** {}\\n\", incident.created_at));\n        report.push_str(&format!(\"**Detected:** {}\\n\\n\", incident.detected_at));\n\n        report.push_str(\"## Description\\n\\n\");\n        report.push_str(&incident.description);\n        report.push_str(\"\\n\\n\");\n\n        report.push_str(\"## Affected Systems\\n\\n\");\n        for system in &incident.affected_systems {\n            report.push_str(&format!(\"- {}\\n\", system));\n        }\n        report.push_str(\"\\n\");\n\n        report.push_str(\"## Indicators of Compromise\\n\\n\");\n        for ioc in &incident.indicators {\n            report.push_str(&format!(\"- {:?}: {} (confidence: {:.0}%)\\n\", \n                ioc.indicator_type, ioc.value, ioc.confidence * 100.0));\n        }\n        report.push_str(\"\\n\");\n\n        report.push_str(\"## Timeline\\n\\n\");\n        for event in &incident.timeline {\n            report.push_str(&format!(\"- {}: {} (by {})\\n\", \n                event.timestamp, event.description, event.actor));\n        }\n\n        Some(report)\n    }\n}\n\n#[derive(Default)]\npub struct IncidentPlaybook;\n```\n\n## Collecte de Preuves\n\n```rust\nuse std::fs;\nuse std::path::Path;\n\npub struct EvidenceCollector;\n\n#[derive(Debug)]\npub struct Evidence {\n    pub id: String,\n    pub evidence_type: EvidenceType,\n    pub source: String,\n    pub collected_at: DateTime<Utc>,\n    pub hash: String,\n    pub content: Vec<u8>,\n}\n\n#[derive(Debug)]\npub enum EvidenceType {\n    MemoryDump,\n    DiskImage,\n    LogFile,\n    NetworkCapture,\n    ProcessSnapshot,\n    FileSystem,\n}\n\nimpl EvidenceCollector {\n    pub fn collect_file(path: &str) -> Result<Evidence, String> {\n        let content = fs::read(path).map_err(|e| e.to_string())?;\n        let hash = blake3::hash(&content).to_hex().to_string();\n\n        Ok(Evidence {\n            id: Uuid::new_v4().to_string(),\n            evidence_type: EvidenceType::FileSystem,\n            source: path.to_string(),\n            collected_at: Utc::now(),\n            hash,\n            content,\n        })\n    }\n\n    pub fn collect_logs(log_dir: &str, pattern: &str) -> Result<Vec<Evidence>, String> {\n        let mut evidence = vec![];\n\n        for entry in fs::read_dir(log_dir).map_err(|e| e.to_string())? {\n            let entry = entry.map_err(|e| e.to_string())?;\n            let path = entry.path();\n\n            if path.to_string_lossy().contains(pattern) {\n                evidence.push(Self::collect_file(path.to_str().unwrap())?);\n            }\n        }\n\n        Ok(evidence)\n    }\n\n    pub fn verify_integrity(evidence: &Evidence) -> bool {\n        let computed_hash = blake3::hash(&evidence.content).to_hex().to_string();\n        computed_hash == evidence.hash\n    }\n}\n```",
      "terminal_task": {
        "command": "cargo test incident -- --nocapture",
        "expected_output": "test incident::tests::test_incident_creation ... ok",
        "hint": "Suivez la procédure NIST (prep, detect, contain, eradicate, recover, lessons), preservez les preuves avec hash, et documentez tout"
      }
    },
    {
      "id": "cyber-010-red-team",
      "track": "CYBER_OPS",
      "level": "Expert",
      "title": "Red Team: Simulation d'Attaques",
      "duration": "7h30",
      "description": "Méthodologies red team, MITRE ATT&CK, et tests d'intrusion.",
      "markdown_content": "# Red Team: Simulation d'Attaques\n\n## Framework MITRE ATT&CK\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    MITRE ATT&CK MATRIX                      │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│  Initial Access        Execution          Persistence       │\n│  - Phishing            - Command Line     - Registry        │\n│  - Exploit Public      - Scripts          - Scheduled Tasks │\n│    Facing App          - User Execution   - Services        │\n│                                                             │\n│  Privilege Escalation  Defense Evasion    Credential Access │\n│  - Exploitation        - Obfuscation      - Credential      │\n│  - Bypass UAC          - Process Injection  Dumping         │\n│                                                             │\n│  Discovery             Lateral Movement     Exfiltration    │\n│  - System Info         - Remote Services    - C2 Channels   │\n│  - Network Sniffing    - Pass the Hash    - Cloud Storage   │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Simulateur d'Attaques\n\n```rust\nuse serde::{Deserialize, Serialize};\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct AttackTechnique {\n    pub id: String, // T1234\n    pub name: String,\n    pub tactic: Tactic,\n    pub description: String,\n    pub platform: Vec<Platform>,\n    pub detection: Vec<String>,\n    pub mitigation: Vec<String>,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum Tactic {\n    InitialAccess,\n    Execution,\n    Persistence,\n    PrivilegeEscalation,\n    DefenseEvasion,\n    CredentialAccess,\n    Discovery,\n    LateralMovement,\n    Collection,\n    CommandAndControl,\n    Exfiltration,\n    Impact,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum Platform {\n    Windows,\n    Linux,\n    macOS,\n    AWS,\n    Azure,\n    GCP,\n}\n\npub struct RedTeamSimulator {\n    techniques: Vec<AttackTechnique>,\n}\n\nimpl RedTeamSimulator {\n    pub fn new() -> Self {\n        Self {\n            techniques: Self::load_techniques(),\n        }\n    }\n\n    fn load_techniques() -> Vec<AttackTechnique> {\n        vec![\n            AttackTechnique {\n                id: \"T1566.001\".to_string(),\n                name: \"Spearphishing Attachment\".to_string(),\n                tactic: Tactic::InitialAccess,\n                description: \"Adversary sends spearphishing emails with malicious attachments\".to_string(),\n                platform: vec![Platform::Windows, Platform::Linux, Platform::macOS],\n                detection: vec![\n                    \"Email gateway logs\".to_string(),\n                    \"Attachment analysis\".to_string(),\n                ],\n                mitigation: vec![\n                    \"User training\".to_string(),\n                    \"Email filtering\".to_string(),\n                ],\n            },\n            AttackTechnique {\n                id: \"T1059.003\".to_string(),\n                name: \"Windows Command Shell\".to_string(),\n                tactic: Tactic::Execution,\n                description: \"Adversary uses cmd.exe to execute commands\".to_string(),\n                platform: vec![Platform::Windows],\n                detection: vec![\n                    \"Process monitoring\".to_string(),\n                    \"Command line logging\".to_string(),\n                ],\n                mitigation: vec![\n                    \"Application whitelisting\".to_string(),\n                    \"User account control\".to_string(),\n                ],\n            },\n            AttackTechnique {\n                id: \"T1003.001\".to_string(),\n                name: \"LSASS Memory\".to_string(),\n                tactic: Tactic::CredentialAccess,\n                description: \"Adversary dumps credentials from LSASS memory\".to_string(),\n                platform: vec![Platform::Windows],\n                detection: vec![\n                    \"LSASS access monitoring\".to_string(),\n                    \"Credential guard\".to_string(),\n                ],\n                mitigation: vec![\n                    \"Credential Guard\".to_string(),\n                    \"LSASS protection\".to_string(),\n                ],\n            },\n        ]\n    }\n\n    pub fn get_techniques_by_tactic(&self, tactic: Tactic) -> Vec<&AttackTechnique> {\n        self.techniques.iter()\n            .filter(|t| matches!(t.tactic, tactic))\n            .collect()\n    }\n\n    pub fn simulate(&self, technique_id: &str) -> SimulationResult {\n        let technique = self.techniques.iter()\n            .find(|t| t.id == technique_id);\n\n        match technique {\n            Some(t) => {\n                println!(\"Simulating: {} - {}\", t.id, t.name);\n\n                // Execute simulation based on technique\n                let result = match t.id.as_str() {\n                    \"T1059.003\" => self.simulate_cmd_execution(),\n                    \"T1003.001\" => self.simulate_credential_dump(),\n                    _ => SimulationResult::NotImplemented,\n                };\n\n                result\n            }\n            None => SimulationResult::TechniqueNotFound,\n        }\n    }\n\n    fn simulate_cmd_execution(&self) -> SimulationResult {\n        // Simulate benign command execution for testing detection\n        let output = std::process::Command::new(\"cmd\")\n            .args(&[\"/c\", \"echo\", \"Red Team Simulation\"])\n            .output();\n\n        match output {\n            Ok(_) => SimulationResult::Success,\n            Err(e) => SimulationResult::Failed(e.to_string()),\n        }\n    }\n\n    fn simulate_credential_dump(&self) -> SimulationResult {\n        // Simulate detection of credential access attempt\n        // In real scenario, would attempt to access LSASS\n        // For testing, just log the attempt\n        println!(\"Simulating credential dump attempt (would be detected)\");\n        SimulationResult::Detected\n    }\n\n    pub fn generate_report(&self, simulations: &[SimulationResult]) -> String {\n        let mut report = String::new();\n\n        report.push_str(\"# Red Team Simulation Report\\n\\n\");\n\n        let success = simulations.iter()\n            .filter(|s| matches!(s, SimulationResult::Success))\n            .count();\n        let detected = simulations.iter()\n            .filter(|s| matches!(s, SimulationResult::Detected))\n            .count();\n        let failed = simulations.iter()\n            .filter(|s| matches!(s, SimulationResult::Failed(_)))\n            .count();\n\n        report.push_str(&format!(\"Total simulations: {}\\n\", simulations.len()));\n        report.push_str(&format!(\"Successful: {}\\n\", success));\n        report.push_str(&format!(\"Detected: {}\\n\", detected));\n        report.push_str(&format!(\"Failed: {}\\n\\n\", failed));\n\n        report.push_str(\"## Recommendations\\n\\n\");\n\n        if success > 0 {\n            report.push_str(\"- Review detection rules for successful techniques\\n\");\n        }\n\n        if detected > 0 {\n            report.push_str(\"- Detection is working for {} techniques\\n\");\n        }\n\n        report\n    }\n}\n\n#[derive(Debug)]\npub enum SimulationResult {\n    Success,\n    Failed(String),\n    Detected,\n    Blocked,\n    NotImplemented,\n    TechniqueNotFound,\n}\n\n// Purple Team exercise\npub struct PurpleTeamExercise {\n    red_team: RedTeamSimulator,\n    blue_team: BlueTeamDefender,\n}\n\nimpl PurpleTeamExercise {\n    pub fn run_exercise(&self, scenarios: Vec<AttackScenario>) -> ExerciseReport {\n        let mut results = vec![];\n\n        for scenario in scenarios {\n            println!(\"Running scenario: {}\", scenario.name);\n\n            // Red team executes\n            let red_result = self.red_team.simulate(&scenario.technique_id);\n\n            // Blue team detects\n            let blue_result = self.blue_team.detect(&scenario.technique_id);\n\n            results.push(ScenarioResult {\n                scenario,\n                red_result,\n                blue_result,\n            });\n        }\n\n        ExerciseReport { results }\n    }\n}\n\n#[derive(Debug)]\npub struct AttackScenario {\n    pub name: String,\n    pub technique_id: String,\n    pub objectives: Vec<String>,\n}\n\n#[derive(Debug)]\npub struct ScenarioResult {\n    pub scenario: AttackScenario,\n    pub red_result: SimulationResult,\n    pub blue_result: DetectionResult,\n}\n\n#[derive(Debug)]\npub struct ExerciseReport {\n    pub results: Vec<ScenarioResult>,\n}\n\npub struct BlueTeamDefender;\n\n#[derive(Debug)]\npub enum DetectionResult {\n    Detected,\n    Missed,\n    Partial,\n}\n\nimpl BlueTeamDefender {\n    pub fn detect(&self, technique_id: &str) -> DetectionResult {\n        // Check if detection rules exist\n        // In real implementation, would query SIEM\n        match technique_id {\n            \"T1059.003\" => DetectionResult::Detected, // We have rules for cmd.exe\n            \"T1003.001\" => DetectionResult::Partial,  // Some coverage\n            _ => DetectionResult::Missed,\n        }\n    }\n}\n```",
      "terminal_task": {
        "command": "cargo test redteam -- --nocapture",
        "expected_output": "test redteam::tests::test_simulation ... ok",
        "hint": "Suivez MITRE ATT&CK pour la cartographie, simulez des techniques benignes pour tester la détection, et collaborez avec le blue team"
      }
    }
  ]
}